{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IMPORTING \n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "from sklearn import linear_model\n",
    "import sklearn.cross_validation\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import ensemble\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLLECTING APPAREL TRAINING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "app_data=pd.read_csv('/Users/s0c01xa/Documents/Walmart_Bplan/train_data_apparel.csv')\n",
    "app_data=np.array(app_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DATA PREPROCESSING FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TOKENIZING THE DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tokenized_doc(document):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokenized_doc=[]\n",
    "    for i in range(0,len(document)):\n",
    "        word=word_tokenize(document[i])\n",
    "        tokenized_doc.append(word)\n",
    "    \n",
    "    return tokenized_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONVERTING TO LOWERCASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting all words to lower case so that it is of same type at par with word to vec\n",
    "def get_lower_doc(document):\n",
    "    low_words=[]\n",
    "    low_doc=[]\n",
    "    for i in range(len(document)):\n",
    "        for word in document[i]:\n",
    "            lwords=word.lower()\n",
    "            low_words.append(lwords)\n",
    "        low_doc.append(low_words)\n",
    "        low_words=[]\n",
    "    return low_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMOVING STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given a document,remove the stopwords\n",
    "def get_stopwrd_free_doc(document):\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stpwrd_free_doc = []\n",
    "    doc=[]\n",
    "\n",
    "    for i in range (len(document)):\n",
    "        for w in document[i]:\n",
    "            if w not in stop_words:\n",
    "                doc.append(w)\n",
    "        stpwrd_free_doc.append(doc)\n",
    "        doc=[]\n",
    "    \n",
    "    return stpwrd_free_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMOVING PUNCTUATIONS AND NON-ALPHABETIC DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Removing non alpha words\n",
    "def get_alpha_doc(document):\n",
    "    alpha_doc=[]\n",
    "    alpha=[]\n",
    "    for i in range(len(document)):\n",
    "        for word in document[i]:\n",
    "            if (word.isalpha()==True):\n",
    "                alpha.append(word)\n",
    "        alpha_doc.append(alpha)\n",
    "        alpha=[]\n",
    "        \n",
    "    return alpha_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEMMATIZING AND STEMMING THE DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lem_doc(document):\n",
    "    from nltk import WordNetLemmatizer\n",
    "    from nltk.corpus import wordnet\n",
    "    wnl=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "    lem_words=[]\n",
    "    lem_doc=[]\n",
    "\n",
    "    for i in range(len(document)):\n",
    "        for word in document[i]:\n",
    "            lem=wnl.lemmatize(word)\n",
    "            lem_words.append(lem)\n",
    "        \n",
    "        lem_doc.append(lem_words)\n",
    "        lem_words=[]\n",
    "\n",
    "\n",
    "    return lem_doc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMOVING SMALL LETTER WORDS FROM DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Removing small (2) letter words\n",
    "def get_long_wrd_doc(document):\n",
    "    long_wrd_doc=[]\n",
    "    import re\n",
    "    shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "\n",
    "    for i in range(0,len(document)):\n",
    "        reg=[shortword.sub(' ', str(document[i]))]\n",
    "        long_wrd_doc.append(reg)\n",
    "    return long_wrd_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering the Most COMMON WORDS FROM THE LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Keeping only top frequen words\n",
    "def get_most_common(document):\n",
    "    from collections import Counter\n",
    "    count_m=[]\n",
    "    most_common=[]\n",
    "\n",
    "    for i in range(0,len(document)):\n",
    "        count=Counter(document[i]).most_common(120)\n",
    "        for j in count:\n",
    "            count_m.append(j[0])\n",
    "        most_common.append(count_m)   \n",
    "        count_m=[]\n",
    "    \n",
    "    \n",
    "    return most_common\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GETTING STRING VERSION OF DOC FOR TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_string_version(document):\n",
    "    Str_data=[]\n",
    "    for i in range(0,len(document)):\n",
    "        val=str(document[i])\n",
    "        Str_data.append(val)\n",
    "    return Str_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGNORES ANY CHARACTER WHICH IS NOT utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_udoc(document):\n",
    "    utf_doc=[]\n",
    "    for i in range(0,len(document)):\n",
    "        val=unicode(x[i],errors=\"ignore\")\n",
    "        utf_doc.append(val)\n",
    "\n",
    "    return utf_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOVE WORD2VEC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_path= '/Users/s0c01xa/Downloads/glove.6B/glove.6B.50d.txt'\n",
    "f=open(folder_path)\n",
    "doc=f.readlines()\n",
    "#****VIP\n",
    "#**WORD TO VEC DICTIONARY\n",
    "#Forming a dictionary-word2vec\n",
    "word2vec={}\n",
    "key=[]\n",
    "#looping though the doc.in the doc the entire thing is saved and is separated by a space bar.\n",
    "for line in doc:\n",
    "    #parts contains every word separately for doc1\n",
    "    parts=line.split(' ')\n",
    "    #part[0] contains the word\n",
    "    word=parts[0]\n",
    "    key.append(word)\n",
    "    #embed contains the vector\n",
    "    embed=np.array(parts[1:],dtype='float32')\n",
    "    #filling up the dictionary\n",
    "    word2vec[word]=embed\n",
    "\n",
    "#NOW IF i give word as the key, the corresponding vec rep will return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# APPAREL DATA CONSISTS OF CATEGORY DESCRIPTION AND THE ITEM DESCRIPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apparel_data=pd.read_csv('/Users/s0c01xa/Documents/Walmart_Bplan/train_data_apparel.csv')\n",
    "app_data=np.array(app_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BELOW STEP CONCATENATES ALL THE ITEM DESCRIPTIONS CORR TO EACH CATEGORY DESCRIPTION IN COMB_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#uniq_labels contain the names of the 332 category description\n",
    "uniq_labels=np.unique(app_data[:,1])\n",
    "\n",
    "#Labels contain item wise category description\n",
    "labels=app_data[:,1]\n",
    "\n",
    "\n",
    "\n",
    "item2cat={}\n",
    "cat=[]\n",
    "#looping though the doc.in the doc the entire thing is saved and is separated by a space bar.\n",
    "\n",
    "for i in range(0,len(uniq_labels)):\n",
    "    app=app_data[labels==uniq_labels[i]][:,0]\n",
    "    unq_lbl=uniq_labels[i]\n",
    "    cat.append(unq_lbl)\n",
    "    item2cat[unq_lbl]=app\n",
    "\n",
    "#comb_data contains all the concatenated information based on category description and item desc\n",
    "#uniq_labels contains the corr. labels\n",
    "comb_data=[]\n",
    "for i in range(0,len(uniq_labels)):\n",
    "    comb_data.append(str(item2cat[uniq_labels[i]]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PRE_PROCESSING FOR DESCRIPTION AND KEEPING TOP 120 WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc0=get_tokenized_doc(comb_data)\n",
    "doc1=get_lower_doc(doc0)\n",
    "doc2=get_stopwrd_free_doc(doc1)\n",
    "doc3=get_alpha_doc(doc2)\n",
    "doc4=get_most_common(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc4 saves the preprocessed description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE_PROCESSING UNIQUE LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab0=get_tokenized_doc(uniq_labels)\n",
    "lab1=get_lower_doc(lab0)\n",
    "lab2=get_stopwrd_free_doc(lab1)\n",
    "lab3=get_alpha_doc(lab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lab3 saves the preprocessed unique labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEIGHTED COMBINING THE DESCRIPTION AND THE UNIQUE LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_combined_doc(description,labels):\n",
    "    w=10\n",
    "    comb_doc=[]\n",
    "    for i in range(0,len(description)):\n",
    "        comb_doc.append(labels[i]*w+description[i])\n",
    "\n",
    "    return comb_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_doc=get_combined_doc(doc4,lab3)  #combinded_doc saves the combined description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL PREPROCESSING OF COMBINED DOCUMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_doc1=get_long_wrd_doc(combined_doc)\n",
    "data=combined_doc1\n",
    "data_string=get_string_version(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data goes for tfidf and further process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=sklearn.feature_extraction.text.CountVectorizer(input='content', encoding='utf-8', decode_error='strict', \n",
    "                                            strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, \n",
    "                                            stop_words='english', ngram_range=(1,1), analyzer='word', \n",
    "                                            max_df=0.99, min_df=0.001, max_features=3000, \n",
    "                                            vocabulary=None, binary=False)\n",
    "\n",
    "#Transforming the data to count of token\n",
    "term_frq=model.fit_transform(data_string)\n",
    "#features_names\n",
    "features=model.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse to Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Returns dense matrix\n",
    "def get_dense_matrix(term_frquency):\n",
    "    from scipy.sparse import csr_matrix\n",
    "    dense_trm_frq=term_frquency.todense()\n",
    "\n",
    "    return dense_trm_frq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_term_freq=get_dense_matrix(term_frq) #saves the dense representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncated Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the SVD MATRIX\n",
    "def get_svd(matrix):\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    svd = TruncatedSVD(n_components=500, n_iter=50, random_state=42)\n",
    "    lsa=svd.fit_transform(matrix)  \n",
    "\n",
    "    return lsa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd_matrix=get_svd(dense_term_freq)  #saves the svd vector for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMILARITY BETWEEN DOCUMENTS BASED ON TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cosine_similarity(matrix):\n",
    "    from sklearn.metrics.pairwise import cosine_similarity as cs\n",
    "    #sim_mat=np.zeros(332,332)\n",
    "    similarity_matrix=cs(matrix)\n",
    "    return similarity_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim_mat=get_cosine_similarity(svd_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKING THE SIMILARITY OF DOCUMENTS VISUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sim_mat=pd.DataFrame(sim_mat)  #Stores the similarity amongst the documents\n",
    "#uniq_labels[np.argsort(sim_mat[23])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#uniq_labels[np.argsort(sim_mat[23])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIERARCHICAL CLUSTERING THE DOCUMENTS BASED ON SVD MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4                  ACCESSORIES\n",
       "5            ACCESSORIES (D23)\n",
       "6            ACCESSORIES (D24)\n",
       "7            ACCESSORIES (D33)\n",
       "8              ACCESSORIES D26\n",
       "36     BEDDING AND ACCESSORIES\n",
       "321          WATCH ACCESSORIES\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(new_trm_frq)\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "hclust = AgglomerativeClustering(n_clusters=32)\n",
    "hclust.fit(dense_term_freq)\n",
    "#Labels it will store the entire labels for each of the \n",
    "#32 gave best\n",
    "clust_labels=hclust.labels_\n",
    "labelled_data=np.column_stack((uniq_labels,clust_labels))\n",
    "labelled_data=pd.DataFrame(labelled_data)\n",
    "a=labelled_data[0]\n",
    "a[labelled_data[1]==9]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART-2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUSINESS SENSE AND HIERARCHICAL CLUSTERING COMBINED CATEGORIZATION OF DATA HAS BEEN DONE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Combined Description</th>\n",
       "      <th>CATEGORY_DESCRIPTION</th>\n",
       "      <th>Social Intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RHD AMERICA NECK 18\"IRH AMERICA NECK SPINNER 1...</td>\n",
       "      <td>JEWELRY COSTUME</td>\n",
       "      <td>Jewelry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NB 6PK LOW CUT NB 6PK LOW CUT NB 6PK LOW CUT P...</td>\n",
       "      <td>CASUAL SOCKS</td>\n",
       "      <td>Socks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SWEATSHIRTS ONLINE ONLY DSV GILDAN MENS CREWNE...</td>\n",
       "      <td>DOTCOM (D23)</td>\n",
       "      <td>Others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SCRUB OUTFITS ONLINE ONLY DSV FEELIN' FELINE S...</td>\n",
       "      <td>SCRUBS</td>\n",
       "      <td>Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOL 6PK SEAMLESS LRB FOL BRF 6P AST 7 FOL 6PK ...</td>\n",
       "      <td>PACKAGED UNDERWEAR</td>\n",
       "      <td>Undergarments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18 HR BREATH BEAUTY 4716B WHITE 44D PLAYTEX 18...</td>\n",
       "      <td>FULL FIGURE</td>\n",
       "      <td>Undergarments</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Combined Description CATEGORY_DESCRIPTION  \\\n",
       "0  RHD AMERICA NECK 18\"IRH AMERICA NECK SPINNER 1...      JEWELRY COSTUME   \n",
       "1  NB 6PK LOW CUT NB 6PK LOW CUT NB 6PK LOW CUT P...         CASUAL SOCKS   \n",
       "2  SWEATSHIRTS ONLINE ONLY DSV GILDAN MENS CREWNE...         DOTCOM (D23)   \n",
       "3  SCRUB OUTFITS ONLINE ONLY DSV FEELIN' FELINE S...               SCRUBS   \n",
       "4  FOL 6PK SEAMLESS LRB FOL BRF 6P AST 7 FOL 6PK ...   PACKAGED UNDERWEAR   \n",
       "5  18 HR BREATH BEAUTY 4716B WHITE 44D PLAYTEX 18...          FULL FIGURE   \n",
       "\n",
       "   Social Intent  \n",
       "0        Jewelry  \n",
       "1          Socks  \n",
       "2         Others  \n",
       "3    Accessories  \n",
       "4  Undergarments  \n",
       "5  Undergarments  "
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_clust_labels=pd.read_csv('/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels.csv')\n",
    "data_with_clust_labels.head(6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STORING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydata=data_with_clust_labels   #mydata saves the combined data with cluster labels\n",
    "#x-item description is being stored and y stores the corresponding labels\n",
    "x=mydata['Combined Description']\n",
    "y=mydata['Social Intent']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ENCODING CLUSTER LABELS FOR FINAL CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Label Encoder and converting the y's into integers\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_coded=le.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING ITEM DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=get_udoc(x)  #Ignoring which are not utf-8\n",
    "x2=get_tokenized_doc(x1)\n",
    "x3=get_lower_doc(x2)\n",
    "x4=get_stopwrd_free_doc(x3)\n",
    "x5=get_alpha_doc(x4)\n",
    "x6=get_lem_doc(x5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x6 stores thepre-processed doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVING SMALL LETTER WORDS\n",
    "x7=get_long_wrd_doc(x6)\n",
    "desc_string=get_string_version(x7) #desc_string is input to tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING TERM FREQUENCY FOR CLASSIFICATION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saving the particular characteristics giving the best feature size\n",
    "term_freq_model=sklearn.feature_extraction.text.CountVectorizer(input='content', encoding='utf-8', decode_error='strict', \n",
    "                                                strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, \n",
    "                                                stop_words='english', ngram_range=(1,1), analyzer='word', \n",
    "                                                max_df=0.999, min_df=0,max_features=2000,\n",
    "                                                vocabulary=None, binary=False)\n",
    "\n",
    "#Transforming the data to count of token\n",
    "term_frq_p2=term_freq_model.fit_transform(desc_string)\n",
    "#features_names\n",
    "features_p2=term_freq_model.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'acc',\n",
       " u'accent',\n",
       " u'accessory',\n",
       " u'aci',\n",
       " u'acid',\n",
       " u'acrylic',\n",
       " u'act',\n",
       " u'active',\n",
       " u'activewear',\n",
       " u'adesso',\n",
       " u'adj',\n",
       " u'adjustable',\n",
       " u'adult',\n",
       " u'adv',\n",
       " u'advanced',\n",
       " u'aggro',\n",
       " u'air',\n",
       " u'alaska',\n",
       " u'aline',\n",
       " u'allison',\n",
       " u'allover',\n",
       " u'almond',\n",
       " u'aloha',\n",
       " u'amazing',\n",
       " u'amer',\n",
       " u'america',\n",
       " u'american',\n",
       " u'americana',\n",
       " u'analog',\n",
       " u'anchor',\n",
       " u'angel',\n",
       " u'angeles',\n",
       " u'angelique',\n",
       " u'angelstat',\n",
       " u'angry',\n",
       " u'animal',\n",
       " u'ank',\n",
       " u'ankle',\n",
       " u'anklet',\n",
       " u'anml',\n",
       " u'annual',\n",
       " u'anorak',\n",
       " u'aop',\n",
       " u'app',\n",
       " u'apparel',\n",
       " u'appel',\n",
       " u'applique',\n",
       " u'aqua',\n",
       " u'archive',\n",
       " u'arctic',\n",
       " u'argyle',\n",
       " u'ariel',\n",
       " u'armitron',\n",
       " u'arrow',\n",
       " u'art',\n",
       " u'ashirt',\n",
       " u'ashley',\n",
       " u'assorted',\n",
       " u'asst',\n",
       " u'ast',\n",
       " u'asymmetrical',\n",
       " u'ath',\n",
       " u'athleisure',\n",
       " u'athletic',\n",
       " u'athletics',\n",
       " u'attitude',\n",
       " u'auto',\n",
       " u'avalor',\n",
       " u'avenger',\n",
       " u'average',\n",
       " u'avg',\n",
       " u'avia',\n",
       " u'aviator',\n",
       " u'aviva',\n",
       " u'awesome',\n",
       " u'aztec',\n",
       " u'bab',\n",
       " u'babba',\n",
       " u'baby',\n",
       " u'babydoll',\n",
       " u'backpack',\n",
       " u'bag',\n",
       " u'bal',\n",
       " u'balconette',\n",
       " u'ball',\n",
       " u'ballerina',\n",
       " u'ballet',\n",
       " u'band',\n",
       " u'bandana',\n",
       " u'bandeau',\n",
       " u'banded',\n",
       " u'bandini',\n",
       " u'bangle',\n",
       " u'bank',\n",
       " u'baptism',\n",
       " u'bar',\n",
       " u'barbie',\n",
       " u'base',\n",
       " u'baseball',\n",
       " u'basic',\n",
       " u'basketball',\n",
       " u'bat',\n",
       " u'batgirl',\n",
       " u'batman',\n",
       " u'bay',\n",
       " u'bch',\n",
       " u'bck',\n",
       " u'beach',\n",
       " u'bead',\n",
       " u'beaded',\n",
       " u'bean',\n",
       " u'beanie',\n",
       " u'bear',\n",
       " u'beast',\n",
       " u'beau',\n",
       " u'beauty',\n",
       " u'bebe',\n",
       " u'beefy',\n",
       " u'beer',\n",
       " u'beige',\n",
       " u'believe',\n",
       " u'bell',\n",
       " u'belle',\n",
       " u'belly',\n",
       " u'belt',\n",
       " u'belted',\n",
       " u'ben',\n",
       " u'benard',\n",
       " u'benefit',\n",
       " u'berm',\n",
       " u'bermuda',\n",
       " u'berry',\n",
       " u'best',\n",
       " u'betty',\n",
       " u'beverly',\n",
       " u'bfp',\n",
       " u'bib',\n",
       " u'big',\n",
       " u'bigs',\n",
       " u'bik',\n",
       " u'bike',\n",
       " u'biki',\n",
       " u'bikini',\n",
       " u'bimini',\n",
       " u'bin',\n",
       " u'bird',\n",
       " u'bishop',\n",
       " u'bite',\n",
       " u'bkn',\n",
       " u'blac',\n",
       " u'black',\n",
       " u'blakely',\n",
       " u'blanket',\n",
       " u'blaze',\n",
       " u'blazer',\n",
       " u'blend',\n",
       " u'bling',\n",
       " u'bliss',\n",
       " u'blissful',\n",
       " u'blk',\n",
       " u'blksot',\n",
       " u'blkt',\n",
       " u'block',\n",
       " u'blouse',\n",
       " u'blt',\n",
       " u'bltd',\n",
       " u'blu',\n",
       " u'blue',\n",
       " u'board',\n",
       " u'boardshort',\n",
       " u'boat',\n",
       " u'boatneck',\n",
       " u'bocini',\n",
       " u'body',\n",
       " u'bodycon',\n",
       " u'bodymapped',\n",
       " u'bodysuit',\n",
       " u'bodysuits',\n",
       " u'boho',\n",
       " u'bomber',\n",
       " u'bon',\n",
       " u'bonded',\n",
       " u'bonus',\n",
       " u'boo',\n",
       " u'boot',\n",
       " u'bootcut',\n",
       " u'bootie',\n",
       " u'border',\n",
       " u'bos',\n",
       " u'bot',\n",
       " u'boucle',\n",
       " u'boundary',\n",
       " u'bow',\n",
       " u'box',\n",
       " u'boxed',\n",
       " u'boxer',\n",
       " u'boy',\n",
       " u'boyfriend',\n",
       " u'boyleg',\n",
       " u'boyshort',\n",
       " u'bra',\n",
       " u'brac',\n",
       " u'bracelet',\n",
       " u'brahma',\n",
       " u'braid',\n",
       " u'braided',\n",
       " u'bralette',\n",
       " u'brand',\n",
       " u'brass',\n",
       " u'brc',\n",
       " u'breathable',\n",
       " u'breathe',\n",
       " u'brf',\n",
       " u'brief',\n",
       " u'bright',\n",
       " u'brinley',\n",
       " u'brittney',\n",
       " u'brn',\n",
       " u'brooke',\n",
       " u'brother',\n",
       " u'brown',\n",
       " u'brownshoe',\n",
       " u'brushed',\n",
       " u'btm',\n",
       " u'btn',\n",
       " u'btrfly',\n",
       " u'bts',\n",
       " u'bttm',\n",
       " u'bubble',\n",
       " u'buckle',\n",
       " u'buddha',\n",
       " u'buffalo',\n",
       " u'built',\n",
       " u'bump',\n",
       " u'bungee',\n",
       " u'bunny',\n",
       " u'burbu',\n",
       " u'burnout',\n",
       " u'burnwash',\n",
       " u'butterfly',\n",
       " u'button',\n",
       " u'bxr',\n",
       " u'bxrbrf',\n",
       " u'byb',\n",
       " u'bys',\n",
       " u'cable',\n",
       " u'cabo',\n",
       " u'cafe',\n",
       " u'caged',\n",
       " u'calf',\n",
       " u'california',\n",
       " u'cami',\n",
       " u'camisole',\n",
       " u'camo',\n",
       " u'camp',\n",
       " u'candy',\n",
       " u'canvas',\n",
       " u'cap',\n",
       " u'cape',\n",
       " u'capital',\n",
       " u'capri',\n",
       " u'captain',\n",
       " u'captivate',\n",
       " u'car',\n",
       " u'card',\n",
       " u'cardi',\n",
       " u'cardigan',\n",
       " u'care',\n",
       " u'career',\n",
       " u'cargo',\n",
       " u'carolina',\n",
       " u'carp',\n",
       " u'carpenter',\n",
       " u'carrini',\n",
       " u'carter',\n",
       " u'cartoon',\n",
       " u'case',\n",
       " u'casio',\n",
       " u'cast',\n",
       " u'casual',\n",
       " u'casuals',\n",
       " u'cat',\n",
       " u'catalina',\n",
       " u'cbc',\n",
       " u'celestial',\n",
       " u'cell',\n",
       " u'center',\n",
       " u'cff',\n",
       " u'cha',\n",
       " u'chain',\n",
       " u'challis',\n",
       " u'chambray',\n",
       " u'character',\n",
       " u'charcoal',\n",
       " u'charm',\n",
       " u'check',\n",
       " u'cheekster',\n",
       " u'cheeky',\n",
       " u'cheetah',\n",
       " u'chemise',\n",
       " u'chenille',\n",
       " u'cherokee',\n",
       " u'chest',\n",
       " u'chevron',\n",
       " u'chevy',\n",
       " u'chic',\n",
       " u'chievous',\n",
       " u'chiffon',\n",
       " u'child',\n",
       " u'chill',\n",
       " u'chino',\n",
       " u'chn',\n",
       " u'choker',\n",
       " u'christening',\n",
       " u'christmas',\n",
       " u'chunky',\n",
       " u'cinch',\n",
       " u'cinched',\n",
       " u'cinderella',\n",
       " u'circle',\n",
       " u'city',\n",
       " u'class',\n",
       " u'classic',\n",
       " u'claw',\n",
       " u'clear',\n",
       " u'cleat',\n",
       " u'climate',\n",
       " u'climateright',\n",
       " u'clip',\n",
       " u'clog',\n",
       " u'close',\n",
       " u'clothing',\n",
       " u'clr',\n",
       " u'clrblk',\n",
       " u'club',\n",
       " u'clutch',\n",
       " u'cmfrt',\n",
       " u'cmft',\n",
       " u'coat',\n",
       " u'coffee',\n",
       " u'coin',\n",
       " u'col',\n",
       " u'cold',\n",
       " u'colette',\n",
       " u'collar',\n",
       " u'collection',\n",
       " u'college',\n",
       " u'color',\n",
       " u'colorblock',\n",
       " u'colored',\n",
       " u'com',\n",
       " u'combo',\n",
       " u'comf',\n",
       " u'comfor',\n",
       " u'comfort',\n",
       " u'comfortblend',\n",
       " u'comfortease',\n",
       " u'comfortsoft',\n",
       " u'comfy',\n",
       " u'comic',\n",
       " u'comment',\n",
       " u'comp',\n",
       " u'compression',\n",
       " u'concept',\n",
       " u'confetti',\n",
       " u'contour',\n",
       " u'contrast',\n",
       " u'control',\n",
       " u'conv',\n",
       " u'convertible',\n",
       " u'cookie',\n",
       " u'cool',\n",
       " u'cooler',\n",
       " u'cora',\n",
       " u'coral',\n",
       " u'cord',\n",
       " u'corduroy',\n",
       " u'core',\n",
       " u'costume',\n",
       " u'cot',\n",
       " u'cotton',\n",
       " u'court',\n",
       " u'cov',\n",
       " u'cover',\n",
       " u'coverage',\n",
       " u'coverall',\n",
       " u'coverup',\n",
       " u'cowboy',\n",
       " u'cowl',\n",
       " u'cozy',\n",
       " u'crd',\n",
       " u'cre',\n",
       " u'cream',\n",
       " u'creep',\n",
       " u'creeper',\n",
       " u'crew',\n",
       " u'crewneck',\n",
       " u'crg',\n",
       " u'crgo',\n",
       " u'crinkle',\n",
       " u'criss',\n",
       " u'critter',\n",
       " u'cro',\n",
       " u'croc',\n",
       " u'crochet',\n",
       " u'crop',\n",
       " u'cropped',\n",
       " u'cross',\n",
       " u'crossbody',\n",
       " u'crown',\n",
       " u'crpr',\n",
       " u'crush',\n",
       " u'crvy',\n",
       " u'crw',\n",
       " u'crystal',\n",
       " u'ctn',\n",
       " u'ctr',\n",
       " u'cttn',\n",
       " u'cttw',\n",
       " u'cub',\n",
       " u'cudd',\n",
       " u'cuff',\n",
       " u'cuffed',\n",
       " u'cup',\n",
       " u'cupcake',\n",
       " u'cupid',\n",
       " u'curious',\n",
       " u'curvation',\n",
       " u'curvy',\n",
       " u'cush',\n",
       " u'cushion',\n",
       " u'cut',\n",
       " u'cute',\n",
       " u'cutie',\n",
       " u'cutout',\n",
       " u'cvc',\n",
       " u'cvo',\n",
       " u'cvy',\n",
       " u'dad',\n",
       " u'daisy',\n",
       " u'dance',\n",
       " u'dangle',\n",
       " u'daniel',\n",
       " u'danksin',\n",
       " u'danskin',\n",
       " u'dark',\n",
       " u'darth',\n",
       " u'date',\n",
       " u'david',\n",
       " u'dawson',\n",
       " u'daxx',\n",
       " u'day',\n",
       " u'daywear',\n",
       " u'dazzle',\n",
       " u'dbl',\n",
       " u'dead',\n",
       " u'deadpool',\n",
       " u'dear',\n",
       " u'deep',\n",
       " u'deer',\n",
       " u'degree',\n",
       " u'del',\n",
       " u'delete',\n",
       " u'demi',\n",
       " u'den',\n",
       " u'denim',\n",
       " u'derek',\n",
       " u'despicable',\n",
       " u'destructed',\n",
       " u'dew',\n",
       " u'dia',\n",
       " u'diamond',\n",
       " u'dickie',\n",
       " u'dicky',\n",
       " u'digital',\n",
       " u'dino',\n",
       " u'dinosaur',\n",
       " u'dip',\n",
       " u'dis',\n",
       " u'disney',\n",
       " u'distressed',\n",
       " u'diva',\n",
       " u'dme',\n",
       " u'dnm',\n",
       " u'doc',\n",
       " u'dog',\n",
       " u'doll',\n",
       " u'dollhouse',\n",
       " u'dolman',\n",
       " u'dolphin',\n",
       " u'dome',\n",
       " u'dont',\n",
       " u'dora',\n",
       " u'dorm',\n",
       " u'dory',\n",
       " u'dot',\n",
       " u'double',\n",
       " u'dragon',\n",
       " u'drape',\n",
       " u'drawstring',\n",
       " u'dre',\n",
       " u'dream',\n",
       " u'dreamfit',\n",
       " u'dres',\n",
       " u'dress',\n",
       " u'dresspant',\n",
       " u'dri',\n",
       " u'drop',\n",
       " u'drs',\n",
       " u'dsv',\n",
       " u'duck',\n",
       " u'duffel',\n",
       " u'duffle',\n",
       " u'dunlop',\n",
       " u'duo',\n",
       " u'duster',\n",
       " u'dye',\n",
       " u'dyed',\n",
       " u'dynasty',\n",
       " u'eagle',\n",
       " u'ear',\n",
       " u'earring',\n",
       " u'earth',\n",
       " u'easter',\n",
       " u'eastsport',\n",
       " u'easy',\n",
       " u'eboard',\n",
       " u'ecosmart',\n",
       " u'edge',\n",
       " u'elan',\n",
       " u'elastic',\n",
       " u'elbow',\n",
       " u'elena',\n",
       " u'elephant',\n",
       " u'elevate',\n",
       " u'elevated',\n",
       " u'elf',\n",
       " u'elmo',\n",
       " u'elsa',\n",
       " u'emb',\n",
       " u'embel',\n",
       " u'embellished',\n",
       " u'embellshed',\n",
       " u'embossed',\n",
       " u'embroidered',\n",
       " u'embroidery',\n",
       " u'emma',\n",
       " u'emoji',\n",
       " u'empire',\n",
       " u'endcap',\n",
       " u'energie',\n",
       " u'energy',\n",
       " u'envelope',\n",
       " u'epic',\n",
       " u'erika',\n",
       " u'escape',\n",
       " u'eso',\n",
       " u'essent',\n",
       " u'essential',\n",
       " u'esteem',\n",
       " u'eva',\n",
       " u'event',\n",
       " u'everyday',\n",
       " u'expectation',\n",
       " u'extend',\n",
       " u'extra',\n",
       " u'extreme',\n",
       " u'eye',\n",
       " u'eyelash',\n",
       " u'eyelet',\n",
       " u'fab',\n",
       " u'fabric',\n",
       " u'face',\n",
       " u'faded',\n",
       " u'fair',\n",
       " u'fairisle',\n",
       " u'fall',\n",
       " u'family',\n",
       " u'famous',\n",
       " u'fan',\n",
       " u'fancy',\n",
       " u'farm',\n",
       " u'fash',\n",
       " u'fashi',\n",
       " u'fashion',\n",
       " u'fast',\n",
       " u'father',\n",
       " u'faux',\n",
       " u'feather',\n",
       " u'fedora',\n",
       " u'female',\n",
       " u'fer',\n",
       " u'ffm',\n",
       " u'fge',\n",
       " u'fiber',\n",
       " u'fig',\n",
       " u'figure',\n",
       " u'filigree',\n",
       " u'finding',\n",
       " u'firm',\n",
       " u'fish',\n",
       " u'fit',\n",
       " u'fitspiration',\n",
       " u'fitted',\n",
       " u'fitting',\n",
       " u'fla',\n",
       " u'flag',\n",
       " u'flamingo',\n",
       " u'flannel',\n",
       " u'flap',\n",
       " u'flare',\n",
       " u'flash',\n",
       " u'flashing',\n",
       " u'flashlight',\n",
       " u'flat',\n",
       " u'flatback',\n",
       " u'flc',\n",
       " u'fld',\n",
       " u'fle',\n",
       " u'fleec',\n",
       " u'fleece',\n",
       " u'fleeceback',\n",
       " u'flex',\n",
       " u'flexees',\n",
       " u'flip',\n",
       " u'flirt',\n",
       " u'flnl',\n",
       " u'flop',\n",
       " u'floppy',\n",
       " u'floral',\n",
       " u'florida',\n",
       " u'flounce',\n",
       " u'flower',\n",
       " u'floyd',\n",
       " u'flt',\n",
       " u'flutter',\n",
       " u'flwr',\n",
       " u'fly',\n",
       " u'flyaway',\n",
       " u'fmg',\n",
       " u'foam',\n",
       " u'foil',\n",
       " u'fol',\n",
       " u'fold',\n",
       " u'folded',\n",
       " u'foot',\n",
       " u'football',\n",
       " u'footed',\n",
       " u'footless',\n",
       " u'footwear',\n",
       " u'force',\n",
       " u'ford',\n",
       " u'forever',\n",
       " u'foster',\n",
       " u'fotl',\n",
       " u'fox',\n",
       " u'frame',\n",
       " u'fred',\n",
       " u'free',\n",
       " u'freestyle',\n",
       " u'french',\n",
       " u'fresh',\n",
       " u'freshiq',\n",
       " u'friend',\n",
       " u'fringe',\n",
       " u'frnt',\n",
       " u'fro',\n",
       " u'frog',\n",
       " u'frozen',\n",
       " u'frt',\n",
       " u'fruit',\n",
       " u'frzn',\n",
       " u'fshn',\n",
       " u'ftl',\n",
       " u'fubu',\n",
       " u'fun',\n",
       " u'fur',\n",
       " u'fuzzy',\n",
       " u'game',\n",
       " u'gan',\n",
       " u'gar',\n",
       " u'garani',\n",
       " u'garanimal',\n",
       " u'garanimals',\n",
       " u'gauze',\n",
       " u'gear',\n",
       " u'gel',\n",
       " u'gen',\n",
       " u'generic',\n",
       " u'genevieve',\n",
       " u'genie',\n",
       " u'genuine',\n",
       " u'geo',\n",
       " u'george',\n",
       " u'gerber',\n",
       " u'ghost',\n",
       " u'ghostbusters',\n",
       " u'giant',\n",
       " u'gift',\n",
       " u'giftable',\n",
       " u'gifting',\n",
       " u'gil',\n",
       " u'gildan',\n",
       " u'gir',\n",
       " u'giraffe',\n",
       " u'girl',\n",
       " u'giselle',\n",
       " u'glam',\n",
       " u'glamour',\n",
       " u'glass',\n",
       " u'gld',\n",
       " u'glitter',\n",
       " u'glo',\n",
       " u'glory',\n",
       " u'glove',\n",
       " u'glow',\n",
       " u'glv',\n",
       " u'gmg',\n",
       " u'going',\n",
       " u'gold',\n",
       " u'goldtoe',\n",
       " u'golf',\n",
       " u'good',\n",
       " u'gore',\n",
       " u'gown',\n",
       " u'gra',\n",
       " u'grad',\n",
       " u'gradient',\n",
       " u'graduate',\n",
       " u'grandma',\n",
       " u'grant',\n",
       " u'grap',\n",
       " u'graph',\n",
       " u'graphi',\n",
       " u'graphic',\n",
       " u'gray',\n",
       " u'great',\n",
       " u'green',\n",
       " u'grey',\n",
       " u'grid',\n",
       " u'grinch',\n",
       " u'grl',\n",
       " u'grls',\n",
       " u'grn',\n",
       " u'grommet',\n",
       " u'group',\n",
       " u'grp',\n",
       " u'grph',\n",
       " u'grphc',\n",
       " u'grx',\n",
       " u'gry',\n",
       " u'guard',\n",
       " u'guy',\n",
       " u'gxp',\n",
       " u'gym',\n",
       " u'hacci',\n",
       " u'hair',\n",
       " u'half',\n",
       " u'hallmark',\n",
       " u'halloween',\n",
       " u'halo',\n",
       " u'halter',\n",
       " u'hand',\n",
       " u'handbag',\n",
       " u'hanes',\n",
       " u'hang',\n",
       " u'hangdown',\n",
       " u'hanging',\n",
       " u'happening',\n",
       " u'happy',\n",
       " u'hard',\n",
       " u'harley',\n",
       " u'harry',\n",
       " u'harve',\n",
       " u'hat',\n",
       " u'hatchi',\n",
       " u'hawaii',\n",
       " u'hawk',\n",
       " u'head',\n",
       " u'headband',\n",
       " u'headwear',\n",
       " u'headwrap',\n",
       " u'healthex',\n",
       " u'healthtex',\n",
       " u'heart',\n",
       " u'heat',\n",
       " u'heather',\n",
       " u'heathered',\n",
       " u'heatseal',\n",
       " u'heavenly',\n",
       " u'heel',\n",
       " u'hello',\n",
       " u'hem',\n",
       " u'henley',\n",
       " u'herman',\n",
       " u'hero',\n",
       " u'hfp',\n",
       " u'hibiscus',\n",
       " u'hicut',\n",
       " u'high',\n",
       " u'hiker',\n",
       " u'hill',\n",
       " u'hillary',\n",
       " u'hilo',\n",
       " u'hip',\n",
       " u'hipster',\n",
       " u'hipstr',\n",
       " u'hiwaist',\n",
       " u'hltx',\n",
       " u'hlwn',\n",
       " u'hng',\n",
       " u'hobo',\n",
       " u'hockey',\n",
       " u'hogan',\n",
       " u'holiday',\n",
       " u'hollywood',\n",
       " u'home',\n",
       " u'honey',\n",
       " u'hoo',\n",
       " u'hood',\n",
       " u'hooded',\n",
       " u'hoodie',\n",
       " u'hoodies',\n",
       " u'hoody',\n",
       " u'hook',\n",
       " u'hoop',\n",
       " u'hot',\n",
       " u'hour',\n",
       " u'hrt',\n",
       " u'hthr',\n",
       " u'htr',\n",
       " u'htx',\n",
       " u'hudson',\n",
       " u'hulk',\n",
       " u'humor',\n",
       " u'husky',\n",
       " u'hybrid',\n",
       " u'hyi',\n",
       " u'ice',\n",
       " u'iceburg',\n",
       " u'icon',\n",
       " u'igloo',\n",
       " u'iii',\n",
       " u'iml',\n",
       " u'impact',\n",
       " u'inch',\n",
       " u'indigo',\n",
       " u'industry',\n",
       " u'inf',\n",
       " u'infant',\n",
       " u'infinity',\n",
       " u'initial',\n",
       " u'inseam',\n",
       " u'inset',\n",
       " u'inspire',\n",
       " u'inspired',\n",
       " u'insulated',\n",
       " u'int',\n",
       " u'interceptor',\n",
       " u'interlock',\n",
       " u'intimate',\n",
       " u'intl',\n",
       " u'iron',\n",
       " u'island',\n",
       " u'isotoner',\n",
       " u'itb',\n",
       " u'item',\n",
       " u'itg',\n",
       " u'ixtreme',\n",
       " u'jac',\n",
       " u'jack',\n",
       " u'jacke',\n",
       " u'jacket',\n",
       " u'jacq',\n",
       " u'jacquard',\n",
       " u'jam',\n",
       " u'jane',\n",
       " u'jason',\n",
       " u'jckt',\n",
       " u'jea',\n",
       " u'jean',\n",
       " u'jeg',\n",
       " u'jegg',\n",
       " u'jegging',\n",
       " u'jeggings',\n",
       " u'jelly',\n",
       " u'jersey',\n",
       " u'jerzees',\n",
       " u'jet',\n",
       " u'jewel',\n",
       " u'jewelry',\n",
       " u'jillian',\n",
       " u'jkt',\n",
       " u'jms',\n",
       " u'jockey',\n",
       " u'jog',\n",
       " u'jogger',\n",
       " u'jordache',\n",
       " u'jpr',\n",
       " u'jrsy',\n",
       " u'jumper',\n",
       " u'junior',\n",
       " u'jurassic',\n",
       " u'justice',\n",
       " u'karma',\n",
       " u'kate',\n",
       " u'kaye',\n",
       " u'keepsake',\n",
       " u'key',\n",
       " u'keychain',\n",
       " u'keyhole',\n",
       " u'keyring',\n",
       " u'khaki',\n",
       " u'khole',\n",
       " u'kid',\n",
       " u'kimono',\n",
       " u'king',\n",
       " u'kiss',\n",
       " u'kitty',\n",
       " u'kiwi',\n",
       " u'knee',\n",
       " u'knit',\n",
       " u'knkt',\n",
       " u'knot',\n",
       " u'kodiak',\n",
       " u'komar',\n",
       " u'komen',\n",
       " u'label',\n",
       " u'lac',\n",
       " u'lace',\n",
       " u'laceup',\n",
       " u'lad',\n",
       " u'ladie',\n",
       " u'lady',\n",
       " u'lag',\n",
       " u'laguna',\n",
       " u'lamaze',\n",
       " u'landau',\n",
       " u'lanyard',\n",
       " u'large',\n",
       " u'laser',\n",
       " u'lattice',\n",
       " u'laundry',\n",
       " u'layer',\n",
       " u'layered',\n",
       " u'layering',\n",
       " u'lcd',\n",
       " u'lce',\n",
       " u'lds',\n",
       " u'lead',\n",
       " u'leading',\n",
       " u'leaf',\n",
       " u'league',\n",
       " u'leather',\n",
       " u'led',\n",
       " u'lee',\n",
       " u'leg',\n",
       " u'legacy',\n",
       " u'legend',\n",
       " u'legg',\n",
       " u'leggi',\n",
       " u'leggin',\n",
       " u'legging',\n",
       " u'leggs',\n",
       " u'lego',\n",
       " u'lei',\n",
       " u'leigh',\n",
       " u'lemondrop',\n",
       " u'length',\n",
       " u'lens',\n",
       " u'leo',\n",
       " u'leopard',\n",
       " u'leotard',\n",
       " u'levi',\n",
       " u'liberty',\n",
       " u'lic',\n",
       " u'license',\n",
       " u'licensed',\n",
       " u'life',\n",
       " u'lifestyle',\n",
       " u'lift',\n",
       " u'light',\n",
       " u'lightweight',\n",
       " u'like',\n",
       " u'lil',\n",
       " u'lilly',\n",
       " u'lime',\n",
       " u'limited',\n",
       " u'line',\n",
       " u'linear',\n",
       " u'lined',\n",
       " u'linen',\n",
       " u'liner',\n",
       " u'link',\n",
       " u'lion',\n",
       " u'lite',\n",
       " u'little',\n",
       " u'lnr',\n",
       " u'local',\n",
       " u'logo',\n",
       " ...]"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_p2 #saves the final features_list for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dense_term_freq_p2=get_dense_matrix(term_frq_p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP LEARNING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense\n",
    "from keras.objectives import categorical_crossentropy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dropout\n",
    "np.random.seed(0)\n",
    "deep_model=Sequential()\n",
    "#Dense layer\n",
    "from keras.layers import Dense,Activation\n",
    "deep_model.add(Dense(1000,input_shape=(2000,),activation='relu'))\n",
    "deep_model.add(Dropout(0.25))\n",
    "deep_model.add(Dense(800,activation='relu'))\n",
    "deep_model.add(Dropout(0.25))\n",
    "deep_model.add(Dense(400,activation='relu'))\n",
    "deep_model.add(Dropout(0.25))\n",
    "deep_model.add(Dense(200,activation='relu'))\n",
    "deep_model.add(Dropout(0.25))\n",
    "deep_model.add(Dense(100,activation='relu'))\n",
    "deep_model.add(Dense(49 ,activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 1000)              2001000   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 800)               800800    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 400)               320400    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 49)                4949      \n",
      "=================================================================\n",
      "Total params: 3,227,449\n",
      "Trainable params: 3,227,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model compilation\n",
    "from keras.optimizers import SGD\n",
    "#model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])\n",
    "deep_model.summary()\n",
    "deep_model.compile(loss='categorical_crossentropy',optimizer=SGD(),metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#len(np.unique(y_coded)\n",
    "\n",
    "#converting the y to be given as an input\n",
    "from keras.utils import to_categorical\n",
    "cat_y=to_categorical(y_coded, num_classes=49)\n",
    "\n",
    "\n",
    "#x_scaled=sklearn.preprocessing.scale(x, axis=0, with_mean=True, with_std=True, copy=True)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(dense_term_freq_p2, cat_y, test_size=0.3, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 325494 samples, validate on 139498 samples\n",
      "Epoch 1/15\n",
      "325494/325494 [==============================] - 226s 694us/step - loss: 1.6595 - acc: 0.5761 - val_loss: 0.8098 - val_acc: 0.7664\n",
      "Epoch 2/15\n",
      "325494/325494 [==============================] - 217s 668us/step - loss: 0.7746 - acc: 0.7767 - val_loss: 0.5523 - val_acc: 0.8366\n",
      "Epoch 3/15\n",
      "325494/325494 [==============================] - 219s 673us/step - loss: 0.5888 - acc: 0.8261 - val_loss: 0.4559 - val_acc: 0.8643\n",
      "Epoch 4/15\n",
      "325494/325494 [==============================] - 220s 676us/step - loss: 0.4938 - acc: 0.8529 - val_loss: 0.3992 - val_acc: 0.8800\n",
      "Epoch 5/15\n",
      "325494/325494 [==============================] - 232s 713us/step - loss: 0.4337 - acc: 0.8697 - val_loss: 0.3650 - val_acc: 0.8918\n",
      "Epoch 6/15\n",
      "325494/325494 [==============================] - 233s 714us/step - loss: 0.3921 - acc: 0.8810 - val_loss: 0.3406 - val_acc: 0.8985\n",
      "Epoch 7/15\n",
      "325494/325494 [==============================] - 232s 712us/step - loss: 0.3594 - acc: 0.8900 - val_loss: 0.3208 - val_acc: 0.9047\n",
      "Epoch 8/15\n",
      "325494/325494 [==============================] - 232s 712us/step - loss: 0.3337 - acc: 0.8978 - val_loss: 0.3071 - val_acc: 0.9092\n",
      "Epoch 9/15\n",
      "325494/325494 [==============================] - 232s 712us/step - loss: 0.3125 - acc: 0.9037 - val_loss: 0.3017 - val_acc: 0.9105\n",
      "Epoch 10/15\n",
      "325494/325494 [==============================] - 232s 712us/step - loss: 0.2953 - acc: 0.9090 - val_loss: 0.2887 - val_acc: 0.9153\n",
      "Epoch 11/15\n",
      "325494/325494 [==============================] - 235s 721us/step - loss: 0.2817 - acc: 0.9125 - val_loss: 0.2819 - val_acc: 0.9167\n",
      "Epoch 12/15\n",
      "325494/325494 [==============================] - 232s 712us/step - loss: 0.2674 - acc: 0.9169 - val_loss: 0.2774 - val_acc: 0.9195\n",
      "Epoch 13/15\n",
      "325494/325494 [==============================] - 233s 716us/step - loss: 0.2554 - acc: 0.9202 - val_loss: 0.2749 - val_acc: 0.9208\n",
      "Epoch 14/15\n",
      "325494/325494 [==============================] - 226s 695us/step - loss: 0.2467 - acc: 0.9225 - val_loss: 0.2694 - val_acc: 0.9218\n",
      "Epoch 15/15\n",
      "325494/325494 [==============================] - 266s 819us/step - loss: 0.2372 - acc: 0.9255 - val_loss: 0.2648 - val_acc: 0.9246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aa9b52e90>"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=1,min_delta=0)\n",
    "deep_model.fit(X_train,Y_train, epochs=15, batch_size = 25, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLLECTING TWEET DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=open('/Users/s0c01xa/Documents/Walmart_Bplan/Gen_Tweet.csv')\n",
    "tweet_data=f.readline()\n",
    "tweet=word_tokenize(tweet_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING TWEET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_tweet(document):\n",
    "    val_test=''\n",
    "    for i in range(0,len(document)):\n",
    "        val=document[i]\n",
    "        #print val\n",
    "        val_test=str(val+\" \"+val_test)\n",
    "    return val_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweet=get_processed_tweet(tweet)\n",
    "#processed_tweet saves the tweet data to be transformed to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMING TWEET DATA1 AS PER TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweet=term_freq_model.transform([processed_tweet])\n",
    "dense_tweet=get_dense_matrix(test_tweet)\n",
    "#test_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting through DEEP MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.85574217e-15,   1.75338192e-14,   2.47476413e-14,\n",
       "          1.49912418e-13,   4.56828097e-13,   7.66417474e-13,\n",
       "          1.40034772e-12,   1.67643471e-12,   4.09801403e-12,\n",
       "          8.56795017e-12,   1.17137914e-11,   2.41491931e-11,\n",
       "          2.85085375e-11,   1.18750926e-10,   2.04804312e-10,\n",
       "          4.14886403e-10,   4.15999735e-10,   4.26703950e-10,\n",
       "          8.70874539e-10,   1.14058685e-09,   1.92986205e-09,\n",
       "          2.32229680e-09,   2.82150969e-09,   3.90272126e-09,\n",
       "          6.83731072e-09,   8.00512989e-09,   3.55730805e-08,\n",
       "          3.74500289e-08,   3.78442415e-08,   4.62133123e-08,\n",
       "          1.28327599e-07,   1.49903599e-07,   5.08321989e-07,\n",
       "          8.69713972e-07,   1.34079255e-06,   3.98417842e-06,\n",
       "          4.18923582e-06,   5.98170755e-06,   1.31455909e-05,\n",
       "          5.84939480e-05,   8.02198265e-05,   9.30533424e-05,\n",
       "          1.74047920e-04,   4.15659131e-04,   5.99148800e-04,\n",
       "          7.33702618e-04,   9.86952684e-04,   3.98322791e-01,\n",
       "          5.98505557e-01]], dtype=float32)"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(deep_model.predict_proba(dense_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['School', 'Shorts', 'Pants', 'Jackets', 'Dancewear', 'Capri',\n",
       "        'Missy', 'Sweaters', 'Slippers and sandals', 'Casual', 'Woven',\n",
       "        'Skirts', 'Hawaii', 'Dress', 'Rain', 'Outerwear', 'No Descriptiom',\n",
       "        'Holiday Collection', 'Jeans', 'Workwear', 'Activewear',\n",
       "        'Sets and Separates', 'Tights and leggings', 'Knit ', 'Thermals',\n",
       "        'Kids', 'Shoes and boots', 'Sports and athletics', 'Puerto Rico',\n",
       "        'Maternity', 'Gifts', 'Beachwear', 'Seasonal', 'Swimwear',\n",
       "        'Daywear', 'Alaska', 'Fleece', 'Socks', 'Head and hand',\n",
       "        'Accessories', 'Baby', 'Others', 'Promotions', 'Jewelry',\n",
       "        'Sleepwear', 'Tops', 'Undergarments', 'Licensed', 'Tshirts']], dtype=object)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(np.argsort(deep_model.predict_proba(dense_tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/Users/s0c01xa/Documents/Walmart_Bplan/Collection of tweets/Gen_tweet_2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWEET DATA2 AS PER TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('/Users/s0c01xa/Documents/Walmart_Bplan/Gen_Tweet_3.csv')\n",
    "tweet_data=f.readline()\n",
    "tweet=word_tokenize(tweet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_tweet=get_processed_tweet(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tweet=term_freq_model.transform([processed_tweet])\n",
    "dense_tweet=get_dense_matrix(test_tweet)\n",
    "#test_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting through DEEP MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Seasonal', 'Promotions', 'Tops', 'Tshirts', 'Fleece', 'Jackets',\n",
       "        'Activewear', 'Knit ', 'Beachwear', 'Sports and athletics', 'Rain',\n",
       "        'Slippers and sandals', 'Hawaii', 'Thermals', 'Licensed',\n",
       "        'Holiday Collection', 'Alaska', 'Woven', 'Sweaters', 'Sleepwear',\n",
       "        'Swimwear', 'Capri', 'School', 'Pants', 'Dancewear', 'Maternity',\n",
       "        'Outerwear', 'Jewelry', 'Sets and Separates', 'Puerto Rico',\n",
       "        'Head and hand', 'Skirts', 'No Descriptiom', 'Tights and leggings',\n",
       "        'Daywear', 'Shorts', 'Kids', 'Accessories', 'Undergarments',\n",
       "        'Baby', 'Gifts', 'Dress', 'Socks', 'Workwear', 'Missy', 'Casual',\n",
       "        'Others', 'Shoes and boots', 'Jeans']], dtype=object)"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(np.argsort(deep_model.predict_proba(dense_tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.59098713e-13,   1.78963886e-12,   5.76090477e-12,\n",
       "          8.24352565e-12,   1.29102709e-11,   1.33263895e-11,\n",
       "          1.60772697e-11,   2.52703830e-11,   1.37723874e-10,\n",
       "          2.08949830e-10,   3.39005768e-10,   4.47385212e-10,\n",
       "          6.32806418e-10,   7.45949635e-10,   9.84210380e-10,\n",
       "          4.49576998e-09,   4.70888528e-09,   5.63766855e-09,\n",
       "          8.01433231e-09,   8.27839131e-09,   1.40689176e-08,\n",
       "          1.55853215e-08,   2.61976112e-08,   3.95402004e-08,\n",
       "          4.38656294e-08,   6.70585223e-08,   9.91609070e-08,\n",
       "          1.07136330e-07,   1.16919473e-07,   1.69258485e-07,\n",
       "          3.41929507e-07,   4.23379362e-07,   5.62335856e-07,\n",
       "          5.69025246e-07,   7.04521767e-07,   7.70824215e-07,\n",
       "          1.24663086e-06,   2.20076276e-06,   4.09988206e-06,\n",
       "          4.88570777e-06,   5.98651104e-06,   6.95289691e-06,\n",
       "          7.89485057e-06,   1.03705454e-04,   1.81588999e-04,\n",
       "          2.61140260e-04,   1.49977754e-03,   3.73394135e-03,\n",
       "          9.94182527e-01]], dtype=float32)"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(deep_model.predict_proba(dense_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Once the particular cluster is being refered to,we have to see all the items belonging to document wise.\n",
    "#Work is to find out in the particular group which of the items "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling Back the data with cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Combined Description</th>\n",
       "      <th>CATEGORY_DESCRIPTION</th>\n",
       "      <th>Social Intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RHD AMERICA NECK 18\"IRH AMERICA NECK SPINNER 1...</td>\n",
       "      <td>JEWELRY COSTUME</td>\n",
       "      <td>Jewelry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NB 6PK LOW CUT NB 6PK LOW CUT NB 6PK LOW CUT P...</td>\n",
       "      <td>CASUAL SOCKS</td>\n",
       "      <td>Socks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SWEATSHIRTS ONLINE ONLY DSV GILDAN MENS CREWNE...</td>\n",
       "      <td>DOTCOM (D23)</td>\n",
       "      <td>Others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SCRUB OUTFITS ONLINE ONLY DSV FEELIN' FELINE S...</td>\n",
       "      <td>SCRUBS</td>\n",
       "      <td>Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOL 6PK SEAMLESS LRB FOL BRF 6P AST 7 FOL 6PK ...</td>\n",
       "      <td>PACKAGED UNDERWEAR</td>\n",
       "      <td>Undergarments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18 HR BREATH BEAUTY 4716B WHITE 44D PLAYTEX 18...</td>\n",
       "      <td>FULL FIGURE</td>\n",
       "      <td>Undergarments</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Combined Description CATEGORY_DESCRIPTION  \\\n",
       "0  RHD AMERICA NECK 18\"IRH AMERICA NECK SPINNER 1...      JEWELRY COSTUME   \n",
       "1  NB 6PK LOW CUT NB 6PK LOW CUT NB 6PK LOW CUT P...         CASUAL SOCKS   \n",
       "2  SWEATSHIRTS ONLINE ONLY DSV GILDAN MENS CREWNE...         DOTCOM (D23)   \n",
       "3  SCRUB OUTFITS ONLINE ONLY DSV FEELIN' FELINE S...               SCRUBS   \n",
       "4  FOL 6PK SEAMLESS LRB FOL BRF 6P AST 7 FOL 6PK ...   PACKAGED UNDERWEAR   \n",
       "5  18 HR BREATH BEAUTY 4716B WHITE 44D PLAYTEX 18...          FULL FIGURE   \n",
       "\n",
       "   Social Intent  \n",
       "0        Jewelry  \n",
       "1          Socks  \n",
       "2         Others  \n",
       "3    Accessories  \n",
       "4  Undergarments  \n",
       "5  Undergarments  "
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_clust_labels=pd.read_csv('/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels.csv')\n",
    "data_with_clust_labels.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''document_1_text = 'This is document one'\n",
    "document_2_text = 'This is document two'\n",
    "\n",
    "document_1_words = word_tokenize(document_1_text)\n",
    "document_2_words = document_2_text.split()\n",
    "common = set(document_1_words).intersection( set(document_2_words) )'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filtered_data saves the reccommended basket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILTERING THE DOCUMENTS BASED ON THE MOST SIMILAR CLUSTER\n",
    "df=data_with_clust_labels\n",
    "filtered_data=df[df['Social Intent']=='Jeans']\n",
    "doc_basket=filtered_data['Combined Description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING doc_basket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_basket1=get_tokenized_doc(doc_basket)\n",
    "doc_basket2=get_lower_doc(doc_basket1)\n",
    "doc_basket3=get_stopwrd_free_doc(doc_basket2)\n",
    "doc_basket4=get_alpha_doc(doc_basket3)\n",
    "doc_basket5=get_lem_doc(doc_basket4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_basket5 has the list of items with whom the tweet is compared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWEET DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_lem=get_lem_doc(tweet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPARISON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similarity_basket=[]\n",
    "for i in range(0,len(doc_basket5)):\n",
    "    common=len(set(doc_basket5[i]).intersection( set(tweet)))\n",
    "    similarity_basket.append(common)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0, 1910, 4566, ..., 6997, 5338, 1061])"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(similarity_basket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FG BOOT CUT JEAN WRANGLER BOYS BOOTCUT JEAN FADED GLORY'"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_basket[1061]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
