{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import inflection\n",
    "\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test and Train Data\n",
    "train = pd.read_csv(\"/Users/s0c02nj/Desktop/Innoplexus-AV/train_F3WbcTw.csv\",encoding=\"utf-8\")\n",
    "test = pd.read_csv(\"/Users/s0c02nj/Desktop/Innoplexus-AV/test_tOlRoBf.csv\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_diff = set(test['drug']) - set(train['drug'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['drug'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test['drug'] == 'afainib'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_test = {}\n",
    "for i in list(set_diff):\n",
    "    dict_test[i] = test[test['drug'] == i].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical Variable ---> Drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.Label Encoding the Categorical Varaible----TransactionType\n",
    "train_copy = train.drop(['sentiment'],axis=1)\n",
    "y_lab = train['sentiment']\n",
    "df_join = train_copy.append(test,sort=False)\n",
    "\n",
    "#Label enncoding combined\n",
    "le = LabelEncoder()\n",
    "df_join['drug'] = le.fit_transform(df_join['drug'])\n",
    "\n",
    "#Diving test and train\n",
    "df_train = df_join[0:5279]\n",
    "df_test = df_join[5279:]\n",
    "\n",
    "#Getting the cat_col\n",
    "drug_train = df_train['drug']\n",
    "drug_test =  df_test['drug']\n",
    "\n",
    "#No of category_count\n",
    "count_drug = len(df_join['drug'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat = to_categorical(y_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_join.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    #d = enchant.Dict(\"en_US\")\n",
    "    text= text.lower()\n",
    "    text= re.sub(r'[^a-z]',' ',text)\n",
    "    text= \" \".join([s for s in text.split() if len(s)>2])\n",
    "    #text = drop_duplicates(text)\n",
    "    text= \" \".join([x for x in text.split() if x not in stopwords.words('english')])\n",
    "    text= \" \".join([inflection.singularize(x) for x in text.split()])\n",
    "    text= ' '.join(text.split())\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text_clean'] = train['text'].apply(lambda x:text_preprocessing(x))\n",
    "test['text_clean'] =  test['text'].apply(lambda x:text_preprocessing(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing the text\n",
    "max_features =80000\n",
    "tokenizer = Tokenizer(lower = True, filters='', num_words=max_features)\n",
    "full_text = list(train['text_clean'].values) + list(test['text_clean'].values)\n",
    "tokenizer.fit_on_texts(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing the documents---- convert to strings\n",
    "train_tokenized = tokenizer.texts_to_sequences(train['text_clean'].fillna('missing'))\n",
    "test_tokenized =  tokenizer.texts_to_sequences(test['text_clean'].fillna('missing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding the same\n",
    "max_len = 200\n",
    "X_train = pad_sequences(train_tokenized, maxlen = max_len,padding='pre')\n",
    "X_test =  pad_sequences(test_tokenized, maxlen = max_len,padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_index is dictionary of the words and the sequence\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path= '/Users/s0c02nj/Downloads/glove.6B/glove.6B.50d.txt'\n",
    "f=open(folder_path)\n",
    "doc=f.readlines()\n",
    "#****VIP\n",
    "#**WORD TO VEC DICTIONARY\n",
    "#Forming a dictionary-word2vec\n",
    "word2vec={}\n",
    "key=[]\n",
    "#looping though the doc.in the doc the entire thing is saved and is separated by a space bar.\n",
    "for line in doc:\n",
    "    #parts contains every word separately for doc1\n",
    "    parts=line.split(' ')\n",
    "    #part[0] contains the word\n",
    "    word=parts[0]\n",
    "    key.append(word)\n",
    "    #embed contains the vector\n",
    "    embed=np.array(parts[1:],dtype='float32')\n",
    "    #filling up the dictionary\n",
    "    word2vec[word]=embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding matrix creation\n",
    "nb_words = min(max_features, len(word_index)+1)\n",
    "embedding_matrix = np.zeros((nb_words, 50))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    #print i\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    if word in word2vec:\n",
    "        embedding_vector = word2vec[word]\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BiLsTM_ATTention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_attn_bilstm():\n",
    "    \n",
    "    #Defining the input-----> Transaction\n",
    "    inputs1 = Input(shape=(1,))\n",
    "    layer_drug = Embedding(count_drug , 70 ,input_length=1)(inputs1)\n",
    "    layer_drug = Flatten()(layer_drug)\n",
    "    \n",
    "    \n",
    "    ################################################################################# ---> LSTM\n",
    "    \n",
    "    #Defining the input for the text\n",
    "    \n",
    "    inputs2 = Input(shape=(max_len,))\n",
    "    layer =  Embedding(38146 ,50,input_length=max_len,trainable=False,weights = [embedding_matrix])(inputs2)\n",
    "    layer =  Bidirectional(LSTM(64,return_sequences=True))(layer)\n",
    "    \n",
    "    #Attention\n",
    "    activations_weights = Dense(1, activation='tanh')(layer)\n",
    "    activations_weights = Flatten()(activations_weights)\n",
    "    activations_weights = Activation('softmax')(activations_weights)\n",
    "    activations_weights = RepeatVector(128)(activations_weights)\n",
    "    activations_weights = Permute([2, 1])(activations_weights)\n",
    "    activations_weighted = multiply([layer, activations_weights])\n",
    "    sent_representation = Lambda(lambda x: K.sum(x, axis=-2))(activations_weighted)\n",
    "    \n",
    "    \n",
    "    #Concatenating\n",
    "    layer_sentiment = concatenate([sent_representation,layer_drug],axis=1)\n",
    "    \n",
    "    #Dense Layer\n",
    "    layer_sentiment= Dense(30, activation='tanh')(layer_sentiment)\n",
    "    \n",
    "    #Output Layer\n",
    "    probabilities = Dense(3,activation='softmax')(layer_sentiment)\n",
    "\n",
    "    model = Model(inputs=[inputs1,inputs2],outputs=probabilities)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_lstm = model_attn_bilstm()\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.compile(loss = \"categorical_crossentropy\", optimizer = Adam(0.009), metrics = [f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class weights\n",
    "class_weight = {0: 8.,\n",
    "                1: 8.,\n",
    "                2: 1.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model_lstm.fit([drug_train,\n",
    "                          X_train] ,\n",
    "                          y_cat, \n",
    "                          batch_size = 512, \n",
    "                          epochs = 6, \n",
    "                          validation_split=0.1,\n",
    "                          class_weight=class_weight,\n",
    "                          verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_lstm.predict([drug_test,\n",
    "                          X_test], \n",
    "                          verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class = pred.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(pred_class[pred_class == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('/Users/s0c02nj/Desktop/Innoplexus-AV/sample_submission_i5xnIZD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['sentiment'] = pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('/Users/s0c02nj/Desktop/Innoplexus-AV/BiLSTM_vader.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
