{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IMPORTING \n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "from sklearn import linear_model\n",
    "import sklearn.cross_validation\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import ensemble\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLLECTING APPAREL TRAINING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "app_data=pd.read_csv('/Users/s0c01xa/Documents/Walmart_Bplan/train_data_apparel.csv')\n",
    "app_data=np.array(app_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DATA PREPROCESSING FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TOKENIZING THE DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tokenized_doc(document):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokenized_doc=[]\n",
    "    for i in range(0,len(document)):\n",
    "        word=word_tokenize(document[i])\n",
    "        tokenized_doc.append(word)\n",
    "    \n",
    "    return tokenized_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONVERTING TO LOWERCASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting all words to lower case so that it is of same type at par with word to vec\n",
    "def get_lower_doc(document):\n",
    "    low_words=[]\n",
    "    low_doc=[]\n",
    "    for i in range(len(document)):\n",
    "        for word in document[i]:\n",
    "            lwords=word.lower()\n",
    "            low_words.append(lwords)\n",
    "        low_doc.append(low_words)\n",
    "        low_words=[]\n",
    "    return low_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMOVING STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given a document,remove the stopwords\n",
    "def get_stopwrd_free_doc(document):\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stpwrd_free_doc = []\n",
    "    doc=[]\n",
    "\n",
    "    for i in range (len(document)):\n",
    "        for w in document[i]:\n",
    "            if w not in stop_words:\n",
    "                doc.append(w)\n",
    "        stpwrd_free_doc.append(doc)\n",
    "        doc=[]\n",
    "    \n",
    "    return stpwrd_free_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMOVING PUNCTUATIONS AND NON-ALPHABETIC DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Removing non alpha words\n",
    "def get_alpha_doc(document):\n",
    "    alpha_doc=[]\n",
    "    alpha=[]\n",
    "    for i in range(len(document)):\n",
    "        for word in document[i]:\n",
    "            if (word.isalpha()==True):\n",
    "                alpha.append(word)\n",
    "        alpha_doc.append(alpha)\n",
    "        alpha=[]\n",
    "        \n",
    "    return alpha_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEMMATIZING AND STEMMING THE DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lem_doc(document):\n",
    "    from nltk import WordNetLemmatizer\n",
    "    from nltk.corpus import wordnet\n",
    "    wnl=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "    lem_words=[]\n",
    "    lem_doc=[]\n",
    "\n",
    "    for i in range(len(document)):\n",
    "        for word in document[i]:\n",
    "            lem=wnl.lemmatize(word)\n",
    "            lem_words.append(lem)\n",
    "        \n",
    "        lem_doc.append(lem_words)\n",
    "        lem_words=[]\n",
    "\n",
    "\n",
    "    return lem_doc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMOVING SMALL LETTER WORDS FROM DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Removing small (2) letter words\n",
    "def get_long_wrd_doc(document):\n",
    "    long_wrd_doc=[]\n",
    "    import re\n",
    "    shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "\n",
    "    for i in range(0,len(document)):\n",
    "        reg=[shortword.sub(' ', str(document[i]))]\n",
    "        long_wrd_doc.append(reg)\n",
    "    return long_wrd_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering the Most COMMON WORDS FROM THE LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Keeping only top frequen words\n",
    "def get_most_common(document):\n",
    "    from collections import Counter\n",
    "    count_m=[]\n",
    "    most_common=[]\n",
    "\n",
    "    for i in range(0,len(document)):\n",
    "        count=Counter(document[i]).most_common(120)\n",
    "        for j in count:\n",
    "            count_m.append(j[0])\n",
    "        most_common.append(count_m)   \n",
    "        count_m=[]\n",
    "    \n",
    "    \n",
    "    return most_common\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GETTING STRING VERSION OF DOC FOR TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_string_version(document):\n",
    "    Str_data=[]\n",
    "    for i in range(0,len(document)):\n",
    "        val=str(document[i])\n",
    "        Str_data.append(val)\n",
    "    return Str_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGNORES ANY CHARACTER WHICH IS NOT utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_udoc(document):\n",
    "    utf_doc=[]\n",
    "    for i in range(0,len(document)):\n",
    "        val=unicode(x[i],errors=\"ignore\")\n",
    "        utf_doc.append(val)\n",
    "\n",
    "    return utf_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOVE WORD2VEC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_path= '/Users/s0c01xa/Downloads/glove.6B/glove.6B.50d.txt'\n",
    "f=open(folder_path)\n",
    "doc=f.readlines()\n",
    "#****VIP\n",
    "#**WORD TO VEC DICTIONARY\n",
    "#Forming a dictionary-word2vec\n",
    "word2vec={}\n",
    "key=[]\n",
    "#looping though the doc.in the doc the entire thing is saved and is separated by a space bar.\n",
    "for line in doc:\n",
    "    #parts contains every word separately for doc1\n",
    "    parts=line.split(' ')\n",
    "    #part[0] contains the word\n",
    "    word=parts[0]\n",
    "    key.append(word)\n",
    "    #embed contains the vector\n",
    "    embed=np.array(parts[1:],dtype='float32')\n",
    "    #filling up the dictionary\n",
    "    word2vec[word]=embed\n",
    "\n",
    "#NOW IF i give word as the key, the corresponding vec rep will return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# APPAREL DATA CONSISTS OF CATEGORY DESCRIPTION AND THE ITEM DESCRIPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apparel_data=pd.read_csv('/Users/s0c01xa/Documents/Walmart_Bplan/train_data_apparel.csv')\n",
    "app_data=np.array(app_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BELOW STEP CONCATENATES ALL THE ITEM DESCRIPTIONS CORR TO EACH CATEGORY DESCRIPTION IN COMB_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#uniq_labels contain the names of the 332 category description\n",
    "uniq_labels=np.unique(app_data[:,1])\n",
    "\n",
    "#Labels contain item wise category description\n",
    "labels=app_data[:,1]\n",
    "\n",
    "\n",
    "\n",
    "item2cat={}\n",
    "cat=[]\n",
    "#looping though the doc.in the doc the entire thing is saved and is separated by a space bar.\n",
    "\n",
    "for i in range(0,len(uniq_labels)):\n",
    "    app=app_data[labels==uniq_labels[i]][:,0]\n",
    "    unq_lbl=uniq_labels[i]\n",
    "    cat.append(unq_lbl)\n",
    "    item2cat[unq_lbl]=app\n",
    "\n",
    "#comb_data contains all the concatenated information based on category description and item desc\n",
    "#uniq_labels contains the corr. labels\n",
    "comb_data=[]\n",
    "for i in range(0,len(uniq_labels)):\n",
    "    comb_data.append(str(item2cat[uniq_labels[i]]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PRE_PROCESSING FOR DESCRIPTION AND KEEPING TOP 120 WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc0=get_tokenized_doc(comb_data)\n",
    "doc1=get_lower_doc(doc0)\n",
    "doc2=get_stopwrd_free_doc(doc1)\n",
    "doc3=get_alpha_doc(doc2)\n",
    "doc4=get_most_common(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#doc4 saves the preprocessed description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE_PROCESSING UNIQUE LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lab0=get_tokenized_doc(uniq_labels)\n",
    "lab1=get_lower_doc(lab0)\n",
    "lab2=get_stopwrd_free_doc(lab1)\n",
    "lab3=get_alpha_doc(lab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lab3 saves the preprocessed unique labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEIGHTED COMBINING THE DESCRIPTION AND THE UNIQUE LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_combined_doc(description,labels):\n",
    "    w=10\n",
    "    comb_doc=[]\n",
    "    for i in range(0,len(description)):\n",
    "        comb_doc.append(labels[i]*w+description[i])\n",
    "\n",
    "    return comb_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_doc=get_combined_doc(doc4,lab3)  #combinded_doc saves the combined description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL PREPROCESSING OF COMBINED DOCUMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_doc1=get_long_wrd_doc(combined_doc)\n",
    "data=combined_doc1\n",
    "data_string=get_string_version(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data goes for tfidf and further process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=sklearn.feature_extraction.text.CountVectorizer(input='content', encoding='utf-8', decode_error='strict', \n",
    "                                            strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, \n",
    "                                            stop_words='english', ngram_range=(1,1), analyzer='word', \n",
    "                                            max_df=0.99, min_df=0.001, max_features=3000, \n",
    "                                            vocabulary=None, binary=False)\n",
    "\n",
    "#Transforming the data to count of token\n",
    "term_frq=model.fit_transform(data_string)\n",
    "#features_names\n",
    "features=model.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse to Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Returns dense matrix\n",
    "def get_dense_matrix(term_frquency):\n",
    "    from scipy.sparse import csr_matrix\n",
    "    dense_trm_frq=term_frquency.todense()\n",
    "\n",
    "    return dense_trm_frq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dense_term_freq=get_dense_matrix(term_frq) #saves the dense representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncated Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the SVD MATRIX\n",
    "def get_svd(matrix):\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    svd = TruncatedSVD(n_components=500, n_iter=50, random_state=42)\n",
    "    lsa=svd.fit_transform(matrix)  \n",
    "\n",
    "    return lsa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd_matrix=get_svd(dense_term_freq)  #saves the svd vector for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMILARITY BETWEEN DOCUMENTS BASED ON TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cosine_similarity(matrix):\n",
    "    from sklearn.metrics.pairwise import cosine_similarity as cs\n",
    "    #sim_mat=np.zeros(332,332)\n",
    "    similarity_matrix=cs(matrix)\n",
    "    return similarity_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim_mat=get_cosine_similarity(svd_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKING THE SIMILARITY OF DOCUMENTS VISUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sim_mat=pd.DataFrame(sim_mat)  #Stores the similarity amongst the documents\n",
    "#uniq_labels[np.argsort(sim_mat[23])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#uniq_labels[np.argsort(sim_mat[23])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIERARCHICAL CLUSTERING THE DOCUMENTS BASED ON SVD MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4                  ACCESSORIES\n",
       "5            ACCESSORIES (D23)\n",
       "6            ACCESSORIES (D24)\n",
       "7            ACCESSORIES (D33)\n",
       "8              ACCESSORIES D26\n",
       "36     BEDDING AND ACCESSORIES\n",
       "321          WATCH ACCESSORIES\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(new_trm_frq)\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "hclust = AgglomerativeClustering(n_clusters=32)\n",
    "hclust.fit(dense_term_freq)\n",
    "#Labels it will store the entire labels for each of the \n",
    "#32 gave best\n",
    "clust_labels=hclust.labels_\n",
    "labelled_data=np.column_stack((uniq_labels,clust_labels))\n",
    "labelled_data=pd.DataFrame(labelled_data)\n",
    "a=labelled_data[0]\n",
    "a[labelled_data[1]==9]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART-2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUSINESS SENSE AND HIERARCHICAL CLUSTERING COMBINED CATEGORIZATION OF DATA HAS BEEN DONE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Combined Description</th>\n",
       "      <th>CATEGORY_DESCRIPTION</th>\n",
       "      <th>Social Intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RHD AMERICA NECK 18\"IRH AMERICA NECK SPINNER 1...</td>\n",
       "      <td>JEWELRY COSTUME</td>\n",
       "      <td>Jewelry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NB 6PK LOW CUT NB 6PK LOW CUT NB 6PK LOW CUT P...</td>\n",
       "      <td>CASUAL SOCKS</td>\n",
       "      <td>Socks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SWEATSHIRTS ONLINE ONLY DSV GILDAN MENS CREWNE...</td>\n",
       "      <td>DOTCOM (D23)</td>\n",
       "      <td>Others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SCRUB OUTFITS ONLINE ONLY DSV FEELIN' FELINE S...</td>\n",
       "      <td>SCRUBS</td>\n",
       "      <td>Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOL 6PK SEAMLESS LRB FOL BRF 6P AST 7 FOL 6PK ...</td>\n",
       "      <td>PACKAGED UNDERWEAR</td>\n",
       "      <td>Undergarments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18 HR BREATH BEAUTY 4716B WHITE 44D PLAYTEX 18...</td>\n",
       "      <td>FULL FIGURE</td>\n",
       "      <td>Undergarments</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Combined Description CATEGORY_DESCRIPTION  \\\n",
       "0  RHD AMERICA NECK 18\"IRH AMERICA NECK SPINNER 1...      JEWELRY COSTUME   \n",
       "1  NB 6PK LOW CUT NB 6PK LOW CUT NB 6PK LOW CUT P...         CASUAL SOCKS   \n",
       "2  SWEATSHIRTS ONLINE ONLY DSV GILDAN MENS CREWNE...         DOTCOM (D23)   \n",
       "3  SCRUB OUTFITS ONLINE ONLY DSV FEELIN' FELINE S...               SCRUBS   \n",
       "4  FOL 6PK SEAMLESS LRB FOL BRF 6P AST 7 FOL 6PK ...   PACKAGED UNDERWEAR   \n",
       "5  18 HR BREATH BEAUTY 4716B WHITE 44D PLAYTEX 18...          FULL FIGURE   \n",
       "\n",
       "   Social Intent  \n",
       "0        Jewelry  \n",
       "1          Socks  \n",
       "2         Others  \n",
       "3    Accessories  \n",
       "4  Undergarments  \n",
       "5  Undergarments  "
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_clust_labels=pd.read_csv('/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels.csv')\n",
    "data_with_clust_labels.head(6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STORING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydata=data_with_clust_labels   #mydata saves the combined data with cluster labels\n",
    "#x-item description is being stored and y stores the corresponding labels\n",
    "x=mydata['Combined Description']\n",
    "y=mydata['Social Intent']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ENCODING CLUSTER LABELS FOR FINAL CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Label Encoder and converting the y's into integers\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_coded=le.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING ITEM DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1=get_udoc(x)  #Ignoring which are not utf-8\n",
    "x2=get_tokenized_doc(x1)\n",
    "x3=get_lower_doc(x2)\n",
    "x4=get_stopwrd_free_doc(x3)\n",
    "x5=get_alpha_doc(x4)\n",
    "x6=get_lem_doc(x5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x6 stores thepre-processed doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#REMOVING SMALL LETTER WORDS\n",
    "x7=get_long_wrd_doc(x6)\n",
    "desc_string=get_string_version(x7) #desc_string is input to tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING TERM FREQUENCY FOR CLASSIFICATION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saving the particular characteristics giving the best feature size\n",
    "term_freq_model=sklearn.feature_extraction.text.CountVectorizer(input='content', encoding='utf-8', decode_error='strict', \n",
    "                                                strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, \n",
    "                                                stop_words='english', ngram_range=(1,1), analyzer='word', \n",
    "                                                max_df=0.999, min_df=0,max_features=2000,\n",
    "                                                vocabulary=None, binary=False)\n",
    "\n",
    "#Transforming the data to count of token\n",
    "term_frq_p2=term_freq_model.fit_transform(desc_string)\n",
    "#features_names\n",
    "features_p2=term_freq_model.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'acc',\n",
       " u'accent',\n",
       " u'accessory',\n",
       " u'aci',\n",
       " u'acid',\n",
       " u'acrylic',\n",
       " u'act',\n",
       " u'active',\n",
       " u'activewear',\n",
       " u'adesso',\n",
       " u'adj',\n",
       " u'adjustable',\n",
       " u'adult',\n",
       " u'adv',\n",
       " u'advanced',\n",
       " u'aggro',\n",
       " u'air',\n",
       " u'alaska',\n",
       " u'aline',\n",
       " u'allison',\n",
       " u'allover',\n",
       " u'almond',\n",
       " u'aloha',\n",
       " u'amazing',\n",
       " u'amer',\n",
       " u'america',\n",
       " u'american',\n",
       " u'americana',\n",
       " u'analog',\n",
       " u'anchor',\n",
       " u'angel',\n",
       " u'angeles',\n",
       " u'angelique',\n",
       " u'angelstat',\n",
       " u'angry',\n",
       " u'animal',\n",
       " u'ank',\n",
       " u'ankle',\n",
       " u'anklet',\n",
       " u'anml',\n",
       " u'annual',\n",
       " u'anorak',\n",
       " u'aop',\n",
       " u'app',\n",
       " u'apparel',\n",
       " u'appel',\n",
       " u'applique',\n",
       " u'aqua',\n",
       " u'archive',\n",
       " u'arctic',\n",
       " u'argyle',\n",
       " u'ariel',\n",
       " u'armitron',\n",
       " u'arrow',\n",
       " u'art',\n",
       " u'ashirt',\n",
       " u'ashley',\n",
       " u'assorted',\n",
       " u'asst',\n",
       " u'ast',\n",
       " u'asymmetrical',\n",
       " u'ath',\n",
       " u'athleisure',\n",
       " u'athletic',\n",
       " u'athletics',\n",
       " u'attitude',\n",
       " u'auto',\n",
       " u'avalor',\n",
       " u'avenger',\n",
       " u'average',\n",
       " u'avg',\n",
       " u'avia',\n",
       " u'aviator',\n",
       " u'aviva',\n",
       " u'awesome',\n",
       " u'aztec',\n",
       " u'bab',\n",
       " u'babba',\n",
       " u'baby',\n",
       " u'babydoll',\n",
       " u'backpack',\n",
       " u'bag',\n",
       " u'bal',\n",
       " u'balconette',\n",
       " u'ball',\n",
       " u'ballerina',\n",
       " u'ballet',\n",
       " u'band',\n",
       " u'bandana',\n",
       " u'bandeau',\n",
       " u'banded',\n",
       " u'bandini',\n",
       " u'bangle',\n",
       " u'bank',\n",
       " u'baptism',\n",
       " u'bar',\n",
       " u'barbie',\n",
       " u'base',\n",
       " u'baseball',\n",
       " u'basic',\n",
       " u'basketball',\n",
       " u'bat',\n",
       " u'batgirl',\n",
       " u'batman',\n",
       " u'bay',\n",
       " u'bch',\n",
       " u'bck',\n",
       " u'beach',\n",
       " u'bead',\n",
       " u'beaded',\n",
       " u'bean',\n",
       " u'beanie',\n",
       " u'bear',\n",
       " u'beast',\n",
       " u'beau',\n",
       " u'beauty',\n",
       " u'bebe',\n",
       " u'beefy',\n",
       " u'beer',\n",
       " u'beige',\n",
       " u'believe',\n",
       " u'bell',\n",
       " u'belle',\n",
       " u'belly',\n",
       " u'belt',\n",
       " u'belted',\n",
       " u'ben',\n",
       " u'benard',\n",
       " u'benefit',\n",
       " u'berm',\n",
       " u'bermuda',\n",
       " u'berry',\n",
       " u'best',\n",
       " u'betty',\n",
       " u'beverly',\n",
       " u'bfp',\n",
       " u'bib',\n",
       " u'big',\n",
       " u'bigs',\n",
       " u'bik',\n",
       " u'bike',\n",
       " u'biki',\n",
       " u'bikini',\n",
       " u'bimini',\n",
       " u'bin',\n",
       " u'bird',\n",
       " u'bishop',\n",
       " u'bite',\n",
       " u'bkn',\n",
       " u'blac',\n",
       " u'black',\n",
       " u'blakely',\n",
       " u'blanket',\n",
       " u'blaze',\n",
       " u'blazer',\n",
       " u'blend',\n",
       " u'bling',\n",
       " u'bliss',\n",
       " u'blissful',\n",
       " u'blk',\n",
       " u'blksot',\n",
       " u'blkt',\n",
       " u'block',\n",
       " u'blouse',\n",
       " u'blt',\n",
       " u'bltd',\n",
       " u'blu',\n",
       " u'blue',\n",
       " u'board',\n",
       " u'boardshort',\n",
       " u'boat',\n",
       " u'boatneck',\n",
       " u'bocini',\n",
       " u'body',\n",
       " u'bodycon',\n",
       " u'bodymapped',\n",
       " u'bodysuit',\n",
       " u'bodysuits',\n",
       " u'boho',\n",
       " u'bomber',\n",
       " u'bon',\n",
       " u'bonded',\n",
       " u'bonus',\n",
       " u'boo',\n",
       " u'boot',\n",
       " u'bootcut',\n",
       " u'bootie',\n",
       " u'border',\n",
       " u'bos',\n",
       " u'bot',\n",
       " u'boucle',\n",
       " u'boundary',\n",
       " u'bow',\n",
       " u'box',\n",
       " u'boxed',\n",
       " u'boxer',\n",
       " u'boy',\n",
       " u'boyfriend',\n",
       " u'boyleg',\n",
       " u'boyshort',\n",
       " u'bra',\n",
       " u'brac',\n",
       " u'bracelet',\n",
       " u'brahma',\n",
       " u'braid',\n",
       " u'braided',\n",
       " u'bralette',\n",
       " u'brand',\n",
       " u'brass',\n",
       " u'brc',\n",
       " u'breathable',\n",
       " u'breathe',\n",
       " u'brf',\n",
       " u'brief',\n",
       " u'bright',\n",
       " u'brinley',\n",
       " u'brittney',\n",
       " u'brn',\n",
       " u'brooke',\n",
       " u'brother',\n",
       " u'brown',\n",
       " u'brownshoe',\n",
       " u'brushed',\n",
       " u'btm',\n",
       " u'btn',\n",
       " u'btrfly',\n",
       " u'bts',\n",
       " u'bttm',\n",
       " u'bubble',\n",
       " u'buckle',\n",
       " u'buddha',\n",
       " u'buffalo',\n",
       " u'built',\n",
       " u'bump',\n",
       " u'bungee',\n",
       " u'bunny',\n",
       " u'burbu',\n",
       " u'burnout',\n",
       " u'burnwash',\n",
       " u'butterfly',\n",
       " u'button',\n",
       " u'bxr',\n",
       " u'bxrbrf',\n",
       " u'byb',\n",
       " u'bys',\n",
       " u'cable',\n",
       " u'cabo',\n",
       " u'cafe',\n",
       " u'caged',\n",
       " u'calf',\n",
       " u'california',\n",
       " u'cami',\n",
       " u'camisole',\n",
       " u'camo',\n",
       " u'camp',\n",
       " u'candy',\n",
       " u'canvas',\n",
       " u'cap',\n",
       " u'cape',\n",
       " u'capital',\n",
       " u'capri',\n",
       " u'captain',\n",
       " u'captivate',\n",
       " u'car',\n",
       " u'card',\n",
       " u'cardi',\n",
       " u'cardigan',\n",
       " u'care',\n",
       " u'career',\n",
       " u'cargo',\n",
       " u'carolina',\n",
       " u'carp',\n",
       " u'carpenter',\n",
       " u'carrini',\n",
       " u'carter',\n",
       " u'cartoon',\n",
       " u'case',\n",
       " u'casio',\n",
       " u'cast',\n",
       " u'casual',\n",
       " u'casuals',\n",
       " u'cat',\n",
       " u'catalina',\n",
       " u'cbc',\n",
       " u'celestial',\n",
       " u'cell',\n",
       " u'center',\n",
       " u'cff',\n",
       " u'cha',\n",
       " u'chain',\n",
       " u'challis',\n",
       " u'chambray',\n",
       " u'character',\n",
       " u'charcoal',\n",
       " u'charm',\n",
       " u'check',\n",
       " u'cheekster',\n",
       " u'cheeky',\n",
       " u'cheetah',\n",
       " u'chemise',\n",
       " u'chenille',\n",
       " u'cherokee',\n",
       " u'chest',\n",
       " u'chevron',\n",
       " u'chevy',\n",
       " u'chic',\n",
       " u'chievous',\n",
       " u'chiffon',\n",
       " u'child',\n",
       " u'chill',\n",
       " u'chino',\n",
       " u'chn',\n",
       " u'choker',\n",
       " u'christening',\n",
       " u'christmas',\n",
       " u'chunky',\n",
       " u'cinch',\n",
       " u'cinched',\n",
       " u'cinderella',\n",
       " u'circle',\n",
       " u'city',\n",
       " u'class',\n",
       " u'classic',\n",
       " u'claw',\n",
       " u'clear',\n",
       " u'cleat',\n",
       " u'climate',\n",
       " u'climateright',\n",
       " u'clip',\n",
       " u'clog',\n",
       " u'close',\n",
       " u'clothing',\n",
       " u'clr',\n",
       " u'clrblk',\n",
       " u'club',\n",
       " u'clutch',\n",
       " u'cmfrt',\n",
       " u'cmft',\n",
       " u'coat',\n",
       " u'coffee',\n",
       " u'coin',\n",
       " u'col',\n",
       " u'cold',\n",
       " u'colette',\n",
       " u'collar',\n",
       " u'collection',\n",
       " u'college',\n",
       " u'color',\n",
       " u'colorblock',\n",
       " u'colored',\n",
       " u'com',\n",
       " u'combo',\n",
       " u'comf',\n",
       " u'comfor',\n",
       " u'comfort',\n",
       " u'comfortblend',\n",
       " u'comfortease',\n",
       " u'comfortsoft',\n",
       " u'comfy',\n",
       " u'comic',\n",
       " u'comment',\n",
       " u'comp',\n",
       " u'compression',\n",
       " u'concept',\n",
       " u'confetti',\n",
       " u'contour',\n",
       " u'contrast',\n",
       " u'control',\n",
       " u'conv',\n",
       " u'convertible',\n",
       " u'cookie',\n",
       " u'cool',\n",
       " u'cooler',\n",
       " u'cora',\n",
       " u'coral',\n",
       " u'cord',\n",
       " u'corduroy',\n",
       " u'core',\n",
       " u'costume',\n",
       " u'cot',\n",
       " u'cotton',\n",
       " u'court',\n",
       " u'cov',\n",
       " u'cover',\n",
       " u'coverage',\n",
       " u'coverall',\n",
       " u'coverup',\n",
       " u'cowboy',\n",
       " u'cowl',\n",
       " u'cozy',\n",
       " u'crd',\n",
       " u'cre',\n",
       " u'cream',\n",
       " u'creep',\n",
       " u'creeper',\n",
       " u'crew',\n",
       " u'crewneck',\n",
       " u'crg',\n",
       " u'crgo',\n",
       " u'crinkle',\n",
       " u'criss',\n",
       " u'critter',\n",
       " u'cro',\n",
       " u'croc',\n",
       " u'crochet',\n",
       " u'crop',\n",
       " u'cropped',\n",
       " u'cross',\n",
       " u'crossbody',\n",
       " u'crown',\n",
       " u'crpr',\n",
       " u'crush',\n",
       " u'crvy',\n",
       " u'crw',\n",
       " u'crystal',\n",
       " u'ctn',\n",
       " u'ctr',\n",
       " u'cttn',\n",
       " u'cttw',\n",
       " u'cub',\n",
       " u'cudd',\n",
       " u'cuff',\n",
       " u'cuffed',\n",
       " u'cup',\n",
       " u'cupcake',\n",
       " u'cupid',\n",
       " u'curious',\n",
       " u'curvation',\n",
       " u'curvy',\n",
       " u'cush',\n",
       " u'cushion',\n",
       " u'cut',\n",
       " u'cute',\n",
       " u'cutie',\n",
       " u'cutout',\n",
       " u'cvc',\n",
       " u'cvo',\n",
       " u'cvy',\n",
       " u'dad',\n",
       " u'daisy',\n",
       " u'dance',\n",
       " u'dangle',\n",
       " u'daniel',\n",
       " u'danksin',\n",
       " u'danskin',\n",
       " u'dark',\n",
       " u'darth',\n",
       " u'date',\n",
       " u'david',\n",
       " u'dawson',\n",
       " u'daxx',\n",
       " u'day',\n",
       " u'daywear',\n",
       " u'dazzle',\n",
       " u'dbl',\n",
       " u'dead',\n",
       " u'deadpool',\n",
       " u'dear',\n",
       " u'deep',\n",
       " u'deer',\n",
       " u'degree',\n",
       " u'del',\n",
       " u'delete',\n",
       " u'demi',\n",
       " u'den',\n",
       " u'denim',\n",
       " u'derek',\n",
       " u'despicable',\n",
       " u'destructed',\n",
       " u'dew',\n",
       " u'dia',\n",
       " u'diamond',\n",
       " u'dickie',\n",
       " u'dicky',\n",
       " u'digital',\n",
       " u'dino',\n",
       " u'dinosaur',\n",
       " u'dip',\n",
       " u'dis',\n",
       " u'disney',\n",
       " u'distressed',\n",
       " u'diva',\n",
       " u'dme',\n",
       " u'dnm',\n",
       " u'doc',\n",
       " u'dog',\n",
       " u'doll',\n",
       " u'dollhouse',\n",
       " u'dolman',\n",
       " u'dolphin',\n",
       " u'dome',\n",
       " u'dont',\n",
       " u'dora',\n",
       " u'dorm',\n",
       " u'dory',\n",
       " u'dot',\n",
       " u'double',\n",
       " u'dragon',\n",
       " u'drape',\n",
       " u'drawstring',\n",
       " u'dre',\n",
       " u'dream',\n",
       " u'dreamfit',\n",
       " u'dres',\n",
       " u'dress',\n",
       " u'dresspant',\n",
       " u'dri',\n",
       " u'drop',\n",
       " u'drs',\n",
       " u'dsv',\n",
       " u'duck',\n",
       " u'duffel',\n",
       " u'duffle',\n",
       " u'dunlop',\n",
       " u'duo',\n",
       " u'duster',\n",
       " u'dye',\n",
       " u'dyed',\n",
       " u'dynasty',\n",
       " u'eagle',\n",
       " u'ear',\n",
       " u'earring',\n",
       " u'earth',\n",
       " u'easter',\n",
       " u'eastsport',\n",
       " u'easy',\n",
       " u'eboard',\n",
       " u'ecosmart',\n",
       " u'edge',\n",
       " u'elan',\n",
       " u'elastic',\n",
       " u'elbow',\n",
       " u'elena',\n",
       " u'elephant',\n",
       " u'elevate',\n",
       " u'elevated',\n",
       " u'elf',\n",
       " u'elmo',\n",
       " u'elsa',\n",
       " u'emb',\n",
       " u'embel',\n",
       " u'embellished',\n",
       " u'embellshed',\n",
       " u'embossed',\n",
       " u'embroidered',\n",
       " u'embroidery',\n",
       " u'emma',\n",
       " u'emoji',\n",
       " u'empire',\n",
       " u'endcap',\n",
       " u'energie',\n",
       " u'energy',\n",
       " u'envelope',\n",
       " u'epic',\n",
       " u'erika',\n",
       " u'escape',\n",
       " u'eso',\n",
       " u'essent',\n",
       " u'essential',\n",
       " u'esteem',\n",
       " u'eva',\n",
       " u'event',\n",
       " u'everyday',\n",
       " u'expectation',\n",
       " u'extend',\n",
       " u'extra',\n",
       " u'extreme',\n",
       " u'eye',\n",
       " u'eyelash',\n",
       " u'eyelet',\n",
       " u'fab',\n",
       " u'fabric',\n",
       " u'face',\n",
       " u'faded',\n",
       " u'fair',\n",
       " u'fairisle',\n",
       " u'fall',\n",
       " u'family',\n",
       " u'famous',\n",
       " u'fan',\n",
       " u'fancy',\n",
       " u'farm',\n",
       " u'fash',\n",
       " u'fashi',\n",
       " u'fashion',\n",
       " u'fast',\n",
       " u'father',\n",
       " u'faux',\n",
       " u'feather',\n",
       " u'fedora',\n",
       " u'female',\n",
       " u'fer',\n",
       " u'ffm',\n",
       " u'fge',\n",
       " u'fiber',\n",
       " u'fig',\n",
       " u'figure',\n",
       " u'filigree',\n",
       " u'finding',\n",
       " u'firm',\n",
       " u'fish',\n",
       " u'fit',\n",
       " u'fitspiration',\n",
       " u'fitted',\n",
       " u'fitting',\n",
       " u'fla',\n",
       " u'flag',\n",
       " u'flamingo',\n",
       " u'flannel',\n",
       " u'flap',\n",
       " u'flare',\n",
       " u'flash',\n",
       " u'flashing',\n",
       " u'flashlight',\n",
       " u'flat',\n",
       " u'flatback',\n",
       " u'flc',\n",
       " u'fld',\n",
       " u'fle',\n",
       " u'fleec',\n",
       " u'fleece',\n",
       " u'fleeceback',\n",
       " u'flex',\n",
       " u'flexees',\n",
       " u'flip',\n",
       " u'flirt',\n",
       " u'flnl',\n",
       " u'flop',\n",
       " u'floppy',\n",
       " u'floral',\n",
       " u'florida',\n",
       " u'flounce',\n",
       " u'flower',\n",
       " u'floyd',\n",
       " u'flt',\n",
       " u'flutter',\n",
       " u'flwr',\n",
       " u'fly',\n",
       " u'flyaway',\n",
       " u'fmg',\n",
       " u'foam',\n",
       " u'foil',\n",
       " u'fol',\n",
       " u'fold',\n",
       " u'folded',\n",
       " u'foot',\n",
       " u'football',\n",
       " u'footed',\n",
       " u'footless',\n",
       " u'footwear',\n",
       " u'force',\n",
       " u'ford',\n",
       " u'forever',\n",
       " u'foster',\n",
       " u'fotl',\n",
       " u'fox',\n",
       " u'frame',\n",
       " u'fred',\n",
       " u'free',\n",
       " u'freestyle',\n",
       " u'french',\n",
       " u'fresh',\n",
       " u'freshiq',\n",
       " u'friend',\n",
       " u'fringe',\n",
       " u'frnt',\n",
       " u'fro',\n",
       " u'frog',\n",
       " u'frozen',\n",
       " u'frt',\n",
       " u'fruit',\n",
       " u'frzn',\n",
       " u'fshn',\n",
       " u'ftl',\n",
       " u'fubu',\n",
       " u'fun',\n",
       " u'fur',\n",
       " u'fuzzy',\n",
       " u'game',\n",
       " u'gan',\n",
       " u'gar',\n",
       " u'garani',\n",
       " u'garanimal',\n",
       " u'garanimals',\n",
       " u'gauze',\n",
       " u'gear',\n",
       " u'gel',\n",
       " u'gen',\n",
       " u'generic',\n",
       " u'genevieve',\n",
       " u'genie',\n",
       " u'genuine',\n",
       " u'geo',\n",
       " u'george',\n",
       " u'gerber',\n",
       " u'ghost',\n",
       " u'ghostbusters',\n",
       " u'giant',\n",
       " u'gift',\n",
       " u'giftable',\n",
       " u'gifting',\n",
       " u'gil',\n",
       " u'gildan',\n",
       " u'gir',\n",
       " u'giraffe',\n",
       " u'girl',\n",
       " u'giselle',\n",
       " u'glam',\n",
       " u'glamour',\n",
       " u'glass',\n",
       " u'gld',\n",
       " u'glitter',\n",
       " u'glo',\n",
       " u'glory',\n",
       " u'glove',\n",
       " u'glow',\n",
       " u'glv',\n",
       " u'gmg',\n",
       " u'going',\n",
       " u'gold',\n",
       " u'goldtoe',\n",
       " u'golf',\n",
       " u'good',\n",
       " u'gore',\n",
       " u'gown',\n",
       " u'gra',\n",
       " u'grad',\n",
       " u'gradient',\n",
       " u'graduate',\n",
       " u'grandma',\n",
       " u'grant',\n",
       " u'grap',\n",
       " u'graph',\n",
       " u'graphi',\n",
       " u'graphic',\n",
       " u'gray',\n",
       " u'great',\n",
       " u'green',\n",
       " u'grey',\n",
       " u'grid',\n",
       " u'grinch',\n",
       " u'grl',\n",
       " u'grls',\n",
       " u'grn',\n",
       " u'grommet',\n",
       " u'group',\n",
       " u'grp',\n",
       " u'grph',\n",
       " u'grphc',\n",
       " u'grx',\n",
       " u'gry',\n",
       " u'guard',\n",
       " u'guy',\n",
       " u'gxp',\n",
       " u'gym',\n",
       " u'hacci',\n",
       " u'hair',\n",
       " u'half',\n",
       " u'hallmark',\n",
       " u'halloween',\n",
       " u'halo',\n",
       " u'halter',\n",
       " u'hand',\n",
       " u'handbag',\n",
       " u'hanes',\n",
       " u'hang',\n",
       " u'hangdown',\n",
       " u'hanging',\n",
       " u'happening',\n",
       " u'happy',\n",
       " u'hard',\n",
       " u'harley',\n",
       " u'harry',\n",
       " u'harve',\n",
       " u'hat',\n",
       " u'hatchi',\n",
       " u'hawaii',\n",
       " u'hawk',\n",
       " u'head',\n",
       " u'headband',\n",
       " u'headwear',\n",
       " u'headwrap',\n",
       " u'healthex',\n",
       " u'healthtex',\n",
       " u'heart',\n",
       " u'heat',\n",
       " u'heather',\n",
       " u'heathered',\n",
       " u'heatseal',\n",
       " u'heavenly',\n",
       " u'heel',\n",
       " u'hello',\n",
       " u'hem',\n",
       " u'henley',\n",
       " u'herman',\n",
       " u'hero',\n",
       " u'hfp',\n",
       " u'hibiscus',\n",
       " u'hicut',\n",
       " u'high',\n",
       " u'hiker',\n",
       " u'hill',\n",
       " u'hillary',\n",
       " u'hilo',\n",
       " u'hip',\n",
       " u'hipster',\n",
       " u'hipstr',\n",
       " u'hiwaist',\n",
       " u'hltx',\n",
       " u'hlwn',\n",
       " u'hng',\n",
       " u'hobo',\n",
       " u'hockey',\n",
       " u'hogan',\n",
       " u'holiday',\n",
       " u'hollywood',\n",
       " u'home',\n",
       " u'honey',\n",
       " u'hoo',\n",
       " u'hood',\n",
       " u'hooded',\n",
       " u'hoodie',\n",
       " u'hoodies',\n",
       " u'hoody',\n",
       " u'hook',\n",
       " u'hoop',\n",
       " u'hot',\n",
       " u'hour',\n",
       " u'hrt',\n",
       " u'hthr',\n",
       " u'htr',\n",
       " u'htx',\n",
       " u'hudson',\n",
       " u'hulk',\n",
       " u'humor',\n",
       " u'husky',\n",
       " u'hybrid',\n",
       " u'hyi',\n",
       " u'ice',\n",
       " u'iceburg',\n",
       " u'icon',\n",
       " u'igloo',\n",
       " u'iii',\n",
       " u'iml',\n",
       " u'impact',\n",
       " u'inch',\n",
       " u'indigo',\n",
       " u'industry',\n",
       " u'inf',\n",
       " u'infant',\n",
       " u'infinity',\n",
       " u'initial',\n",
       " u'inseam',\n",
       " u'inset',\n",
       " u'inspire',\n",
       " u'inspired',\n",
       " u'insulated',\n",
       " u'int',\n",
       " u'interceptor',\n",
       " u'interlock',\n",
       " u'intimate',\n",
       " u'intl',\n",
       " u'iron',\n",
       " u'island',\n",
       " u'isotoner',\n",
       " u'itb',\n",
       " u'item',\n",
       " u'itg',\n",
       " u'ixtreme',\n",
       " u'jac',\n",
       " u'jack',\n",
       " u'jacke',\n",
       " u'jacket',\n",
       " u'jacq',\n",
       " u'jacquard',\n",
       " u'jam',\n",
       " u'jane',\n",
       " u'jason',\n",
       " u'jckt',\n",
       " u'jea',\n",
       " u'jean',\n",
       " u'jeg',\n",
       " u'jegg',\n",
       " u'jegging',\n",
       " u'jeggings',\n",
       " u'jelly',\n",
       " u'jersey',\n",
       " u'jerzees',\n",
       " u'jet',\n",
       " u'jewel',\n",
       " u'jewelry',\n",
       " u'jillian',\n",
       " u'jkt',\n",
       " u'jms',\n",
       " u'jockey',\n",
       " u'jog',\n",
       " u'jogger',\n",
       " u'jordache',\n",
       " u'jpr',\n",
       " u'jrsy',\n",
       " u'jumper',\n",
       " u'junior',\n",
       " u'jurassic',\n",
       " u'justice',\n",
       " u'karma',\n",
       " u'kate',\n",
       " u'kaye',\n",
       " u'keepsake',\n",
       " u'key',\n",
       " u'keychain',\n",
       " u'keyhole',\n",
       " u'keyring',\n",
       " u'khaki',\n",
       " u'khole',\n",
       " u'kid',\n",
       " u'kimono',\n",
       " u'king',\n",
       " u'kiss',\n",
       " u'kitty',\n",
       " u'kiwi',\n",
       " u'knee',\n",
       " u'knit',\n",
       " u'knkt',\n",
       " u'knot',\n",
       " u'kodiak',\n",
       " u'komar',\n",
       " u'komen',\n",
       " u'label',\n",
       " u'lac',\n",
       " u'lace',\n",
       " u'laceup',\n",
       " u'lad',\n",
       " u'ladie',\n",
       " u'lady',\n",
       " u'lag',\n",
       " u'laguna',\n",
       " u'lamaze',\n",
       " u'landau',\n",
       " u'lanyard',\n",
       " u'large',\n",
       " u'laser',\n",
       " u'lattice',\n",
       " u'laundry',\n",
       " u'layer',\n",
       " u'layered',\n",
       " u'layering',\n",
       " u'lcd',\n",
       " u'lce',\n",
       " u'lds',\n",
       " u'lead',\n",
       " u'leading',\n",
       " u'leaf',\n",
       " u'league',\n",
       " u'leather',\n",
       " u'led',\n",
       " u'lee',\n",
       " u'leg',\n",
       " u'legacy',\n",
       " u'legend',\n",
       " u'legg',\n",
       " u'leggi',\n",
       " u'leggin',\n",
       " u'legging',\n",
       " u'leggs',\n",
       " u'lego',\n",
       " u'lei',\n",
       " u'leigh',\n",
       " u'lemondrop',\n",
       " u'length',\n",
       " u'lens',\n",
       " u'leo',\n",
       " u'leopard',\n",
       " u'leotard',\n",
       " u'levi',\n",
       " u'liberty',\n",
       " u'lic',\n",
       " u'license',\n",
       " u'licensed',\n",
       " u'life',\n",
       " u'lifestyle',\n",
       " u'lift',\n",
       " u'light',\n",
       " u'lightweight',\n",
       " u'like',\n",
       " u'lil',\n",
       " u'lilly',\n",
       " u'lime',\n",
       " u'limited',\n",
       " u'line',\n",
       " u'linear',\n",
       " u'lined',\n",
       " u'linen',\n",
       " u'liner',\n",
       " u'link',\n",
       " u'lion',\n",
       " u'lite',\n",
       " u'little',\n",
       " u'lnr',\n",
       " u'local',\n",
       " u'logo',\n",
       " ...]"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_p2 #saves the final features_list for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dense_term_freq_p2=get_dense_matrix(term_frq_p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP LEARNING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense\n",
    "from keras.objectives import categorical_crossentropy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dropout\n",
    "np.random.seed(0)\n",
    "deep_model=Sequential()\n",
    "#Dense layer\n",
    "from keras.layers import Dense,Activation\n",
    "deep_model.add(Dense(1000,input_shape=(2000,),activation='relu'))\n",
    "deep_model.add(Dropout(0.25))\n",
    "deep_model.add(Dense(800,activation='relu'))\n",
    "deep_model.add(Dropout(0.25))\n",
    "deep_model.add(Dense(400,activation='relu'))\n",
    "deep_model.add(Dropout(0.25))\n",
    "deep_model.add(Dense(200,activation='relu'))\n",
    "deep_model.add(Dropout(0.25))\n",
    "deep_model.add(Dense(100,activation='relu'))\n",
    "deep_model.add(Dense(49 ,activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 1000)              2001000   \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 800)               800800    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 400)               320400    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 49)                4949      \n",
      "=================================================================\n",
      "Total params: 3,227,449\n",
      "Trainable params: 3,227,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model compilation\n",
    "from keras.optimizers import SGD\n",
    "#model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])\n",
    "deep_model.summary()\n",
    "deep_model.compile(loss='categorical_crossentropy',optimizer=SGD(),metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#len(np.unique(y_coded)\n",
    "\n",
    "#converting the y to be given as an input\n",
    "from keras.utils import to_categorical\n",
    "cat_y=to_categorical(y_coded, num_classes=49)\n",
    "\n",
    "\n",
    "#x_scaled=sklearn.preprocessing.scale(x, axis=0, with_mean=True, with_std=True, copy=True)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(dense_term_freq_p2, cat_y, test_size=0.3, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 325494 samples, validate on 139498 samples\n",
      "Epoch 1/10\n",
      "325494/325494 [==============================] - 221s 680us/step - loss: 1.6592 - acc: 0.5763 - val_loss: 0.8103 - val_acc: 0.7662\n",
      "Epoch 2/10\n",
      "325494/325494 [==============================] - 1682s 5ms/step - loss: 0.7747 - acc: 0.7769 - val_loss: 0.5527 - val_acc: 0.8365\n",
      "Epoch 3/10\n",
      "325494/325494 [==============================] - 222s 682us/step - loss: 0.5887 - acc: 0.8264 - val_loss: 0.4562 - val_acc: 0.8638\n",
      "Epoch 4/10\n",
      "325494/325494 [==============================] - 242s 744us/step - loss: 0.4936 - acc: 0.8523 - val_loss: 0.3995 - val_acc: 0.8803\n",
      "Epoch 5/10\n",
      "325494/325494 [==============================] - 261s 801us/step - loss: 0.4339 - acc: 0.8698 - val_loss: 0.3645 - val_acc: 0.8916\n",
      "Epoch 6/10\n",
      "325494/325494 [==============================] - 267s 822us/step - loss: 0.3920 - acc: 0.8810 - val_loss: 0.3431 - val_acc: 0.8979\n",
      "Epoch 7/10\n",
      "325494/325494 [==============================] - 264s 811us/step - loss: 0.3595 - acc: 0.8901 - val_loss: 0.3202 - val_acc: 0.9042\n",
      "Epoch 8/10\n",
      "325494/325494 [==============================] - 274s 841us/step - loss: 0.3336 - acc: 0.8979 - val_loss: 0.3080 - val_acc: 0.9087\n",
      "Epoch 9/10\n",
      "325494/325494 [==============================] - 492s 2ms/step - loss: 0.3127 - acc: 0.9036 - val_loss: 0.3007 - val_acc: 0.9110\n",
      "Epoch 10/10\n",
      "325494/325494 [==============================] - 369s 1ms/step - loss: 0.2950 - acc: 0.9090 - val_loss: 0.2906 - val_acc: 0.9152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1acd375210>"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=1,min_delta=0)\n",
    "deep_model.fit(X_train,Y_train, epochs=10, batch_size = 25, validation_data=(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLLECTING TWEET DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=open('/Users/s0c01xa/Documents/Walmart_Bplan/Gen_Tweet.csv')\n",
    "tweet_data=f.readline()\n",
    "tweet=word_tokenize(tweet_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING TWEET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_processed_tweet(document):\n",
    "    val_test=''\n",
    "    for i in range(0,len(document)):\n",
    "        val=document[i]\n",
    "        #print val\n",
    "        val_test=str(val+\" \"+val_test)\n",
    "    return val_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_tweet=get_processed_tweet(tweet)\n",
    "\n",
    "#processed_tweet saves the tweet data to be transformed to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMING TWEET DATA1 AS PER TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tweet=term_freq_model.transform([processed_tweet])\n",
    "dense_tweet=get_dense_matrix(test_tweet)\n",
    "#test_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting through DEEP MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.41445359e-12,   2.04678930e-11,   5.52985296e-11,\n",
       "          5.79791180e-11,   8.40185352e-11,   5.20789467e-10,\n",
       "          6.06991957e-10,   7.84837917e-10,   2.60715294e-09,\n",
       "          2.71472511e-09,   3.75738773e-09,   4.24484980e-09,\n",
       "          6.51892273e-09,   2.49858711e-08,   3.91591506e-08,\n",
       "          4.80964388e-08,   5.18187022e-08,   5.78572497e-08,\n",
       "          7.91170152e-08,   1.48318506e-07,   2.91084234e-07,\n",
       "          3.47993392e-07,   5.04390414e-07,   8.00218743e-07,\n",
       "          1.96422752e-06,   4.60082765e-06,   5.51997209e-06,\n",
       "          6.51054597e-06,   6.88627870e-06,   1.21776238e-05,\n",
       "          1.36587269e-05,   1.89412858e-05,   4.90579441e-05,\n",
       "          5.51369776e-05,   5.60667213e-05,   1.00060839e-04,\n",
       "          1.22473939e-04,   1.91291459e-04,   2.20145303e-04,\n",
       "          3.67413071e-04,   3.86800617e-04,   5.84786292e-04,\n",
       "          1.14310405e-03,   1.93945807e-03,   2.03228667e-02,\n",
       "          5.54570109e-02,   6.51651621e-02,   2.46826187e-01,\n",
       "          6.06940269e-01]], dtype=float32)"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(deep_model.predict_proba(dense_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['School', 'Shorts', 'Sweaters', 'Pants', 'Missy', 'Woven',\n",
       "        'Jackets', 'Capri', 'Skirts', 'Dancewear', 'Slippers and sandals',\n",
       "        'Casual', 'Dress', 'Hawaii', 'Sets and Separates', 'Knit ',\n",
       "        'Holiday Collection', 'Jeans', 'Outerwear', 'Activewear',\n",
       "        'No Descriptiom', 'Rain', 'Tights and leggings', 'Workwear',\n",
       "        'Thermals', 'Kids', 'Shoes and boots', 'Swimwear',\n",
       "        'Sports and athletics', 'Puerto Rico', 'Maternity', 'Seasonal',\n",
       "        'Socks', 'Daywear', 'Beachwear', 'Gifts', 'Others', 'Fleece',\n",
       "        'Alaska', 'Head and hand', 'Baby', 'Sleepwear', 'Accessories',\n",
       "        'Undergarments', 'Promotions', 'Jewelry', 'Tops', 'Tshirts',\n",
       "        'Licensed']], dtype=object)"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(np.argsort(deep_model.predict_proba(dense_tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWEET DATA2 AS PER TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=open('/Users/s0c01xa/Documents/Walmart_Bplan/Gen_Tweet_3.csv')\n",
    "tweet_data=f.readline()\n",
    "tweet=word_tokenize(tweet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_tweet=get_processed_tweet(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tweet=term_freq_model.transform([processed_tweet])\n",
    "dense_tweet=get_dense_matrix(test_tweet)\n",
    "#test_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting through DEEP MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Seasonal', 'Fleece', 'Promotions', 'Tshirts', 'Activewear',\n",
       "        'Jackets', 'Tops', 'Licensed', 'Knit ', 'Sports and athletics',\n",
       "        'Beachwear', 'Rain', 'Thermals', 'Holiday Collection', 'Hawaii',\n",
       "        'Capri', 'Alaska', 'Sleepwear', 'Slippers and sandals',\n",
       "        'Maternity', 'Woven', 'School', 'Sweaters', 'Sets and Separates',\n",
       "        'Dancewear', 'Swimwear', 'Daywear', 'Pants', 'Jewelry',\n",
       "        'Puerto Rico', 'Skirts', 'Tights and leggings', 'Head and hand',\n",
       "        'Kids', 'No Descriptiom', 'Undergarments', 'Baby', 'Outerwear',\n",
       "        'Accessories', 'Socks', 'Gifts', 'Shorts', 'Dress', 'Workwear',\n",
       "        'Casual', 'Others', 'Missy', 'Shoes and boots', 'Jeans']], dtype=object)"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(np.argsort(deep_model.predict_proba(dense_tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.20840338e-12,   1.29768219e-11,   2.02193210e-11,\n",
       "          2.12193128e-11,   3.24677923e-11,   2.33658898e-10,\n",
       "          2.45094833e-10,   2.69656797e-10,   2.93302077e-10,\n",
       "          5.34033984e-10,   2.02228900e-09,   2.39636178e-09,\n",
       "          7.43288053e-09,   1.75170243e-08,   1.77247266e-08,\n",
       "          2.37883935e-08,   2.45065159e-08,   2.96390326e-08,\n",
       "          3.01851273e-08,   5.02192208e-08,   6.05706205e-08,\n",
       "          6.83533230e-08,   8.98261234e-08,   9.11191194e-08,\n",
       "          1.33745360e-07,   2.10808068e-07,   2.37023514e-07,\n",
       "          2.65993037e-07,   3.30237810e-07,   4.72918401e-07,\n",
       "          8.09114738e-07,   9.29012742e-07,   1.20950085e-06,\n",
       "          1.33954904e-06,   1.78495418e-06,   1.88089655e-06,\n",
       "          4.64135292e-06,   4.77977437e-06,   8.74703619e-06,\n",
       "          9.87244493e-06,   1.19763208e-05,   1.35680384e-05,\n",
       "          2.47476382e-05,   4.56394104e-04,   5.08917437e-04,\n",
       "          6.21817191e-04,   6.22810679e-04,   1.85632613e-02,\n",
       "          9.79138315e-01]], dtype=float32)"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(deep_model.predict_proba(dense_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Once the particular cluster is being refered to,we have to see all the items belonging to document wise.\n",
    "#Work is to find out in the particular group which of the items "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling Back the data with cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Combined Description</th>\n",
       "      <th>CATEGORY_DESCRIPTION</th>\n",
       "      <th>Social Intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RHD AMERICA NECK 18\"IRH AMERICA NECK SPINNER 1...</td>\n",
       "      <td>JEWELRY COSTUME</td>\n",
       "      <td>Jewelry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NB 6PK LOW CUT NB 6PK LOW CUT NB 6PK LOW CUT P...</td>\n",
       "      <td>CASUAL SOCKS</td>\n",
       "      <td>Socks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SWEATSHIRTS ONLINE ONLY DSV GILDAN MENS CREWNE...</td>\n",
       "      <td>DOTCOM (D23)</td>\n",
       "      <td>Others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SCRUB OUTFITS ONLINE ONLY DSV FEELIN' FELINE S...</td>\n",
       "      <td>SCRUBS</td>\n",
       "      <td>Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOL 6PK SEAMLESS LRB FOL BRF 6P AST 7 FOL 6PK ...</td>\n",
       "      <td>PACKAGED UNDERWEAR</td>\n",
       "      <td>Undergarments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18 HR BREATH BEAUTY 4716B WHITE 44D PLAYTEX 18...</td>\n",
       "      <td>FULL FIGURE</td>\n",
       "      <td>Undergarments</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Combined Description CATEGORY_DESCRIPTION  \\\n",
       "0  RHD AMERICA NECK 18\"IRH AMERICA NECK SPINNER 1...      JEWELRY COSTUME   \n",
       "1  NB 6PK LOW CUT NB 6PK LOW CUT NB 6PK LOW CUT P...         CASUAL SOCKS   \n",
       "2  SWEATSHIRTS ONLINE ONLY DSV GILDAN MENS CREWNE...         DOTCOM (D23)   \n",
       "3  SCRUB OUTFITS ONLINE ONLY DSV FEELIN' FELINE S...               SCRUBS   \n",
       "4  FOL 6PK SEAMLESS LRB FOL BRF 6P AST 7 FOL 6PK ...   PACKAGED UNDERWEAR   \n",
       "5  18 HR BREATH BEAUTY 4716B WHITE 44D PLAYTEX 18...          FULL FIGURE   \n",
       "\n",
       "   Social Intent  \n",
       "0        Jewelry  \n",
       "1          Socks  \n",
       "2         Others  \n",
       "3    Accessories  \n",
       "4  Undergarments  \n",
       "5  Undergarments  "
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_clust_labels=pd.read_csv('/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels.csv')\n",
    "data_with_clust_labels.head(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''document_1_text = 'This is document one'\n",
    "document_2_text = 'This is document two'\n",
    "\n",
    "document_1_words = word_tokenize(document_1_text)\n",
    "document_2_words = document_2_text.split()\n",
    "common = set(document_1_words).intersection( set(document_2_words) )'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filtered_data saves the reccommended basket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FILTERING THE DOCUMENTS BASED ON THE MOST SIMILAR CLUSTER\n",
    "df=data_with_clust_labels\n",
    "filtered_data=df[df['Social Intent']=='Jeans']\n",
    "doc_basket=filtered_data['Combined Description']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING doc_basket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_basket1=get_tokenized_doc(doc_basket)\n",
    "doc_basket2=get_lower_doc(doc_basket1)\n",
    "doc_basket3=get_stopwrd_free_doc(doc_basket2)\n",
    "doc_basket4=get_alpha_doc(doc_basket3)\n",
    "doc_basket5=get_lem_doc(doc_basket4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#doc_basket5 has the list of items with whom the tweet is compared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWEET DATA\n",
    "#tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPARISON 1 using MAX FREQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#similarity basket saves the items which are similar to the tweet in most similar cluster label\n",
    "similarity_basket=[]\n",
    "for i in range(0,len(doc_basket5)):\n",
    "    common=len(set(doc_basket5[i]).intersection( set(tweet)))\n",
    "    similarity_basket.append(common)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'SLIM BOOTCUT DENIM ONLINE ONLY FADED GLORY COMFORT WB SLIM BOOT JEAN FADED GLORY',\n",
       "       'FG BOOT CUT JEAN ONLINE ONLY BOYS BOOTCUT JEAN FADED GLORY',\n",
       "       'FG BOOT CUT JEAN FADED GLORY BOYS BOOTCUT JEAN FADED GLORY',\n",
       "       'FG REGULAR FIT JEAN FG REGULAR FIT JEAN FG REGULAR FIT JEAN BLACK 36X29 FADED GLORY',\n",
       "       'FG REGULAR FIT JEAN FG REGULAR FIT JEAN FG REGULAR FIT JEAN BLACK 34X32 FADED GLORY',\n",
       "       'FG REGULAR FIT JEAN FG REGULAR FIT JEAN FG REGULAR FIT JEAN BLACK 48X30 FADED GLORY',\n",
       "       'FG BOOT CUT JEAN DEL WK19 2017 BOYS BOOTCUT JEAN FADED GLORY',\n",
       "       'FG REGULAR FIT JEAN FG REGULAR FIT JEAN FG REGULAR FIT JEAN BLACK 34X36 FADED GLORY',\n",
       "       'FG BOOT CUT JEAN ONLINE ONLY BOYS BOOTCUT JEAN FADED GLORY',\n",
       "       'FG REGULAR FIT JEAN FG REGULAR FIT JEAN FG REGULAR FIT JEAN BLACK 34X29 FADED GLORY',\n",
       "       'FG BOOT CUT JEAN FADED GLORY BOYS BOOTCUT JEAN FADED GLORY',\n",
       "       'FG BOOT CUT JEAN ONLINE ONLY DSV BOYS BOOTCUT JEAN FADED GLORY',\n",
       "       'FG BOOT CUT JEAN ONLINE ONLY BOYS BOOTCUT JEAN FADED GLORY',\n",
       "       'FG CURVY BOOT JEAN FG CURVY BOOT JEAN FG CURVY BOOTCUT JEAN FADED GLORY',\n",
       "       'FG BOOT CUT JEAN FADED GLORY BOYS BOOTCUT JEAN FADED GLORY',\n",
       "       'FG BOOT CUT JEAN DEL WK44 2017 BOYS BOOTCUT JEAN FADED GLORY',\n",
       "       'FG BOOT CUT JEAN FADED GLORY BOYS BOOTCUT JEAN FADED GLORY',\n",
       "       'FG BOOT CUT JEAN 10K/SS 1/10 TRIO BOYS BOOTCUT JEAN FADED GLORY',\n",
       "       'WR BOOTCUT JEAN WR CLASSIC BOOT FIT BOYS WRANGLER CLASSIC BOOT FIT JEAN WRANGLER',\n",
       "       'FG BOOT CUT JEAN WRANGLER BOYS BOOTCUT JEAN FADED GLORY'], dtype=object)"
      ]
     },
     "execution_count": 712,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TOP 20 ITEMS RECCOMMENDED\n",
    "a=np.argsort(similarity_basket)[-20:]\n",
    "doc_basket[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENSIM WORD2VEC BASED SIMILARITY BETWEEN ITEMS AND TWEETS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2V FOR ITEM DOCS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "from collections import namedtuple\n",
    "item_gensim_doc=get_string_version(doc_basket5)\n",
    "data=item_gensim_doc\n",
    "docs = []\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "\n",
    "#converting it to int type\n",
    "'''for i in range(0,len(docLabels)):\n",
    "    docLabels[i]=int(docLabels[i])'''\n",
    "\n",
    "#converting the words to lowercase and appending it in docs in the form of tuples\n",
    "#docs[0] will contain the tuple of first doc and it's label\n",
    "for i in range (0,len(data)):\n",
    "    words = data[i].lower().split()\n",
    "    tags=[str(i)]\n",
    "    docs.append(analyzedDocument(words, tags))\n",
    "\n",
    "# Train model (set min_count = 1, if you want the model to work with the provided example data set)\n",
    "model = doc2vec.Doc2Vec(docs, size = 50, window = 300, min_count = 1, workers = 5)\n",
    "\n",
    "#model.docvecs[i] saves the corresponding vector for the document with label numbers mentioned in tags\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTION TO RETURN A MATRIX OF ALL VECTORS FOR ITEM DESC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gensim_item_vec(doc):\n",
    "    gensim_w2v=np.zeros(((len(doc),50)))\n",
    "    for i in range(0,len(doc)):\n",
    "        gensim_w2v[i]=doc[i]\n",
    "        \n",
    "    return gensim_w2v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_item_vec=get_gensim_item_vec(model.docvecs) #it stores the entire vectors corr. to each doc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2V FOR TWEETS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "from collections import namedtuple\n",
    "#tweet_str=get_string_version(tweet)\n",
    "data=str(tweet)\n",
    "docs = []\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "\n",
    "\n",
    "words = data.lower().split()\n",
    "tags=[str(0)]\n",
    "docs.append(analyzedDocument(words, tags))\n",
    "\n",
    "# Train model (set min_count = 1, if you want the model to work with the provided example data set)\n",
    "tweet_model = doc2vec.Doc2Vec(docs, size = 50, window = 300, min_count = 1, workers = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_gensim_vec=tweet_model.docvecs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPARISON BETWEEN ITEM DESC AND TWEET VEC TO FIND HIDDEN SIMILARITY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gensim_cosine_similarity(document_vec,tweet_vec):\n",
    "    from scipy.spatial.distance import cosine as cs\n",
    "    gensim_cosine_similarity=np.zeros(len(document_vec))\n",
    "    for i in range(0,len(document_vec)):\n",
    "        gensim_cosine_similarity[i]=cs(document_vec[i],tweet_vec)\n",
    "    return gensim_cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_sim_mat=get_gensim_cosine_similarity(gensim_item_vec,tweet_gensim_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.argsort(gensim_sim_mat)[-40:]\n",
    "#doc_basket[a]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TWEET2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
