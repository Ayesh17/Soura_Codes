{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s0c02nj/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/s0c02nj/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import sklearn.cross_validation\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import ensemble\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "import keras \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.cross_validation import train_test_split\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "#from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['Well done!','Good work','Great effort','nice work','Excellent!','Weak','Poor effort!','not good','poor work','Could have done better']\n",
    "# define class labels\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_size = 100\n",
    "#encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "#print(encoded_docs)\n",
    "#train_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the IMDB Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.datasets import imdb\n",
    "'''(x_train, y_train),(x_test, y_test) = imdb.load_data(path=\"imdb.npz\",num_words=None,seed=113)from keras.utils.data_utils import get_file\n",
    "path = get_file('imdb_full.pkl',\n",
    "               origin='https://s3.amazonaws.com/text-datasets/imdb_full.pkl',\n",
    "                md5_hash='d091312047c43cf9e4e38fef92437263')\n",
    "f = open(path, 'rb')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(x_train[4])\n",
    "#IMDB DATASET\n",
    "#f = open('../glove/glove.6B.100d.txt', encoding=\"utf-8\")\n",
    "\n",
    "print np.shape(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos=[]\n",
    "import os\n",
    "folder_path = '/Users/s0c02nj/Downloads/aclImdb/train/pos'\n",
    "#it will take the filenames inside the mentioned folder path one at a time\n",
    "for filename in os.listdir(folder_path):\n",
    "    #it will open the filename\n",
    "    f=open(folder_path + '/' + filename)\n",
    "    #readlines will read the lines of the particular folder\n",
    "    text=f.readlines()\n",
    "    train_pos.append(text[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copying the list of csv's from a folder\n",
    "train_neg=[]\n",
    "import os\n",
    "folder_path_neg = '/Users/s0c02nj/Downloads/aclImdb/train/neg'\n",
    "for filename in os.listdir(folder_path_neg):\n",
    "    f=open(folder_path_neg + '/' + filename)\n",
    "    text1=f.readlines()\n",
    "    train_neg.append(text1[0])\n",
    "    \n",
    "#makes that no of ones as the len of the item given\n",
    "pos_Y = [1]*len(train_pos)\n",
    "#makes that no of ones as the len of the item given\n",
    "neg_Y = [0]*len(train_neg)\n",
    "#rowwise appends the two arrays\n",
    "train_X = np.append(train_pos, train_neg)\n",
    "train_Y = np.append(pos_Y, neg_Y)\n",
    "#print(len(train_X))\n",
    "#print(len(train_Y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_udoc(document):\n",
    "    utf_doc=[]\n",
    "    for i in range(0,len(document)):\n",
    "        #val=unicode(document[i],errors=\"ignore\")\n",
    "        val=unicode(document[i],'ascii', 'ignore')\n",
    "        utf_doc.append(val)\n",
    "\n",
    "    return utf_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beautiful film, pure Cassavetes style. Gena Rowland gives a stunning performance of a declining actress, dealing with success, aging, loneliness...and alcoholism. She tries to escape her own subconscious ghosts, embodied by the death spectre of a young girl. Acceptance of oneself, of human condition, though its overall difficulties, is the real purpose of the film. The parallel between the theatrical sequences and the film itself are puzzling: it's like if the stage became a way out for the Heroin. If all american movies could only be that top-quality, dealing with human relations on an adult level, not trying to infantilize and standardize feelings... One of the best dramas ever. 10/10.\n"
     ]
    }
   ],
   "source": [
    "enc_trainx=get_udoc(train_X)\n",
    "print enc_trainx[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''enc_trainx=[]\n",
    "for i in range(0,len(train_X)):\n",
    "    enc=train_X[i].encode('utf8')\n",
    "    enc_trainx.append(enc)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/s0c02nj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Tokenizing the words\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_doc=[]\n",
    "for i in range(0,len(enc_trainx)):\n",
    "    word=word_tokenize(enc_trainx[i])\n",
    "    tokenized_doc.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'For', u'a', u'movie', u'that', u'gets', u'no', u'respect', u'there', u'sure', u'are', u'a', u'lot', u'of', u'memorable', u'quotes', u'listed', u'for', u'this', u'gem', u'.', u'Imagine', u'a', u'movie', u'where', u'Joe', u'Piscopo', u'is', u'actually', u'funny', u'!', u'Maureen', u'Stapleton', u'is', u'a', u'scene', u'stealer', u'.', u'The', u'Moroni', u'character', u'is', u'an', u'absolute', u'scream', u'.', u'Watch', u'for', u'Alan', u'``', u'The', u'Skipper', u\"''\", u'Hale', u'jr.', u'as', u'a', u'police', u'Sgt', u'.']\n"
     ]
    }
   ],
   "source": [
    "print tokenized_doc[0]\n",
    "#unicode(word, 'ascii', 'ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/s0c02nj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove Stop Words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stpwrd_free_doc = []\n",
    "doc=[]\n",
    "\n",
    "for i in range (len(tokenized_doc)):\n",
    "    for w in tokenized_doc[i]:\n",
    "        if w not in stop_words:\n",
    "            doc.append(w)\n",
    "    stpwrd_free_doc.append(doc)\n",
    "    doc=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_doc=[]\n",
    "alpha=[]\n",
    "for i in range(len(stpwrd_free_doc)):\n",
    "    for word in stpwrd_free_doc[i]:\n",
    "        if (word.isalpha()==True):\n",
    "            alpha.append(word)\n",
    "    alpha_doc.append(alpha)\n",
    "    alpha=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'For', u'movie', u'gets', u'respect', u'sure', u'lot', u'memorable', u'quotes', u'listed', u'gem', u'Imagine', u'movie', u'Joe', u'Piscopo', u'actually', u'funny', u'Maureen', u'Stapleton', u'scene', u'stealer', u'The', u'Moroni', u'character', u'absolute', u'scream', u'Watch', u'Alan', u'The', u'Skipper', u'Hale', u'police', u'Sgt']\n"
     ]
    }
   ],
   "source": [
    "print alpha_doc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a dictionary of the words and giving a sequence to the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s0c02nj/anaconda2/lib/python2.7/site-packages/keras_preprocessing/text.py:175: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "#alpha_doc[1]\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#Tokenizing the document and creating am encoding for all the words.\n",
    "tokenizer = Tokenizer(nb_words=72000)\n",
    "tokenizer.fit_on_texts(alpha_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequences converts the word to sequences for each doc\n",
    "sequences = tokenizer.texts_to_sequences(alpha_doc)\n",
    "#sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha_doc[0]\n",
    "#len(sequences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index of each word as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequences[0]\n",
    "#word_index is dictionary of the words and the sequence\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336\n"
     ]
    }
   ],
   "source": [
    "print word_index['boy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71652"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)\n",
    "#type(word_index)\n",
    "#word_index['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequences[11]\n",
    "len(sequences)\n",
    "#len(train_X)\n",
    "len_of_seq=[]\n",
    "for i in range(0,len(sequences)):\n",
    "    a=len(sequences[i])\n",
    "    len_of_seq.append(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len_of_seq[20149]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding the sequence for input to LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data stores the padded sequence \n",
    "data = pad_sequences(sequences, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data[10000])\n",
    "len(data)\n",
    "#LSTM input will take no of time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD2VEC  Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path= '/Users/s0c02nj/Downloads/glove.6B/glove.6B.50d.txt'\n",
    "f=open(folder_path)\n",
    "doc=f.readlines()\n",
    "#****VIP\n",
    "#**WORD TO VEC DICTIONARY\n",
    "#Forming a dictionary-word2vec\n",
    "word2vec={}\n",
    "key=[]\n",
    "#looping though the doc.in the doc the entire thing is saved and is separated by a space bar.\n",
    "for line in doc:\n",
    "    #parts contains every word separately for doc1\n",
    "    parts=line.split(' ')\n",
    "    #part[0] contains the word\n",
    "    word=parts[0]\n",
    "    key.append(word)\n",
    "    #embed contains the vector\n",
    "    embed=np.array(parts[1:],dtype='float32')\n",
    "    #filling up the dictionary\n",
    "    word2vec[word]=embed\n",
    "\n",
    "#NOW IF i give word as the key, the corresponding vec rep will return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(key)\n",
    "#word2vec['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "x = np.random.random(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_index['the']>>>>>>>>>>>3\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 50))\n",
    "null_vec=x\n",
    "embedding_matrix[0]=x\n",
    "for word, i in word_index.items():\n",
    "    #print i\n",
    "    if word in word2vec:\n",
    "        embedding_matrix[i] = word2vec[word]\n",
    "    else:\n",
    "        embedding_matrix[i] = x\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word,i in  word_index.items()[0]\n",
    "#embedding_matrix\n",
    "#i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deferment\n",
      "(71653, 50)\n"
     ]
    }
   ],
   "source": [
    "print word_index.items()[4][0]\n",
    "print np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word, i in word_index.items()\n",
    "#word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_matrix[0]-x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix[2].shape\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_index.items()\n",
    "vocabulary_size=71653\n",
    "input_length=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "cat_y=to_categorical(train_Y, num_classes=2)\n",
    "\n",
    "#x_scaled=sklearn.preprocessing.scale(x, axis=0, with_mean=True, with_std=True, copy=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, cat_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print cat_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "model_glove = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove.add(Embedding(vocabulary_size, 50, input_length=200, weights=[embedding_matrix], trainable=True))\n",
    "#model_glove.add(Dropout(0.2))\n",
    "#model_glove.add(Conv1D(64, 5, activation='relu'))\n",
    "#model_glove.add(MaxPooling1D(pool_size=4))\n",
    "model_glove.add(LSTM(100,dropout=0.2))\n",
    "model_glove.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 50)           3582650   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 3,643,252\n",
      "Trainable params: 3,643,252\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_glove.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "model_glove.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_glove.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_glove.fit(data, np.array(cat_y), validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
