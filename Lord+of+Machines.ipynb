{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s0c01xa/Desktop/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import linear_model\n",
    "import sklearn.cross_validation\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import ensemble\n",
    "import time\n",
    "import random\n",
    "import gensim\n",
    "from gensim.models import doc2vec\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydata=pd.read_csv('/Users/s0c01xa/Documents/Kaggle/Lord of machines.csv')\n",
    "mydata=np.array(mydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=mydata[:,1]\n",
    "docLabels=mydata[:,0]\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOC TO VEC USING GENSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "#data saves the documents and doclables has just the labels for the corresponding docs\n",
    "#docLabels has just the ids of the documents\n",
    "data=mydata[:,1]\n",
    "docLabels=mydata[:,0]\n",
    "# Transform data (you can add more data preprocessing steps) \n",
    "\n",
    "#Creating a tuple since gensim takes it in the form of tuples,where there will be the doc\n",
    "#attached will be the label\n",
    "docs = []\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "\n",
    "#converting it to int type\n",
    "'''for i in range(0,len(docLabels)):\n",
    "    docLabels[i]=int(docLabels[i])'''\n",
    "\n",
    "#converting the words to lowercase and appending it in docs in the form of tuples\n",
    "#docs[0] will contain the tuple of first doc and it's label\n",
    "for i in range (0,len(data)):\n",
    "    words = data[i].lower().split()\n",
    "    tags=[str(docLabels[i])]\n",
    "    docs.append(analyzedDocument(words, tags))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOC TO VEC CONVERSION USING GENSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "from collections import namedtuple\n",
    "\n",
    "# Train model (set min_count = 1, if you want the model to work with the provided example data set)\n",
    "model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 1, workers = 4)\n",
    "\n",
    "#model.docvecs[i] saves the corresponding vector for the document with label numbers mentioned in tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.docvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_clust_data=pd.read_csv('/Users/s0c01xa/Documents/train_HFxi8kT/train.csv')\n",
    "campaign_data=pd.read_csv('/Users/s0c01xa/Documents/train_HFxi8kT/campaign_data.csv')\n",
    "#train_clust_data=np.array(train_clust_data)\n",
    "#train_clust_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb_data= train_clust_data[['user_id','campaign_id','is_open']]\n",
    "#comb_data2=train_clust_data[['user_id','campaign_id','is_click']]\n",
    "#camp_id= train_clust_data[:,1]\n",
    "#is_open=train_clust_data[:,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb_data2=train_clust_data[['user_id','campaign_id','is_click']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pandascrosstab output checking\n",
    "df = train_clust_data.pivot(index = 'user_id', columns ='campaign_id', values = 'is_open').fillna(0)\n",
    "userId = df.index\n",
    "#df=np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=np.array(df)\n",
    "userId=np.array(userId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clustering algorithm on df to cluster the set of users\n",
    "#No of clusters n_clust will vary and will be adjusted \n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=20, random_state=1).fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Labels it will store the entire labels for each of the \n",
    "clust_labels=kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#f_data stores the user id in the first column and the corr. cluster labels in the second column\n",
    "f_data=np.column_stack((userId,clust_labels))\n",
    "#f_data=pd.DataFrame(f_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2196"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It saves the length of the ids with the corresponding clust labels.\n",
    "#len(f_data[clust_labels==i][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_data=pd.DataFrame(f_data)\n",
    "#f_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLOOKUP IN PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FIT CLASSIFICATION MODELS FOR EACH CLUSTER\n",
    "#which is identified clust_labels==i\n",
    "#Implementing a vlookup in python\n",
    "#first element is the column no where this new column will be added\n",
    "#second element is the column name in the first sheet which needs to be filled\n",
    "#third element is the mapping in the second sheet and the name of the column to be mapped\n",
    "\n",
    "#adding columns to python\n",
    "#comb_data.insert(3, 'communication_type', comb_data['campaign_id'].map(campaign_data.set_index('campaign_id')['communication_type']))\n",
    "#comb_data.insert(4, 'subject', comb_data['campaign_id'].map(campaign_data.set_index('campaign_id')['subject']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#comb_data stores the entire information with the users and the corresponding subject and comm_type\n",
    "#which is needed to predict is_open\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOC TO VEC FOR CAMPAIGN ID OF TRAINING DATA  BASED ON SUBJECT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "#data saves the documents and doclables has just the labels for the corresponding docs\n",
    "#docLabels has just the ids of the documents\n",
    "data=mydata[:,1]\n",
    "docLabels=mydata[:,0]\n",
    "# Transform data (you can add more data preprocessing steps) \n",
    "\n",
    "#Creating a tuple since gensim takes it in the form of tuples,where there will be the doc\n",
    "#attached will be the label\n",
    "docs = []\n",
    "w2v=np.zeros(52)\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "\n",
    "#converting it to int type\n",
    "'''for i in range(0,len(docLabels)):\n",
    "    docLabels[i]=int(docLabels[i])'''\n",
    "\n",
    "#converting the words to lowercase and appending it in docs in the form of tuples\n",
    "#docs[0] will contain the tuple of first doc and it's label\n",
    "for i in range (0,len(data)):\n",
    "    words = data[i].lower().split()\n",
    "    tags=[str(docLabels[i])]\n",
    "    docs.append(analyzedDocument(words, tags))\n",
    "    w2v[i]=str(docLabels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train model (set min_count = 1, if you want the model to work with the provided example data set)\n",
    "model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 1, workers = 4)\n",
    "#model.docvecs[i] saves the corresponding vector for the document with label numbers mentioned in tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec=np.zeros((52,100))\n",
    "for i in range(0,len(w2v)):\n",
    "    word2vec[i]=model.docvecs[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#final word to vec is being saved with corresponding campaign id and is stored in word2vec_f\n",
    "word2vec_f=np.column_stack((w2v,word2vec))\n",
    "word2vec_f=pd.DataFrame(word2vec_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLOOKUP-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first element is the column no where this new column will be added\n",
    "#second element is the column name in the first sheet which needs to be filled\n",
    "#third element is the mapping in the second sheet and the name of the column to be mapped\n",
    "\n",
    "#for loop loops through and adds the number of columns we need, here the vectors are added in comb_data\n",
    "for i in range(0,100):\n",
    "    comb_data.insert(i+5, i, comb_data['campaign_id'].map(word2vec_f.set_index(0)[i+1]))\n",
    "    \n",
    "#Vectors are mapped and saved in comb_data-main document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1=comb_data.drop(comb_data[['campaign_id', 'is_open', 'subject']], axis=1)\n",
    "y1=comb_data['is_open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Encoding the communication type\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "communication_type=le.fit_transform(x1['communication_type'])\n",
    "x1.insert(102,'comm_type',communication_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLOOKUP-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now the cluster labels are also present inside the data x1. We need to filter by Clust_labels and do the classif\n",
    "#save the model\n",
    "#x1.drop(x1[['communication_type']], axis=1)\n",
    "x1=x1.drop(x1[['Clust_labels','is_open']], axis=1)\n",
    "#x1.insert(102, 'Clust_labels', comb_data['user_id'].map(f_data.set_index(0)[1]))\n",
    "#x1.insert(103,'is_open',y1)\n",
    "\n",
    "#x1 and y1 now contains the important features for the first phase i.e for is_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1.insert(103, 'Clust_labels', comb_data['user_id'].map(f_data.set_index(0)[1]))\n",
    "x1.insert(104,'is_open',y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOC TO VEC FOR CAMPAIGN ID OF TRAINING DATA  BASED ON BODY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "#data saves the documents and doclables has just the labels for the corresponding docs\n",
    "#docLabels has just the ids of the documents\n",
    "\n",
    "campaign_data=pd.read_csv('/Users/s0c01xa/Documents/train_HFxi8kT/campaign_data.csv')\n",
    "\n",
    "data2=campaign_data['email_body']\n",
    "docLabels2=campaign_data['campaign_id']\n",
    "\n",
    "# Transform data (you can add more data preprocessing steps) \n",
    "\n",
    "#Creating a tuple since gensim takes it in the form of tuples,where there will be the doc\n",
    "#attached will be the label\n",
    "docs2 = []\n",
    "w2v2=np.zeros(52)\n",
    "analyzedDocument2 = namedtuple('AnalyzedDocument', 'words tags')\n",
    "\n",
    "#converting the words to lowercase and appending it in docs in the form of tuples\n",
    "#docs[0] will contain the tuple of first doc and it's label\n",
    "for i in range (0,len(data2)):\n",
    "    words = data2[i].lower().split()\n",
    "    tags=[str(docLabels2[i])]\n",
    "    docs2.append(analyzedDocument(words, tags))\n",
    "    w2v2[i]=str(docLabels2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train model (set min_count = 1, if you want the model to work with the provided example data set)\n",
    "model2 = doc2vec.Doc2Vec(docs2, size = 100, window = 300, min_count = 1, workers = 4)\n",
    "#model2.docvecs[i] saves the corresponding vector for the document with label numbers mentioned in tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "body2vec=np.zeros((52,100))\n",
    "for i in range(0,len(w2v2)):\n",
    "    body2vec[i]=model2.docvecs[i]\n",
    "\n",
    "#final word to vec is being saved with corresponding campaign id and is stored in word2vec_f\n",
    "body2vec_f=np.column_stack((w2v2,body2vec))\n",
    "body2vec_f=pd.DataFrame(body2vec_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#body2vec_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding important features for predicting is_click\n",
    "\n",
    "comb_data2.insert(3, 'total_links', comb_data2['campaign_id'].map(campaign_data.set_index('campaign_id')['total_links']))\n",
    "comb_data2.insert(4, 'no_of_internal_links', comb_data2['campaign_id'].map(campaign_data.set_index('campaign_id')['no_of_internal_links']))\n",
    "comb_data2.insert(5, 'no_of_images', comb_data2['campaign_id'].map(campaign_data.set_index('campaign_id')['no_of_images']))\n",
    "comb_data2.insert(6, 'no_of_sections', comb_data2['campaign_id'].map(campaign_data.set_index('campaign_id')['no_of_sections']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding body2vec elements in combdata2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#comb_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first element is the column no where this new column will be added\n",
    "#second element is the column name in the first sheet which needs to be filled\n",
    "#third element is the mapping in the second sheet and the name of the column to be mapped\n",
    "\n",
    "#for loop loops through and adds the number of columns we need, here the vectors are added in comb_data\n",
    "\n",
    "for i in range(0,100):\n",
    "    comb_data2.insert(i+7, i, comb_data2['campaign_id'].map(body2vec_f.set_index(0)[i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#comb_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x2=comb_data2.drop(comb_data2[['campaign_id', 'is_click']], axis=1)\n",
    "y2=comb_data2['is_click']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x2\n",
    "#comb_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x2.insert(105, 'Clust_labels', comb_data2['user_id'].map(f_data.set_index(0)[1]))\n",
    "#x2.insert(106, 'Clust_labels', comb_data2['user_id'].map(f_data.set_index(0)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x2 contains the features needed to model for the isclick response\n",
    "#y2 response of isclick\n",
    "#x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL BUILDING TWO LEVEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-733-8daf6d516a63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mres1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mfeat1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m104\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_open_prob'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeat1_isclick\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mres1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/s0c01xa/Desktop/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/logistic.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1231\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/s0c01xa/Desktop/anaconda2/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    851\u001b[0m             raise ValueError(\"This solver needs samples of at least 2 classes\"\n\u001b[1;32m    852\u001b[0m                              \u001b[0;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m                              \" class: %r\" % classes_[0])\n\u001b[0m\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0mclass_weight_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression as lg\n",
    "\n",
    "model1=lg(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None)\n",
    "model2=lg(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None)\n",
    "\n",
    "\n",
    "file_model1=\"model\"\n",
    "\n",
    "\n",
    "for i in range(0,20):\n",
    "    y = x1.loc[x1['Clust_labels']==i]\n",
    "    feat=y.iloc[:,2:102]\n",
    "    res=y.iloc[:,-1]\n",
    "    model1.fit(feat,res)\n",
    "    s = pickle.dump(model, open(file_model1+str(i), 'w'))\n",
    "    #isopen_model1.append(\"isopen_model1\"+str(i))\n",
    "    #feat1 will have the probabilities of opening a mail\n",
    "    feat1_isclick=model1.predict_proba(feat)[:,1]\n",
    "    #Features for model2 needs to be predicted\n",
    "    y2 = x2.loc[x2['Clust_labels']==i]\n",
    "    feat1=y2.iloc[:,1:105]\n",
    "    res1=y2.iloc[:,-1]\n",
    "    feat1.insert(104, 'is_open_prob',feat1_isclick)\n",
    "    model2.fit(feat1,res1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE FUZZY AUGMENTATION/ANY OTHER AUGMENTATION TECHNIQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_clust_data['is_click']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y2 = x2.loc[x2['Clust_labels']==0]\n",
    "feat1=y2.iloc[:,1:105]\n",
    "#res1=y2.iloc[:,-1]\n",
    "#feat1.insert(104, 'is_open_prob',feat1_isclick)\n",
    "#y2\n",
    "\n",
    "#len(feat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1=lg(penalty='l2', dual=False, tol=0.001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None)\n",
    "y=x1.loc[x1['Clust_labels']==1]\n",
    "feat=y.iloc[:,2:102]\n",
    "res=y.iloc[:,-1]\n",
    "\n",
    "model1.fit(feat,res)\n",
    "\n",
    "feat1_isclick=model1.predict_proba(feat)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cross_val_score(model1,feat,res,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression as lg\n",
    "\n",
    "model1=lg(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None)\n",
    "model2=lg(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None)\n",
    "\n",
    " \n",
    "#file_model=\"/Users/s0c01xa/Documents/models/model\"\n",
    "\n",
    "\n",
    "for i in range(0,20):\n",
    "    y = x1.loc[x1['Clust_labels']==i]\n",
    "    feat=y.iloc[:,2:102]\n",
    "    res=y.iloc[:,-1]\n",
    "    model.fit(feat,res)\n",
    "    #the file name and location is specified\n",
    "    #In the folder pklmodel,with the model number corr to cluster numbers,the models are being stored.\n",
    "    #Here the strings/names are made\n",
    "    filename = \"/Users/s0c01xa/Documents/pkl_models/model\"+'_'+str(i+1)+'.pkl'\n",
    "    #Open the filename to write 'w'\n",
    "    file_object = open(filename, 'w')\n",
    "    #dumps the model\n",
    "    s = pickle.dump(model, file_object)\n",
    "    #close the file\n",
    "    file_object.close()\n",
    "    \n",
    "#pkl model stores all the model, now for test data we can use the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 753,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.81809045,  0.81790744,  0.81991952,  0.81690141,  0.81589537,\n",
       "        0.81772407,  0.80664653,  0.81873112,  0.82678751,  0.82477341])"
      ]
     },
     "execution_count": 756,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x1.loc[x1['Clust_labels']==10]\n",
    "feat=y.iloc[:,2:102]\n",
    "res=y.iloc[:,-1]\n",
    "cross_val_score(model,feat,res,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
