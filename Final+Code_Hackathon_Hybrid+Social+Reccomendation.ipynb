{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING \n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "from sklearn import linear_model\n",
    "import sklearn.cross_validation\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import ensemble\n",
    "import time\n",
    "import random\n",
    "\n",
    "##########\n",
    "def get_tokenized_doc(document):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokenized_doc=[]\n",
    "    for i in range(0,len(document)):\n",
    "        word=word_tokenize(document[i])\n",
    "        tokenized_doc.append(word)\n",
    "    \n",
    "    return tokenized_doc\n",
    "\n",
    "##########\n",
    "#Converting all words to lower case so that it is of same type at par with word to vec\n",
    "def get_lower_doc(document):\n",
    "    low_words=[]\n",
    "    low_doc=[]\n",
    "    for i in range(len(document)):\n",
    "        for word in document[i]:\n",
    "            lwords=word.lower()\n",
    "            low_words.append(lwords)\n",
    "        low_doc.append(low_words)\n",
    "        low_words=[]\n",
    "    return low_doc\n",
    "\n",
    "##########\n",
    "#Given a document,remove the stopwords\n",
    "def get_stopwrd_free_doc(document):\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stpwrd_free_doc = []\n",
    "    doc=[]\n",
    "\n",
    "    for i in range (len(document)):\n",
    "        for w in document[i]:\n",
    "            if w not in stop_words:\n",
    "                doc.append(w)\n",
    "        stpwrd_free_doc.append(doc)\n",
    "        doc=[]\n",
    "    \n",
    "    return stpwrd_free_doc\n",
    "\n",
    "##########\n",
    "#Removing non alpha words\n",
    "def get_alpha_doc(document):\n",
    "    alpha_doc=[]\n",
    "    alpha=[]\n",
    "    for i in range(len(document)):\n",
    "        for word in document[i]:\n",
    "            if (word.isalpha()==True):\n",
    "                alpha.append(word)\n",
    "        alpha_doc.append(alpha)\n",
    "        alpha=[]\n",
    "        \n",
    "    return alpha_doc\n",
    "\n",
    "def get_lem_doc(document):\n",
    "    from nltk import WordNetLemmatizer\n",
    "    from nltk.corpus import wordnet\n",
    "    wnl=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "    lem_words=[]\n",
    "    lem_doc=[]\n",
    "\n",
    "    for i in range(len(document)):\n",
    "        for word in document[i]:\n",
    "            lem=wnl.lemmatize(word)\n",
    "            lem_words.append(lem)\n",
    "        \n",
    "        lem_doc.append(lem_words)\n",
    "        lem_words=[]\n",
    "\n",
    "\n",
    "    return lem_doc\n",
    "\n",
    "\n",
    "##########\n",
    "def get_string_version(document):\n",
    "    Str_data=[]\n",
    "    for i in range(0,len(document)):\n",
    "        val=str(document[i])\n",
    "        Str_data.append(val)\n",
    "    return Str_data\n",
    "\n",
    "\n",
    "##########\n",
    "def get_udoc(document):\n",
    "    utf_doc=[]\n",
    "    for i in range(0,len(document)):\n",
    "        val=unicode(x[i],errors=\"ignore\")\n",
    "        utf_doc.append(val)\n",
    "\n",
    "    return utf_doc\n",
    "\n",
    "\n",
    "##########\n",
    "def get_dense_matrix(term_frquency):\n",
    "    from scipy.sparse import csr_matrix\n",
    "    dense_trm_frq=term_frquency.todense()\n",
    "    return dense_trm_frq\n",
    "\n",
    "\n",
    "##########\n",
    "def get_data_with_cluster_labels(filepath):\n",
    "    #filepath='/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels.csv'\n",
    "    data_with_clust_labels=pd.read_csv(filepath)\n",
    "    return data_with_clust_labels\n",
    "\n",
    "##########\n",
    "data_with_cluster_labels=get_data_with_cluster_labels('/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels.csv')\n",
    "#data_with_cluster_labels\n",
    "#filepath='/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels.csv'\n",
    "\n",
    "\n",
    "##########\n",
    "import pickle\n",
    "filename = \"/Users/s0c01xa/Documents/pkl_models hackathon/model.pkl\"\n",
    "#Open the filename to write 'w'\n",
    "file_object = open(filename, 'r')\n",
    "#dumps the model\n",
    "term_frequency_model = pickle.load(file_object)\n",
    "\n",
    "\n",
    "##########\n",
    "# load json and create model\n",
    "from keras.models import model_from_json\n",
    "json_file = open('deep_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"deep_model.h5\")\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "##########\n",
    "deep_model=loaded_model\n",
    "\n",
    "##########\n",
    "mydata=get_data_with_cluster_labels(\"/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels.csv\")   #mydata saves the combined data with cluster labels\n",
    "#x-item description is being stored and y stores the corresponding labels\n",
    "#x=mydata['Combined Description']\n",
    "y=mydata['Social Intent']\n",
    "#Label Encoder and converting the y's into integers\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_coded=le.fit_transform(y)\n",
    "\n",
    "\n",
    "##########\n",
    "def get_tweet_data(tweet_filepath):\n",
    "    from nltk import word_tokenize\n",
    "    #filepath='/Users/s0c01xa/Documents/Walmart_Bplan/Gen_tweet_3.csv'\n",
    "    f=open(tweet_filepath)\n",
    "    tweet_data=f.readline()\n",
    "    tweet=word_tokenize(tweet_data)\n",
    "    return tweet\n",
    "\n",
    "##########\n",
    "def get_processed_tweet(tweet_filepath):\n",
    "    from nltk import word_tokenize\n",
    "    f=open(tweet_filepath)\n",
    "    tweet_data=f.readline()\n",
    "    tweet=word_tokenize(tweet_data)\n",
    "    val_test=''\n",
    "    for i in tweet:\n",
    "        val=i\n",
    "        #print val\n",
    "        val_test=str(val+\" \"+val_test)\n",
    "    return val_test\n",
    "\n",
    "##########\n",
    "def get_dense_tweet(tweet_filepath,model):\n",
    "    processed_tweet=get_processed_tweet(tweet_filepath)\n",
    "    test_tweet=model.transform([processed_tweet])\n",
    "    dense_tweet=get_dense_matrix(test_tweet)\n",
    "    return dense_tweet\n",
    "\n",
    "##########\n",
    "def get_top3_social_content(tweet_filepath,model,le,deep_model):\n",
    "    #filepath=filepath\n",
    "    #model=model\n",
    "    tweet_dense= get_dense_tweet(tweet_filepath,model)\n",
    "    similar_docs=le.inverse_transform(np.argsort(deep_model.predict_proba(tweet_dense)))\n",
    "    top3_social_content=similar_docs[0][-3:]\n",
    "    \n",
    "    return top3_social_content\n",
    "    \n",
    "##########\n",
    "def get_preprocessed_recco_doc(document):\n",
    "    doc_basket1=get_tokenized_doc(document)\n",
    "    doc_basket2=get_lower_doc(doc_basket1)\n",
    "    doc_basket3=get_stopwrd_free_doc(doc_basket2)\n",
    "    doc_basket4=get_alpha_doc(doc_basket3)\n",
    "    doc_basket5=get_lem_doc(doc_basket4)\n",
    "\n",
    "    return doc_basket5\n",
    "\n",
    "##########\n",
    "def get_top20_similar_items_with_tweet(social_content,tweet_filepath):\n",
    "    \n",
    "    tweet=get_tweet_data(tweet_filepath)\n",
    "    df=get_data_with_cluster_labels('/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels.csv')\n",
    "    filtered_data=df[df['Social Intent']==social_content]\n",
    "    doc_basket=filtered_data['Combined Description']\n",
    "    doc_basket=np.array(doc_basket)\n",
    "\n",
    "    preprocessed_recco_doc=get_preprocessed_recco_doc(doc_basket)\n",
    "    #similarity basket saves the items which are similar to the tweet in most similar cluster label\n",
    "    similarity_basket=[]\n",
    "    for i in range(0,len(preprocessed_recco_doc)):\n",
    "        common=len(set(preprocessed_recco_doc[i]).intersection( set(tweet)))\n",
    "        similarity_basket.append(common)\n",
    "\n",
    "    sim=np.argsort(similarity_basket)[-20:]\n",
    "    top_twnty_sim_doc=doc_basket[sim]\n",
    "\n",
    "    return top_twnty_sim_doc\n",
    "\n",
    "\n",
    "##########\n",
    "def get_top_4items_social_content(social_content,tweet_filepath):\n",
    "    document=get_top20_similar_items_with_tweet(social_content,tweet_filepath)\n",
    "    #document=get_top20_similarity_doc_with_tweet(cluster,tweet)\n",
    "    top_4items=[]\n",
    "    #Saving the particular characteristics giving the best feature size\n",
    "    model=sklearn.feature_extraction.text.CountVectorizer(input='content', encoding='utf-8', decode_error='strict', \n",
    "                                                strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, \n",
    "                                                stop_words=None, ngram_range=(1,3), analyzer='word', \n",
    "                                                max_df=10000, min_df=0,max_features=1000,\n",
    "                                                vocabulary=None, binary=False)\n",
    "\n",
    "    #Transforming the data to count of token\n",
    "    term_frequency=model.fit_transform(document)\n",
    "    dense_term_frequency=get_dense_matrix(term_frequency)\n",
    "\n",
    "    #Kmeans Clustering in the dense_matrix\n",
    "    from sklearn.cluster import KMeans\n",
    "    kclust = KMeans(n_clusters=4)\n",
    "    kclust.fit(dense_term_frequency)\n",
    "    clust_labels=kclust.labels_\n",
    "    top_4items.append((document[clust_labels==0][0],document[clust_labels==1][0]\n",
    "                       ,document[clust_labels==2][0],document[clust_labels==3][0] ))\n",
    "                    \n",
    "    \n",
    "    return top_4items\n",
    "\n",
    "\n",
    "##########\n",
    "def get_top12_sim_items(tweet_filepath,model,le,deep_model):\n",
    "    #filepath=tweet_filepath\n",
    "    document=get_top3_social_content(tweet_filepath,model,le,deep_model)\n",
    "    top12_recco=[]\n",
    "    for i in document:\n",
    "        #word=document[i]\n",
    "        #sim_doc=get_top20_similar_items_with_tweet(document[i],tweet_filepath)\n",
    "        #top4_doc=get_top_4recco(sim_doc)\n",
    "        top4_doc=get_top_4items_social_content(i,tweet_filepath)\n",
    "        top12_recco.append((top4_doc,('Frequency Match')))\n",
    "    return top12_recco\n",
    "\n",
    "\n",
    "##########\n",
    "def get_gensim_item_vec(document):\n",
    "    gensim_w2v=np.zeros(((len(document),50)))\n",
    "    for i in range(0,len(document)):\n",
    "        gensim_w2v[i]=document[i]\n",
    "        \n",
    "    return gensim_w2v\n",
    "\n",
    "\n",
    "##########\n",
    "def get_gensim_w2v_sim_matrix(social_content,tweet_filepath):\n",
    "    tweet=get_processed_tweet(tweet_filepath)\n",
    "    \n",
    "    df=get_data_with_cluster_labels('/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels.csv')\n",
    "    filtered_data=df[df['Social Intent']==social_content]\n",
    "    doc_basket=filtered_data['Combined Description']\n",
    "    doc_basket=np.array(doc_basket)\n",
    "\n",
    "    #document=get_top20_similar_items_with_tweet(social_content,tweet_filepath)\n",
    "    from gensim.models import doc2vec\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    data=doc_basket\n",
    "    docs = []\n",
    "    analyzedDocument = namedtuple('AnalyzedDocument', 'words tags') \n",
    "    #converting the words to lowercase and appending it in docs in the form of tuples\n",
    "    #docs[0] will contain the tuple of first doc and it's label\n",
    "    for i in range (0,len(data)):\n",
    "        words = data[i].lower().split()\n",
    "        tags=[str(i)]\n",
    "        docs.append(analyzedDocument(words, tags))\n",
    "\n",
    "    # Train model (set min_count = 1, if you want the model to work with the provided example data set)\n",
    "    model = doc2vec.Doc2Vec(docs, size = 50, window = 300, min_count = 1, workers = 5)\n",
    "    gensim_item_vec=get_gensim_item_vec(model.docvecs)\n",
    "    return gensim_item_vec\n",
    "\n",
    "##########\n",
    "def get_gensim_vec_tweet(tweet_filepath):\n",
    "    tweet_filepath=tweet_filepath\n",
    "    from gensim.models import doc2vec\n",
    "    from collections import namedtuple\n",
    "    tweet_doc=get_tweet_data(tweet_filepath)\n",
    "    data=str(tweet_doc)\n",
    "    docs = []\n",
    "    analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "\n",
    "\n",
    "    words = data.lower().split()\n",
    "    tags=[str(0)]\n",
    "    docs.append(analyzedDocument(words, tags))\n",
    "\n",
    "    # Train model (set min_count = 1, if you want the model to work with the provided example data set)\n",
    "    tweet_model = doc2vec.Doc2Vec(docs, size = 50, window = 300, min_count = 1, workers = 5)\n",
    "    tweet_gensim_vec=tweet_model.docvecs[0]\n",
    "    return tweet_gensim_vec\n",
    "\n",
    "\n",
    "##########\n",
    "def get_gensim_cosine_similarity(document_vec,tweet_vec):\n",
    "    from scipy.spatial.distance import cosine as cs\n",
    "    gensim_cosine_similarity=np.zeros(len(document_vec))\n",
    "    for i in range(0,len(document_vec)):\n",
    "        gensim_cosine_similarity[i]=cs(document_vec[i],tweet_vec)\n",
    "    return gensim_cosine_similarity\n",
    "\n",
    "\n",
    "##########\n",
    "def get_gensim_item_tweet_matrix(social_content,tweet_filepath):\n",
    "    #doc_social_content=doc_social_content\n",
    "    #tweet_filepath=tweet_filepath\n",
    "    gensim_item_matrix=get_gensim_w2v_sim_matrix(social_content,tweet_filepath)\n",
    "    tweet_gensim_vec=get_gensim_vec_tweet(tweet_filepath)\n",
    "    gensim_item_tweet_matrix=get_gensim_cosine_similarity(gensim_item_matrix,tweet_gensim_vec)\n",
    "    return gensim_item_tweet_matrix\n",
    "\n",
    "\n",
    "##########\n",
    "def get_preprocessed_recco_doc(document):\n",
    "    doc_basket1=get_tokenized_doc(document)\n",
    "    doc_basket2=get_lower_doc(doc_basket1)\n",
    "    doc_basket3=get_stopwrd_free_doc(doc_basket2)\n",
    "    doc_basket4=get_alpha_doc(doc_basket3)\n",
    "    doc_basket5=get_lem_doc(doc_basket4)\n",
    "\n",
    "    return doc_basket5\n",
    "\n",
    "\n",
    "##########\n",
    "def get_gensim_similarity_doc_with_tweet(social_content,tweet_filepath):\n",
    "    \n",
    "    df=get_data_with_cluster_labels('/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels.csv')\n",
    "    filtered_data=df[df['Social Intent']==social_content]\n",
    "    gensim_doc_basket=filtered_data['Combined Description']\n",
    "    gensim_doc_basket=np.array(gensim_doc_basket)\n",
    "    \n",
    "    gensim_item_tweet_matrix=get_gensim_item_tweet_matrix(social_content,tweet_filepath)\n",
    "    \n",
    "    sim=np.argsort(gensim_item_tweet_matrix)[-4:]\n",
    "    top_4_recco=gensim_doc_basket[sim]\n",
    "    return top_4_recco\n",
    "\n",
    "\n",
    "##########\n",
    "def get_top12_gensim_sim_items(tweet_filepath,model,le,deep_model):\n",
    "    document=get_top3_social_content(tweet_filepath,model,le,deep_model)\n",
    "    top12_gensim_recco=[]\n",
    "    for i in document:\n",
    "        tweet_filepath=tweet_filepath\n",
    "        gensim_sim_doc=get_gensim_similarity_doc_with_tweet(i,tweet_filepath)\n",
    "        top12_gensim_recco.append((gensim_sim_doc,('Context Based')))\n",
    "    return top12_gensim_recco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s0c01xa/Desktop/anaconda2/lib/python2.7/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([ 'SEAMLESS SLEEP SET SEAMLESS SLEEP SET NO BOUDARIES SEAMLESS SLEEP SET NO BOUNDARIES',\n",
       "         \"LIC TANK SET DME3 DEL WK 26 2017 LICENSE WOMEN'S AND WOMEN'S PLUS MUSCLE LICENSE\",\n",
       "         'ST WAFFLE KNIT ROBE DEL WK 02 2017 WAF ROBE MISMNT SECRET TREASURES',\n",
       "         'LICENSE COAT SET DEL WK 44 2016 MONSTER HIGH COAT SETS MONSTER HIGH AP'], dtype=object),\n",
       "  'Context Based'),\n",
       " (array(['CATSPACE WS WK 22 TEST 2016 MENS SS TEE LICENSE',\n",
       "         'PEACEPNT WK39 SPN TEST FY16 MENS SS TEE LICENSE',\n",
       "         'T-SHIRTS ONLINE ONLY MARILYN TEE LICENSE',\n",
       "         'T-SHIRTS ONLINE ONLY MENS SS TEE LICENSE'], dtype=object),\n",
       "  'Context Based'),\n",
       " (array([ 'GRAPHIC TEES#HOODIES ONLINE ONLY GIRLS HELLO KITTY ZIP VELOUR HOODIE HELLO KITTY AP',\n",
       "         'SUPERMAN SOLID LIC HOODIE SUPERMAN AP',\n",
       "         'MINECRAFT CRPR GLOW CREEPER GLOW BOYS TEES MINECRAFT AP',\n",
       "         'POKEMON PIKACHU POKEBALL PIKACHU POKEBALL 10/12 POKEMON AP'], dtype=object),\n",
       "  'Context Based')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_filepath=\"/Users/s0c01xa/Documents/Walmart_Bplan/Gen_Tweet.csv\"\n",
    "get_top12_gensim_sim_items(tweet_filepath,term_frequency_model,le,deep_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MINNIE SS TOP DOLMAN TOP MINNIE SS PINK TOP 4/5 MINNIE MOUSE AP',\n",
       "       'SNOW QUEEN TOP DISNEY FROZEN SNOW QUEEN ROCKS S/S TOP FROZEN AP',\n",
       "       'SG SUB TIEFRNT TOP SPRGRL SUBL TIE FRNT SUPERGIRL SUBLIMATION TIE FRONT TOP SUPERMAN AP',\n",
       "       'T-SHIRTS ONLINE ONLY MARVEL CAPTAIN AMERICA FOUR SIDED CAP BO ONLINE',\n",
       "       'GRAPHIC TEES ONLINE ONLY S/S SPRGL WOW STRONG RINGER TEE SUPERMAN AP',\n",
       "       'T-SHIRTS ONLINE ONLY ^^MARVEL CAPTAIN AMERICA FLAKSHIELD BOYS ONLINE',\n",
       "       'NEON LOVE TEE KNIT TOP NEON LOVE S/S TEE LICENSE',\n",
       "       'GRAPHIC TEES#TSHIRTS ONLINE ONLY FINDING DORY SS HI LOW CROCHET TOP FINDING DORY AP',\n",
       "       'T-SHIRTS ONLINE ONLY 80S CAPTAIN RYLHTR CAPTAIN AMERICA',\n",
       "       'T-SHIRTS ONLINE ONLY ^^MARVEL CAPTAIN AMERICA LOGO BOYS GRAPH ONLINE',\n",
       "       'CAP AM SS HI LO TOP S3 JULY DELIVERY CPTN AMERICA SS HACCI W/LACE HANGDWN TOP CAPTAIN AMERICA AP',\n",
       "       'SPIDERMAN 80S CAPTAIN BOYS POLY TEE CAPTAIN AMERICA AP',\n",
       "       'SPIDERMAN 80S CAPTAIN BOYS POLY TEE CAPTAIN AMERICA AP',\n",
       "       'SPIDERMAN 80S CAPTAIN BOYS POLY TEE CAPTAIN AMERICA AP',\n",
       "       'CAPTAIN AMERICA STAY HUNGRY BOYS POLY TEE SPIDERMAN AP',\n",
       "       'CAPTAIN AMERICA STAY HUNGRY BOYS POLY TEE SPIDERMAN AP',\n",
       "       'SPIDERMAN 80S CAPTAIN BOYS POLY TEE CAPTAIN AMERICA AP',\n",
       "       'CAPTAIN AMERICA STAY HUNGRY BOYS POLY TEE SPIDERMAN AP',\n",
       "       'CAP AM SS HI LO TOP S3 JULY DELIVERY CPTN AMERICA SS HACCI W/LACE HANGDWN TOP CAPTAIN AMERICA AP',\n",
       "       'CAP AM SS HI LO TOP S3 JULY DELIVERY CPTN AMERICA SS HACCI W/LACE HANGDWN TOP CAPTAIN AMERICA AP'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top20_similar_items_with_tweet('Licensed',tweet_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('GENERIC TANK LOVE CROCHET TANK GENERIC TOP LICENSE',\n",
       "  'JR FASHION TOP HP CROSSING HARRY POTTER JR FASHION TOP HARRY POTTER',\n",
       "  'LION KING HILO TANK  DISNEY TOP THE LION KING AP',\n",
       "  'CAPTAIN SHIELD PANEL CAPTAIN SHIELD PANEL JUNIORS TANK TOP CAPTAIN AMERICA AP')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_4items_social_content('Tshirts',tweet_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([('7502 BNP XXL ONLINE ONLY LANDAU UNISEX  SCRUB TOP ONLINE',\n",
       "    'ST SLP TOP RED DEL WK 44 2016 TOP RED SECRET TREASURES',\n",
       "    'CAPT AMERICA UNDEROOS TEST CAPTAIN AMERICA UNDEROOS BOXER BRIEF LAR AVENGERS AP',\n",
       "    \"SCRUBSTAR WOMEN'S SI ONLINE ONLY ELEC BLUE SCRUB TOP SCRUB STAR\")],\n",
       "  'Frequency Match'),\n",
       " ([('GENERIC TANK LOVE CROCHET TANK GENERIC TOP LICENSE',\n",
       "    'CAPTAIN SHIELD PANEL CAPTAIN SHIELD PANEL JUNIORS TANK TOP CAPTAIN AMERICA AP',\n",
       "    'LION KING HILO TANK  DISNEY TOP THE LION KING AP',\n",
       "    'JR FASHION TOP HP CROSSING HARRY POTTER JR FASHION TOP HARRY POTTER')],\n",
       "  'Frequency Match'),\n",
       " ([('CAP AM SS HI LO TOP S3 JULY DELIVERY CPTN AMERICA SS HACCI W/LACE HANGDWN TOP CAPTAIN AMERICA AP',\n",
       "    'MINNIE SS TOP DOLMAN TOP MINNIE SS PINK TOP 4/5 MINNIE MOUSE AP',\n",
       "    'T-SHIRTS ONLINE ONLY MARVEL CAPTAIN AMERICA FOUR SIDED CAP BO ONLINE',\n",
       "    'SPIDERMAN 80S CAPTAIN BOYS POLY TEE CAPTAIN AMERICA AP')],\n",
       "  'Frequency Match')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top12_sim_items(tweet_filepath,term_frequency_model,le,deep_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sleepwear', 'Tshirts', 'Licensed'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top3_social_content(tweet_filepath,term_frequency_model,le,deep_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet=get_tweet_data(tweet_filepath)\n",
    "#str_tweet=str(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_top20_similar_items_with_tweet('Licensed',tweet_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
