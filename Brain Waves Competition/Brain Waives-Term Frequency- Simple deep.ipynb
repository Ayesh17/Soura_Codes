{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s0c02nj/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/s0c02nj/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import ensemble\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import cross_validation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score , recall_score , f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding,Bidirectional\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "import keras \n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.layers import Merge\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding,Bidirectional,concatenate\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('/Users/s0c02nj/Desktop/c3cc8568-0-dataset/train.csv')\n",
    "test=pd.read_csv('/Users/s0c02nj/Desktop/c3cc8568-0-dataset/test.csv')\n",
    "sample_submission=pd.read_csv('/Users/s0c02nj/Desktop/c3cc8568-0-dataset/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy=train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Available Columns in Test and Train for Pre-processing\n",
    "join=train_copy.append(test,sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Date Time\n",
    "join['Date-received'] =  pd.to_datetime(join['Date-received'], format='%m/%d/%Y')\n",
    "join['Date-sent-to-company'] =  pd.to_datetime(join['Date-sent-to-company'], format='%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Time Related Columns\n",
    "\n",
    "# join['hour_recvd']= join['Date-received'].apply(lambda x: correct_spelling(x, x.hours))\n",
    "# join['month_recvd']=join['Date-received'].apply(lambda x: correct_spelling(x, x.months))\n",
    "# join['year_recvd']= join['Date-received'].apply(lambda x: correct_spelling(x, x.years))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "join['diff_time']=join['Date-sent-to-company']-join['Date-received']\n",
    "join['diff_time']=join['diff_time'].apply(lambda x:x.days )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a Separate category for missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Company Respose\n",
    "join['Company-response'].fillna(value='No response',inplace =True)\n",
    "\n",
    "#Missing Dispute-- No Dispute\n",
    "join['Consumer-disputes'].fillna(value=join['Consumer-disputes'].value_counts().index[0],inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "count_trans=len(join['Transaction-Type'].unique())\n",
    "print count_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n"
     ]
    }
   ],
   "source": [
    "count_complain_reason=len(join['Complaint-reason'].unique())\n",
    "print count_complain_reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "count_company_response=len(join['Company-response'].unique())\n",
    "print count_company_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_time=np.array(join['diff_time']).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s0c02nj/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "enc_time=scaler.fit_transform(diff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing The categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.Label Encoding the Categorical Varaible----TransactionType\n",
    "le1= preprocessing.LabelEncoder()\n",
    "enc_trans_type=le1.fit_transform(join['Transaction-Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.Label Encoding the Categorical Varaible----Complaint-reason\n",
    "le2= preprocessing.LabelEncoder()\n",
    "enc_complain_reason=le2.fit_transform(join['Complaint-reason'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.Label Encoding the Categorical Varaible----Company-response\n",
    "le3= preprocessing.LabelEncoder()\n",
    "enc_comp_response=le3.fit_transform(join['Company-response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.Label Encoding the Categorical Varaible----Consumer-disputes\n",
    "le4= preprocessing.LabelEncoder()\n",
    "enc_cons_dispute=le4.fit_transform(join['Consumer-disputes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Encoding Year\n",
    "le6= preprocessing.LabelEncoder()\n",
    "setc_enc= le6.fit_transform(setc_date_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Encoding Year\n",
    "le7= preprocessing.LabelEncoder()\n",
    "dr_enc= le7.fit_transform(dr_date_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Response \n",
    "le5= preprocessing.LabelEncoder()\n",
    "y_cat=le5.fit_transform(train['Complaint-Status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_enc=to_categorical(y_cat,num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the Text Data Column----> Consumer-complaint-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "join['Consumer-complaint-summary'] = join['Consumer-complaint-summary'].apply(lambda x: re.sub(r'[^a-zA-Z]', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "join['Consumer-complaint-summary'] = join['Consumer-complaint-summary'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "join['Consumer-complaint-summary']=join['Consumer-complaint-summary'].str.replace('XX','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "join['Consumer-complaint-summary']=join['Consumer-complaint-summary'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Featue Engineering on Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len = join['Consumer-complaint-summary'].apply(lambda x: len(str(x)))\n",
    "data_len_char = join['Consumer-complaint-summary'].apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data_len_word = join['Consumer-complaint-summary'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "setc_date_month= join['Date-sent-to-company'].apply(lambda x:x.month)\n",
    "setc_date_year= join['Date-sent-to-company'].apply(lambda x:x.year)\n",
    "setc_date_day= join['Date-sent-to-company'].apply(lambda x:x.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_date_month= join['Date-received'].apply(lambda x:x.month)\n",
    "dr_date_year= join['Date-received'].apply(lambda x:x.year)\n",
    "dr_date_day= join['Date-received'].apply(lambda x:x.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'join' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a85f7a8efe9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtf_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Consumer-complaint-summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'join' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english',max_df=1.0, min_df=1, max_features=1200)\n",
    "tf_feat = vectorizer.fit_transform(join['Consumer-complaint-summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "tf_idf_arr= tf_feat.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Inputs for our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transaction\n",
    "enc_trans_type_train=enc_trans_type[0:43266]\n",
    "enc_trans_type_test=enc_trans_type[43266:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complaint reason\n",
    "enc_complain_reason_train=enc_complain_reason[0:43266]\n",
    "enc_complain_reason_test=enc_complain_reason[43266:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_comp_response_train=enc_comp_response[0:43266]\n",
    "enc_comp_response_test= enc_comp_response[43266:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_cons_dispute_train=enc_cons_dispute[0:43266]\n",
    "enc_cons_dispute_test= enc_cons_dispute[43266:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_time_train=enc_time[0:43266]\n",
    "enc_time_test=enc_time[43266:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_arr_train=tf_idf_arr[0:43266]\n",
    "tf_idf_arr_test=tf_idf_arr[43266:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len_train = data_len[0:43266]\n",
    "data_len_test = data_len[43266:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len_char_train = data_len_char[0:43266]\n",
    "data_len_char_test = data_len_char[43266:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len_word_train = data_len_word[0:43266]\n",
    "data_len_word_test  = data_len_word[43266:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "setc_date_month_train=setc_date_month[0:43266]\n",
    "setc_date_month_test=setc_date_month[43266:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "setc_enc_train = setc_enc[0:43266]\n",
    "setc_enc_test= setc_enc[43266:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "setc_date_day_train = setc_date_day[0:43266]\n",
    "setc_date_day_test = setc_date_day[43266:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_date_month_train = dr_date_month[0:43266]\n",
    "dr_date_month_test = dr_date_month[43266:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_enc_train = dr_enc[0:43266]\n",
    "dr_enc_test = dr_enc[43266:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_date_day_train = dr_date_day[0:43266]\n",
    "dr_date_day_test = dr_date_day[43266:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.hstack((enc_trans_type_train.reshape(-1,1) ,\n",
    "                  enc_complain_reason_train.reshape(-1,1),\n",
    "                  enc_comp_response_train.reshape(-1,1),\n",
    "                  enc_cons_dispute_train.reshape(-1,1),\n",
    "                  enc_time_train.reshape(-1,1),\n",
    "                  tf_idf_arr_train,\n",
    "                  np.array(data_len_train).reshape(-1,1),\n",
    "                  np.array(data_len_char_train).reshape(-1,1),\n",
    "                  np.array(data_len_word_train).reshape(-1,1),\n",
    "                  np.array(setc_date_month_train).reshape(-1,1),\n",
    "                  np.array(setc_date_day_train).reshape(-1,1),\n",
    "                  np.array(dr_date_month_train).reshape(-1,1),\n",
    "                  np.array(dr_date_day_train).reshape(-1,1),\n",
    "                  setc_enc_train.reshape(-1,1), \n",
    "                  dr_enc_train.reshape(-1,1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.hstack((enc_trans_type_test.reshape(-1,1) ,\n",
    "                   enc_complain_reason_test.reshape(-1,1),\n",
    "                  enc_comp_response_test.reshape(-1,1),\n",
    "                  enc_cons_dispute_test.reshape(-1,1),\n",
    "                  enc_time_test.reshape(-1,1), \n",
    "                  tf_idf_arr_test,\n",
    "                  np.array(data_len_test).reshape(-1,1),\n",
    "                  np.array(data_len_char_test).reshape(-1,1),\n",
    "                  np.array(data_len_word_test).reshape(-1,1),\n",
    "                  np.array(setc_date_month_test).reshape(-1,1),\n",
    "                  np.array(setc_date_day_test).reshape(-1,1),\n",
    "                  np.array(dr_date_month_test).reshape(-1,1),\n",
    "                  np.array(dr_date_day_test).reshape(-1,1),\n",
    "                  setc_enc_test.reshape(-1,1),\n",
    "                  dr_enc_test.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43266, 5014)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_deep():\n",
    "    \n",
    "    #Defining the input-----> Transaction\n",
    "    inputs1 = Input(shape=(5014,))\n",
    "    layer_dense = Dense(2000, activation='relu')(inputs1)\n",
    "    layer_dense = Dropout(0.5)(layer_dense)\n",
    "    \n",
    "    layer_dense = Dense(600, activation='relu')(layer_dense)\n",
    "    layer_dense = Dropout(0.5)(layer_dense)\n",
    "    \n",
    "    layer_dense = Dense(300, activation='relu')(layer_dense)\n",
    "    layer_dense = Dropout(0.5)(layer_dense)\n",
    "    \n",
    "    layer_dense = Dense(100, activation='relu')(layer_dense)\n",
    "    layer_dense = Dropout(0.5)(layer_dense)\n",
    "    \n",
    "    layer_dense = Dense(50, activation='relu')(layer_dense)\n",
    "    layer_dense = Dropout(0.5)(layer_dense)\n",
    "    \n",
    "    probabilities = Dense(5,activation='softmax')(layer_dense)\n",
    "    \n",
    "    model = Model(inputs=inputs1,outputs=probabilities)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 5014)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2000)              10030000  \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 600)               1200600   \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 300)               180300    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 11,446,305\n",
      "Trainable params: 11,446,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_deep()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = \"categorical_crossentropy\", optimizer = Adam(lr=0.001), metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38939 samples, validate on 4327 samples\n",
      "Epoch 1/1\n",
      "29696/38939 [=====================>........] - ETA: 8s - loss: 9.8867 - acc: 0.4306"
     ]
    }
   ],
   "source": [
    "#Deep Learning Model\n",
    "history = model.fit(train_x ,y_enc, \n",
    "                    batch_size = 512, \n",
    "                    epochs = 1, validation_split=0.1, \n",
    "                    verbose = 1,\n",
    "                    class_weight = {0:23,1:1,2:5,3:4,4:20})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf =  RandomForestClassifier(bootstrap=True, criterion='gini',\n",
    "                max_depth=18, max_features='auto', max_leaf_nodes=None, \n",
    "                class_weight={0:23,1:1,2:5,3:4,4:20},\n",
    "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                min_samples_leaf=1, min_samples_split=2,\n",
    "                min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=-1,\n",
    "                oob_score=False, random_state=0, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True,\n",
       "            class_weight={0: 23, 1: 1, 2: 5, 3: 4, 4: 20},\n",
       "            criterion='gini', max_depth=18, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=600, n_jobs=-1, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(train_x, y_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9044515323810844"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(train_x, y_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9592089  0.97696793 0.75585522 0.49182941 0.77258567]\n",
      "[0.98602287 0.91114253 0.89987326 0.79974076 1.        ]\n",
      "[0.97243108 0.94290779 0.82160077 0.60908193 0.87170475]\n"
     ]
    }
   ],
   "source": [
    "print recall_score(y_cat, rf.predict(train_x),average=None)\n",
    "print precision_score(y_cat, rf.predict(train_x),average=None)\n",
    "print f1_score(y_cat, rf.predict(train_x), average= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classes = rf.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s0c02nj/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "y_pred=list(le5.inverse_transform(y_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=pd.DataFrame()\n",
    "submission['Complaint-ID']=test['Complaint-ID']\n",
    "submission['Complaint-Status']=y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(path_or_buf=\"/Users/s0c02nj/Desktop/Submission15_add.csv\", encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "skf.get_n_splits(train_x, y_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_rf=[]\n",
    "# for train_index, val_index in skf.split(train_x, y_cat):\n",
    "    \n",
    "#     #Random Forest\n",
    "#     rf =  RandomForestClassifier(bootstrap=True, criterion='gini',\n",
    "#                 max_depth=18, max_features='auto', max_leaf_nodes=None, class_weight={0:23,1:1,2:5,3:4,4:20},\n",
    "#                 min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#                 min_samples_leaf=1, min_samples_split=2,\n",
    "#                 min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=-1,\n",
    "#                 oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
    "    \n",
    "#     X_train, X_val = train_x[train_index], train_x[val_index]\n",
    "#     y_train, y_val = y_cat[train_index], y_cat[val_index]\n",
    "    \n",
    "#     rf.fit(X_train,y_train)\n",
    "    \n",
    "#     print f1_score(y_val, rf.predict(X_val), average= None)\n",
    "    \n",
    "    \n",
    "#     pred_rf.append(rf.predict_proba(test_x))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
