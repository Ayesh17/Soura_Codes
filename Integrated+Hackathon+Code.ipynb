{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s0c01xa/Desktop/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#IMPORTING \n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "from sklearn import linear_model\n",
    "import sklearn.cross_validation\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import ensemble\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA PREPROCESSING FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOKENIZING DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tokenized_doc(document):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokenized_doc=[]\n",
    "    for i in range(0,len(document)):\n",
    "        word=word_tokenize(document[i])\n",
    "        tokenized_doc.append(word)\n",
    "    \n",
    "    return tokenized_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONVERTING TO LOWERCASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting all words to lower case so that it is of same type at par with word to vec\n",
    "def get_lower_doc(document):\n",
    "    low_words=[]\n",
    "    low_doc=[]\n",
    "    for i in range(len(document)):\n",
    "        for word in document[i]:\n",
    "            lwords=word.lower()\n",
    "            low_words.append(lwords)\n",
    "        low_doc.append(low_words)\n",
    "        low_words=[]\n",
    "    return low_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMOVING STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given a document,remove the stopwords\n",
    "def get_stopwrd_free_doc(document):\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stpwrd_free_doc = []\n",
    "    doc=[]\n",
    "\n",
    "    for i in range (len(document)):\n",
    "        for w in document[i]:\n",
    "            if w not in stop_words:\n",
    "                doc.append(w)\n",
    "        stpwrd_free_doc.append(doc)\n",
    "        doc=[]\n",
    "    \n",
    "    return stpwrd_free_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMOVING PUNCTUATIONS AND NON-ALPHABETIC DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Removing non alpha words\n",
    "def get_alpha_doc(document):\n",
    "    alpha_doc=[]\n",
    "    alpha=[]\n",
    "    for i in range(len(document)):\n",
    "        for word in document[i]:\n",
    "            if (word.isalpha()==True):\n",
    "                alpha.append(word)\n",
    "        alpha_doc.append(alpha)\n",
    "        alpha=[]\n",
    "        \n",
    "    return alpha_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEMMATIZING AND STEMMING THE DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lem_doc(document):\n",
    "    from nltk import WordNetLemmatizer\n",
    "    from nltk.corpus import wordnet\n",
    "    wnl=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "    lem_words=[]\n",
    "    lem_doc=[]\n",
    "\n",
    "    for i in range(len(document)):\n",
    "        for word in document[i]:\n",
    "            lem=wnl.lemmatize(word)\n",
    "            lem_words.append(lem)\n",
    "        \n",
    "        lem_doc.append(lem_words)\n",
    "        lem_words=[]\n",
    "\n",
    "\n",
    "    return lem_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GETTING STRING VERSION OF DOC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_string_version(document):\n",
    "    Str_data=[]\n",
    "    for i in range(0,len(document)):\n",
    "        val=str(document[i])\n",
    "        Str_data.append(val)\n",
    "    return Str_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IGNORES ANY CHARACTER WHICH IS NOT utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_udoc(document):\n",
    "    utf_doc=[]\n",
    "    for i in range(0,len(document)):\n",
    "        val=unicode(x[i],errors=\"ignore\")\n",
    "        utf_doc.append(val)\n",
    "\n",
    "    return utf_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse to Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dense_matrix(term_frquency):\n",
    "    from scipy.sparse import csr_matrix\n",
    "    dense_trm_frq=term_frquency.todense()\n",
    "    return dense_trm_frq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save Data -This data is from Hierarchical Clustering and Business Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_with_cluster_labels(filepath):\n",
    "    #filepath='/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels.csv'\n",
    "    data_with_clust_labels=pd.read_csv(filepath)\n",
    "    return data_with_clust_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_cluster_labels=get_data_with_cluster_labels('/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels.csv')\n",
    "#data_with_cluster_labels\n",
    "#filepath='/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.SAVE TERM FREQ MODEL \\n2.SAVE DEEP MODEL\\n3.get_data_with_cluster_labels and the filepath will have the data\\n4.tweet_data-import'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''1.SAVE TERM FREQ MODEL \n",
    "2.SAVE DEEP MODEL\n",
    "3.get_data_with_cluster_labels and the filepath will have the data\n",
    "4.tweet_data-import'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "LOADING THE TERM_FREQ_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = \"/Users/s0c01xa/Documents/pkl_models hackathon/model.pkl\"\n",
    "#Open the filename to write 'w'\n",
    "file_object = open(filename, 'r')\n",
    "#dumps the model\n",
    "term_frequency_model = pickle.load(file_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING THE DEEP LEARNING MODEL##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "from keras.models import model_from_json\n",
    "json_file = open('deep_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"deep_model.h5\")\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deep_model=loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DEEPMODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LABEL ENCODER AND SAVING THE le function,it's needed for decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydata=get_data_with_cluster_labels(\"/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels.csv\")   #mydata saves the combined data with cluster labels\n",
    "#x-item description is being stored and y stores the corresponding labels\n",
    "#x=mydata['Combined Description']\n",
    "y=mydata['Social Intent']\n",
    "#Label Encoder and converting the y's into integers\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_coded=le.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TWEETS INPUT AND RECCOMENDATION-PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GIVEN THE FILE PATH OF TWEET IT WILL RETURN THE TWEET IN REQUIRED FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tweet_data(tweet_filepath):\n",
    "    from nltk import word_tokenize\n",
    "    #filepath='/Users/s0c01xa/Documents/Walmart_Bplan/Gen_tweet_3.csv'\n",
    "    f=open(tweet_filepath)\n",
    "    tweet_data=f.readline()\n",
    "    tweet=word_tokenize(tweet_data)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GETTING PROCESSED TWEET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_processed_tweet(tweet_filepath):\n",
    "    from nltk import word_tokenize\n",
    "    f=open(tweet_filepath)\n",
    "    tweet_data=f.readline()\n",
    "    tweet=word_tokenize(tweet_data)\n",
    "    val_test=''\n",
    "    for i in tweet:\n",
    "        val=i\n",
    "        #print val\n",
    "        val_test=str(val+\" \"+val_test)\n",
    "    return val_test\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tweet_filepath='/Users/s0c01xa/Documents/Walmart_Bplan/Gen_tweet_3.csv'\n",
    "def get_tweet(tweet_filepath):\n",
    "    from nltk import word_tokenize\n",
    "    f=open(tweet_filepath)\n",
    "    tweet_data=f.readline()\n",
    "    tweet=word_tokenize(tweet_data)\n",
    "    #val_test=''\n",
    "    #for i in tweet:\n",
    "    #    val=i\n",
    "        #print val\n",
    "    #    val_test=str(val+\" \"+val_test)\n",
    "    return tweet\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get_processed_tweet('/Users/s0c01xa/Documents/Walmart_Bplan/Gen_Tweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TWEET DATA CONVERTED TO DENSE TWEET FORM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dense_tweet(tweet_filepath,model):\n",
    "    processed_tweet=get_processed_tweet(tweet_filepath)\n",
    "    test_tweet=model.transform([processed_tweet])\n",
    "    dense_tweet=get_dense_matrix(test_tweet)\n",
    "    return dense_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GETTING THE TOP 3 SIMILAR SOCIAL CONTENTS BASED ON SAVED DEEP MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top3_social_content(tweet_filepath,model,le,deep_model):\n",
    "    #filepath=filepath\n",
    "    model=model\n",
    "    tweet_dense= get_dense_tweet(tweet_filepath,model)\n",
    "    similar_docs=le.inverse_transform(np.argsort(deep_model.predict_proba(tweet_dense)))\n",
    "    top3_social_content=similar_docs[0][-3:]\n",
    "    \n",
    "    return top3_social_content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Tops', 'Tshirts', 'Licensed'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top3_social_content(\"/Users/s0c01xa/Documents/Walmart_Bplan/Gen_tweet.csv\",term_frequency_model,le,deep_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PREPROCESSING THE DOCUMENTS SELECTED on EACH SOCIAL CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_recco_doc(document):\n",
    "    doc_basket1=get_tokenized_doc(document)\n",
    "    doc_basket2=get_lower_doc(doc_basket1)\n",
    "    doc_basket3=get_stopwrd_free_doc(doc_basket2)\n",
    "    doc_basket4=get_alpha_doc(doc_basket3)\n",
    "    doc_basket5=get_lem_doc(doc_basket4)\n",
    "\n",
    "    return doc_basket5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=get_preprocessed_recco_doc(doc_basket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GETTING THE TOP 20 ITEM DESCRIPTION FOR EACH SOCIAL CONTENT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top20_similar_items_with_tweet(social_content,tweet_filepath):\n",
    "    \n",
    "    tweet=get_tweet(tweet_filepath)\n",
    "    #print type(tweet)\n",
    "    df=get_data_with_cluster_labels('/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels_n.csv')\n",
    "    filtered_data=df[df['Social Intent']==social_content]\n",
    "    doc_basket=filtered_data['Combined Description']\n",
    "    doc_basket=np.array(doc_basket)\n",
    "    \n",
    "    preprocessed_recco_doc=get_preprocessed_recco_doc(doc_basket)\n",
    "    #similarity basket saves the items which are similar to the tweet in most similar cluster label\n",
    "    similarity_basket=[]\n",
    "    for i in range(0,len(preprocessed_recco_doc)):\n",
    "        common=len(set(preprocessed_recco_doc[i]).intersection( set(tweet)))\n",
    "        similarity_basket.append(common)\n",
    " \n",
    "    sim=np.argsort(similarity_basket)[-20:]\n",
    "    #print sim\n",
    "    top_twnty_sim_doc=doc_basket[sim]\n",
    "\n",
    "    return top_twnty_sim_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=get_top20_similar_items_with_tweet('Licensed',\"/Users/s0c01xa/Documents/Walmart_Bplan/Gen_tweet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MINNIE SS TOP DOLMAN TOP MINNIE SS PINK TOP 4/5 MINNIE MOUSE AP',\n",
       "       'SNOW QUEEN TOP DISNEY FROZEN SNOW QUEEN ROCKS S/S TOP FROZEN AP',\n",
       "       'SG SUB TIEFRNT TOP SPRGRL SUBL TIE FRNT SUPERGIRL SUBLIMATION TIE FRONT TOP SUPERMAN AP',\n",
       "       'T-SHIRTS ONLINE ONLY MARVEL CAPTAIN AMERICA FOUR SIDED CAP BO ONLINE',\n",
       "       'GRAPHIC TEES ONLINE ONLY S/S SPRGL WOW STRONG RINGER TEE SUPERMAN AP',\n",
       "       'T-SHIRTS ONLINE ONLY ^^MARVEL CAPTAIN AMERICA FLAKSHIELD BOYS ONLINE',\n",
       "       'NEON LOVE TEE KNIT TOP NEON LOVE S/S TEE LICENSE',\n",
       "       'GRAPHIC TEES#TSHIRTS ONLINE ONLY FINDING DORY SS HI LOW CROCHET TOP FINDING DORY AP',\n",
       "       'T-SHIRTS ONLINE ONLY 80S CAPTAIN RYLHTR CAPTAIN AMERICA',\n",
       "       'T-SHIRTS ONLINE ONLY ^^MARVEL CAPTAIN AMERICA LOGO BOYS GRAPH ONLINE',\n",
       "       'CAP AM SS HI LO TOP S3 JULY DELIVERY CPTN AMERICA SS HACCI W/LACE HANGDWN TOP CAPTAIN AMERICA AP',\n",
       "       'SPIDERMAN 80S CAPTAIN BOYS POLY TEE CAPTAIN AMERICA AP',\n",
       "       'SPIDERMAN 80S CAPTAIN BOYS POLY TEE CAPTAIN AMERICA AP',\n",
       "       'SPIDERMAN 80S CAPTAIN BOYS POLY TEE CAPTAIN AMERICA AP',\n",
       "       'CAPTAIN AMERICA STAY HUNGRY BOYS POLY TEE SPIDERMAN AP',\n",
       "       'CAPTAIN AMERICA STAY HUNGRY BOYS POLY TEE SPIDERMAN AP',\n",
       "       'SPIDERMAN 80S CAPTAIN BOYS POLY TEE CAPTAIN AMERICA AP',\n",
       "       'CAPTAIN AMERICA STAY HUNGRY BOYS POLY TEE SPIDERMAN AP',\n",
       "       'CAP AM SS HI LO TOP S3 JULY DELIVERY CPTN AMERICA SS HACCI W/LACE HANGDWN TOP CAPTAIN AMERICA AP',\n",
       "       'CAP AM SS HI LO TOP S3 JULY DELIVERY CPTN AMERICA SS HACCI W/LACE HANGDWN TOP CAPTAIN AMERICA AP'], dtype=object)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n",
    "#GETTING TOP 4 ITEMS FOR EACH SOCIAL CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_4items_social_content(social_content,tweet_filepath):\n",
    "    document=get_top20_similar_items_with_tweet(social_content,tweet_filepath)\n",
    "    #document=get_top20_similarity_doc_with_tweet(cluster,tweet)\n",
    "    top_4items=[]\n",
    "    #Saving the particular characteristics giving the best feature size\n",
    "    model=sklearn.feature_extraction.text.CountVectorizer(input='content', encoding='utf-8', decode_error='strict', \n",
    "                                                strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, \n",
    "                                                stop_words=None, ngram_range=(1,3), analyzer='word', \n",
    "                                                max_df=10000, min_df=0,max_features=1000,\n",
    "                                                vocabulary=None, binary=False)\n",
    "\n",
    "    #Transforming the data to count of token\n",
    "    term_frequency=model.fit_transform(document)\n",
    "    dense_term_frequency=get_dense_matrix(term_frequency)\n",
    "\n",
    "    #Kmeans Clustering in the dense_matrix\n",
    "    from sklearn.cluster import KMeans\n",
    "    kclust = KMeans(n_clusters=4)\n",
    "    kclust.fit(dense_term_frequency)\n",
    "    clust_labels=kclust.labels_\n",
    "    top_4items.append((document[clust_labels==0][0],document[clust_labels==1][0]\n",
    "                       ,document[clust_labels==2][0],document[clust_labels==3][0] ))\n",
    "                    \n",
    "    \n",
    "    return top_4items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('GENERIC TANK LOVE CROCHET TANK GENERIC TOP LICENSE',\n",
       "  'LION KING HILO TANK  DISNEY TOP THE LION KING AP',\n",
       "  'CAPTAIN SHIELD PANEL CAPTAIN SHIELD PANEL JUNIORS TANK TOP CAPTAIN AMERICA AP',\n",
       "  'JR FASHION TOP HP CROSSING HARRY POTTER JR FASHION TOP HARRY POTTER')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_4items_social_content('Tshirts',\"/Users/s0c01xa/Documents/Walmart_Bplan/Gen_tweet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#RETURN TOP 12 RECCOMENDED ITEMS FOR THAT TWEET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top12_sim_items(tweet_filepath,model,le,deep_model):\n",
    "    #filepath=tweet_filepath\n",
    "    document=get_top3_social_content(tweet_filepath,model,le,deep_model)\n",
    "    top12_recco=[]\n",
    "    for i in document:\n",
    "        #word=document[i]\n",
    "        #sim_doc=get_top20_similar_items_with_tweet(document[i],tweet_filepath)\n",
    "        #top4_doc=get_top_4recco(sim_doc)\n",
    "        top4_doc=get_top_4items_social_content(i,tweet_filepath)\n",
    "        top12_recco.append((top4_doc,('Frequency Match')))\n",
    "    return top12_recco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([('W ES JANI ONLINE ONLY WOMENS EARTH SHOE JANI EARTH SPIRIT',\n",
       "    \"M HS HNTG 8` SPORT W WK 04 FA 15 DELETE MENS HS HUNTING 8' SPORT WP HERMAN SURVIVORS\",\n",
       "    'W ES ADDI DEL WK 46 2017 WOMENS EARTH ADDI EARTH SPIRIT',\n",
       "    'W ES CAMI WW DEL WK 46 2017 WOMENS EARTH CAMI WIDE WIDTH EARTH SPIRIT')],\n",
       "  'Frequency Match'),\n",
       " ([('ST CHEMISE ROBE SET 08038 WHITE W BODY B SECRET TREASURES SECRET TREASURES',\n",
       "    'WS 2PKT STRETCH PANT WS 2PKT STRETCH PANT WS 2PKT STR PANT DK CHARC HTR L WHITE STAG',\n",
       "    'WS 2PKT STR PANT L WS 2PKT STR PANT L WS 2PKT STR PANT TAUPE LARGE WHITE STAG',\n",
       "    \"WHITE STAG WOMEN'S K ONLINE ONLY WHITE STAG KNIT PANT WHITE STAG\")],\n",
       "  'Frequency Match'),\n",
       " ([(\"JOR TG FLARE GLT JN JOR TG FLARE GLT JN JORDACHE - BABY GIRLS' FLARE JEAN WITH R JORDACHE\",\n",
       "    'RIDER SLENDER STRETC RIDERS CMFRT SS JEAN RIDERS MODERN COMFORT SS JEANS RIDERS',\n",
       "    'LEI BOOT W FLP PKT LEI JR JEAN LEI SLIM BOOT W SCROLL EMB FLP PKT LEI',\n",
       "    \"LSS STRAIGHT FIT LSS STRAIGHT FIT SIGNATURE BY LEVI STRAUSS & CO. MEN'S ST SIGNATURE BY LEVI\")],\n",
       "  'Frequency Match')]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top12_sim_items(tweet_filepath,term_frequency_model,le,deep_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GIVEN A SET OF GENSIM VECTORS,IT CONVERTS THEM TO A MATRIX FORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gensim_item_vec(document):\n",
    "    gensim_w2v=np.zeros(((len(document),50)))\n",
    "    for i in range(0,len(document)):\n",
    "        gensim_w2v[i]=document[i]\n",
    "        \n",
    "    return gensim_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GETTING GENSIM SIMILARITY MATRIX FOR DOCUMENT/ITEM DESC BASED ON TWEET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gensim_w2v_sim_matrix(social_content,tweet_filepath):\n",
    "    tweet=get_processed_tweet(tweet_filepath)\n",
    "    \n",
    "    df=get_data_with_cluster_labels('/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels.csv')\n",
    "    filtered_data=df[df['Social Intent']==social_content]\n",
    "    doc_basket=filtered_data['Combined Description']\n",
    "    doc_basket=np.array(doc_basket)\n",
    "\n",
    "    #document=get_top20_similar_items_with_tweet(social_content,tweet_filepath)\n",
    "    from gensim.models import doc2vec\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    data=doc_basket\n",
    "    docs = []\n",
    "    analyzedDocument = namedtuple('AnalyzedDocument', 'words tags') \n",
    "    #converting the words to lowercase and appending it in docs in the form of tuples\n",
    "    #docs[0] will contain the tuple of first doc and it's label\n",
    "    for i in range (0,len(data)):\n",
    "        words = data[i].lower().split()\n",
    "        tags=[str(i)]\n",
    "        docs.append(analyzedDocument(words, tags))\n",
    "\n",
    "    # Train model (set min_count = 1, if you want the model to work with the provided example data set)\n",
    "    model = doc2vec.Doc2Vec(docs, size = 50, window = 300, min_count = 1, workers = 5)\n",
    "    gensim_item_vec=get_gensim_item_vec(model.docvecs)\n",
    "    return gensim_item_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#a=get_gensim_w2v_sim_matrix('Jeans',tweet_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.shape(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#W2V FOR TWEET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gensim_vec_tweet(tweet_filepath):\n",
    "    tweet_filepath=tweet_filepath\n",
    "    from gensim.models import doc2vec\n",
    "    from collections import namedtuple\n",
    "    tweet_doc=get_tweet_data(tweet_filepath)\n",
    "    data=str(tweet_doc)\n",
    "    docs = []\n",
    "    analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "\n",
    "\n",
    "    words = data.lower().split()\n",
    "    tags=[str(0)]\n",
    "    docs.append(analyzedDocument(words, tags))\n",
    "\n",
    "    # Train model (set min_count = 1, if you want the model to work with the provided example data set)\n",
    "    tweet_model = doc2vec.Doc2Vec(docs, size = 50, window = 300, min_count = 1, workers = 5)\n",
    "    tweet_gensim_vec=tweet_model.docvecs[0]\n",
    "    return tweet_gensim_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get_gensim_vec_tweet(tweet_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GENSIM SIMILARITY MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gensim_cosine_similarity(document_vec,tweet_vec):\n",
    "    from scipy.spatial.distance import cosine as cs\n",
    "    gensim_cosine_similarity=np.zeros(len(document_vec))\n",
    "    for i in range(0,len(document_vec)):\n",
    "        gensim_cosine_similarity[i]=cs(document_vec[i],tweet_vec)\n",
    "    return gensim_cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FINDING GENSIM SIMILARITY FOR EACH SOCIAL CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gensim_item_tweet_matrix(social_content,tweet_filepath):\n",
    "    #doc_social_content=doc_social_content\n",
    "    #tweet_filepath=tweet_filepath\n",
    "    gensim_item_matrix=get_gensim_w2v_sim_matrix(social_content,tweet_filepath)\n",
    "    tweet_gensim_vec=get_gensim_vec_tweet(tweet_filepath)\n",
    "    gensim_item_tweet_matrix=get_gensim_cosine_similarity(gensim_item_matrix,tweet_gensim_vec)\n",
    "    return gensim_item_tweet_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#len(get_gensim_item_tweet_matrix('Jeans',tweet_filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GETTING PRE-PROCESSED DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_recco_doc(document):\n",
    "    doc_basket1=get_tokenized_doc(document)\n",
    "    doc_basket2=get_lower_doc(doc_basket1)\n",
    "    doc_basket3=get_stopwrd_free_doc(doc_basket2)\n",
    "    doc_basket4=get_alpha_doc(doc_basket3)\n",
    "    doc_basket5=get_lem_doc(doc_basket4)\n",
    "\n",
    "    return doc_basket5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TOP 4 RECCO BASED ON EACH SOCIAL CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gensim_similarity_doc_with_tweet(social_content,tweet_filepath):\n",
    "    \n",
    "    df=get_data_with_cluster_labels('/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels.csv')\n",
    "    filtered_data=df[df['Social Intent']==social_content]\n",
    "    gensim_doc_basket=filtered_data['Combined Description']\n",
    "    gensim_doc_basket=np.array(gensim_doc_basket)\n",
    "    \n",
    "    gensim_item_tweet_matrix=get_gensim_item_tweet_matrix(social_content,tweet_filepath)\n",
    "    \n",
    "    sim=np.argsort(gensim_item_tweet_matrix)[-4:]\n",
    "    top_4_recco=gensim_doc_basket[sim]\n",
    "    return top_4_recco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'FG FASHION BOOTCUT FASHION BOOTCUT JEAN FG FASHION BOOTCUT JEAN FADED GLORY',\n",
       "       'JD FASHION SHORTALL JORDACHE  SHORTALLS JORDACHE FRINGE HEM SHORTALL JORDACHE',\n",
       "       'FG BOYFRIEND CROP FG CROP FADED BOYFRIEND CROP JEAN FADED GLORY',\n",
       "       'FG DENIM CROP CAPITAL FG FASHION DENIM CROP FADED GLORY'], dtype=object)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gensim_similarity_doc_with_tweet('Jeans',tweet_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TOP 12 ITEMS RECCO BASED ON GENISM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top12_gensim_sim_items(tweet_filepath,model,le,deep_model):\n",
    "    document=get_top3_social_content(tweet_filepath,model,le,deep_model)\n",
    "    top12_gensim_recco=[]\n",
    "    for i in document:\n",
    "        tweet_filepath=tweet_filepath\n",
    "        gensim_sim_doc=get_gensim_similarity_doc_with_tweet(i,tweet_filepath)\n",
    "        top12_gensim_recco.append((gensim_sim_doc,('Context Based')))\n",
    "    return top12_gensim_recco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 'W FG SWEATER BOOT DEL WK 46 2017 WOMENS FADED GLORY SWEATER BOOT FADED GLORY',\n",
       "         'CF GIRLS SNOW PRINCE  CF GIRLS SNOW PRINCESS BLACK SIZE 3 UNBRANDED',\n",
       "         'M DK TRUXX KODIAK TERRA MENS WORK BOOT DICKIES',\n",
       "         'GT OT WTR HOUNDSTOOT WDSS GIRLS OT WINTER BOOT OZARK TRAIL'], dtype=object),\n",
       "  'Context Based'),\n",
       " (array(['FG FRINGE PONCHO DEL WK 49 2016 FADED GLORY FRINGE PONCHO FADED GLORY',\n",
       "         'FG JEGGINGS DEL WK 22 2017 FADED GLORY KNIT JEGGING FADED GLORY',\n",
       "         'RIDERS CORE BOOT RIDERS RIDERS CORE BOOT RIDERS',\n",
       "         'SEQUIN TWOFER DEL WK 46 2016 FADED GLORY SEQUIN TWOFER FADED GLORY'], dtype=object),\n",
       "  'Context Based'),\n",
       " (array(['FADED GLORY TOUGH FADED GLORY FG PERFORMANCE FADED GLORY',\n",
       "         'FG BOYFRIEND JEAN FG BOYFREIND JEAN FADED GLORY DESTRUCTED BOYFRIEND JEAN FADED GLORY',\n",
       "         'FG FASHION BOOTCUT DEL WK 44 2016 FG FASHION JEAN FADED GLORY',\n",
       "         'SHORTALLS#SHORTS ONLINE ONLY JORDACHE FLARE SKIRTALL JORDACHE'], dtype=object),\n",
       "  'Context Based')]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top12_gensim_sim_items(tweet_filepath,term_frequency_model,le,deep_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IMPORTING \n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "from sklearn import linear_model\n",
    "import sklearn.cross_validation\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import ensemble\n",
    "import time\n",
    "import random\n",
    "\n",
    "##########\n",
    "def get_tokenized_doc(document):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokenized_doc=[]\n",
    "    for i in range(0,len(document)):\n",
    "        word=word_tokenize(document[i])\n",
    "        tokenized_doc.append(word)\n",
    "    \n",
    "    return tokenized_doc\n",
    "\n",
    "##########\n",
    "#Converting all words to lower case so that it is of same type at par with word to vec\n",
    "def get_lower_doc(document):\n",
    "    low_words=[]\n",
    "    low_doc=[]\n",
    "    for i in range(len(document)):\n",
    "        for word in document[i]:\n",
    "            lwords=word.lower()\n",
    "            low_words.append(lwords)\n",
    "        low_doc.append(low_words)\n",
    "        low_words=[]\n",
    "    return low_doc\n",
    "\n",
    "##########\n",
    "#Given a document,remove the stopwords\n",
    "def get_stopwrd_free_doc(document):\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stpwrd_free_doc = []\n",
    "    doc=[]\n",
    "\n",
    "    for i in range (len(document)):\n",
    "        for w in document[i]:\n",
    "            if w not in stop_words:\n",
    "                doc.append(w)\n",
    "        stpwrd_free_doc.append(doc)\n",
    "        doc=[]\n",
    "    \n",
    "    return stpwrd_free_doc\n",
    "\n",
    "##########\n",
    "#Removing non alpha words\n",
    "def get_alpha_doc(document):\n",
    "    alpha_doc=[]\n",
    "    alpha=[]\n",
    "    for i in range(len(document)):\n",
    "        for word in document[i]:\n",
    "            if (word.isalpha()==True):\n",
    "                alpha.append(word)\n",
    "        alpha_doc.append(alpha)\n",
    "        alpha=[]\n",
    "        \n",
    "    return alpha_doc\n",
    "\n",
    "def get_lem_doc(document):\n",
    "    from nltk import WordNetLemmatizer\n",
    "    from nltk.corpus import wordnet\n",
    "    wnl=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "    lem_words=[]\n",
    "    lem_doc=[]\n",
    "\n",
    "    for i in range(len(document)):\n",
    "        for word in document[i]:\n",
    "            lem=wnl.lemmatize(word)\n",
    "            lem_words.append(lem)\n",
    "        \n",
    "        lem_doc.append(lem_words)\n",
    "        lem_words=[]\n",
    "\n",
    "\n",
    "    return lem_doc\n",
    "\n",
    "\n",
    "##########\n",
    "def get_string_version(document):\n",
    "    Str_data=[]\n",
    "    for i in range(0,len(document)):\n",
    "        val=str(document[i])\n",
    "        Str_data.append(val)\n",
    "    return Str_data\n",
    "\n",
    "\n",
    "##########\n",
    "def get_udoc(document):\n",
    "    utf_doc=[]\n",
    "    for i in range(0,len(document)):\n",
    "        val=unicode(x[i],errors=\"ignore\")\n",
    "        utf_doc.append(val)\n",
    "\n",
    "    return utf_doc\n",
    "\n",
    "\n",
    "##########\n",
    "def get_dense_matrix(term_frquency):\n",
    "    from scipy.sparse import csr_matrix\n",
    "    dense_trm_frq=term_frquency.todense()\n",
    "    return dense_trm_frq\n",
    "\n",
    "\n",
    "##########\n",
    "def get_data_with_cluster_labels(filepath):\n",
    "    #filepath='/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels.csv'\n",
    "    data_with_clust_labels=pd.read_csv(filepath)\n",
    "    return data_with_clust_labels\n",
    "\n",
    "##########\n",
    "data_with_cluster_labels=get_data_with_cluster_labels('/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels.csv')\n",
    "#data_with_cluster_labels\n",
    "#filepath='/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels.csv'\n",
    "\n",
    "\n",
    "##########\n",
    "import pickle\n",
    "filename = \"/Users/s0c01xa/Documents/pkl_models hackathon/model.pkl\"\n",
    "#Open the filename to write 'w'\n",
    "file_object = open(filename, 'r')\n",
    "#dumps the model\n",
    "term_frequency_model = pickle.load(file_object)\n",
    "\n",
    "\n",
    "##########\n",
    "# load json and create model\n",
    "from keras.models import model_from_json\n",
    "json_file = open('deep_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"deep_model.h5\")\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "##########\n",
    "deep_model=loaded_model\n",
    "\n",
    "##########\n",
    "mydata=get_data_with_cluster_labels(\"/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels.csv\")   #mydata saves the combined data with cluster labels\n",
    "#x-item description is being stored and y stores the corresponding labels\n",
    "#x=mydata['Combined Description']\n",
    "y=mydata['Social Intent']\n",
    "#Label Encoder and converting the y's into integers\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_coded=le.fit_transform(y)\n",
    "\n",
    "\n",
    "##########\n",
    "def get_tweet_data(tweet_filepath):\n",
    "    from nltk import word_tokenize\n",
    "    #filepath='/Users/s0c01xa/Documents/Walmart_Bplan/Gen_tweet_3.csv'\n",
    "    f=open(tweet_filepath)\n",
    "    tweet_data=f.readline()\n",
    "    tweet=word_tokenize(tweet_data)\n",
    "    return tweet\n",
    "\n",
    "##########\n",
    "def get_processed_tweet(tweet_filepath):\n",
    "    from nltk import word_tokenize\n",
    "    f=open(tweet_filepath)\n",
    "    tweet_data=f.readline()\n",
    "    tweet=word_tokenize(tweet_data)\n",
    "    val_test=''\n",
    "    for i in tweet:\n",
    "        val=i\n",
    "        #print val\n",
    "        val_test=str(val+\" \"+val_test)\n",
    "    return val_test\n",
    "\n",
    "##########\n",
    "def get_dense_tweet(tweet_filepath,model):\n",
    "    processed_tweet=get_processed_tweet(tweet_filepath)\n",
    "    test_tweet=model.transform([processed_tweet])\n",
    "    dense_tweet=get_dense_matrix(test_tweet)\n",
    "    return dense_tweet\n",
    "\n",
    "##########\n",
    "def get_top3_social_content(tweet_filepath,model,le,deep_model):\n",
    "    #filepath=filepath\n",
    "    model=model\n",
    "    tweet_dense= get_dense_tweet(tweet_filepath,model)\n",
    "    similar_docs=le.inverse_transform(np.argsort(deep_model.predict_proba(tweet_dense)))\n",
    "    top3_social_content=similar_docs[0][-3:]\n",
    "    \n",
    "    return top3_social_content\n",
    "    \n",
    "##########\n",
    "def get_preprocessed_recco_doc(document):\n",
    "    doc_basket1=get_tokenized_doc(document)\n",
    "    doc_basket2=get_lower_doc(doc_basket1)\n",
    "    doc_basket3=get_stopwrd_free_doc(doc_basket2)\n",
    "    doc_basket4=get_alpha_doc(doc_basket3)\n",
    "    doc_basket5=get_lem_doc(doc_basket4)\n",
    "\n",
    "    return doc_basket5\n",
    "\n",
    "##########\n",
    "def get_top20_similar_items_with_tweet(social_content,tweet_filepath):\n",
    "    \n",
    "    tweet=get_processed_tweet(tweet_filepath)\n",
    "    df=get_data_with_cluster_labels('/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels.csv')\n",
    "    filtered_data=df[df['Social Intent']==social_content]\n",
    "    doc_basket=filtered_data['Combined Description']\n",
    "    doc_basket=np.array(doc_basket)\n",
    "\n",
    "    preprocessed_recco_doc=get_preprocessed_recco_doc(doc_basket)\n",
    "    #similarity basket saves the items which are similar to the tweet in most similar cluster label\n",
    "    similarity_basket=[]\n",
    "    for i in range(0,len(preprocessed_recco_doc)):\n",
    "        common=len(set(preprocessed_recco_doc[i]).intersection( set(tweet)))\n",
    "        similarity_basket.append(common)\n",
    "\n",
    "    sim=np.argsort(similarity_basket)[-20:]\n",
    "    top_twnty_sim_doc=doc_basket[sim]\n",
    "\n",
    "    return top_twnty_sim_doc\n",
    "\n",
    "\n",
    "##########\n",
    "def get_top_4items_social_content(social_content,tweet_filepath):\n",
    "    document=get_top20_similar_items_with_tweet(social_content,tweet_filepath)\n",
    "    #document=get_top20_similarity_doc_with_tweet(cluster,tweet)\n",
    "    top_4items=[]\n",
    "    #Saving the particular characteristics giving the best feature size\n",
    "    model=sklearn.feature_extraction.text.CountVectorizer(input='content', encoding='utf-8', decode_error='strict', \n",
    "                                                strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, \n",
    "                                                stop_words=None, ngram_range=(1,3), analyzer='word', \n",
    "                                                max_df=10000, min_df=0,max_features=1000,\n",
    "                                                vocabulary=None, binary=False)\n",
    "\n",
    "    #Transforming the data to count of token\n",
    "    term_frequency=model.fit_transform(document)\n",
    "    dense_term_frequency=get_dense_matrix(term_frequency)\n",
    "\n",
    "    #Kmeans Clustering in the dense_matrix\n",
    "    from sklearn.cluster import KMeans\n",
    "    kclust = KMeans(n_clusters=4)\n",
    "    kclust.fit(dense_term_frequency)\n",
    "    clust_labels=kclust.labels_\n",
    "    top_4items.append((document[clust_labels==0][0],document[clust_labels==1][0]\n",
    "                       ,document[clust_labels==2][0],document[clust_labels==3][0] ))\n",
    "                    \n",
    "    \n",
    "    return top_4items\n",
    "\n",
    "\n",
    "##########\n",
    "def get_top12_sim_items(tweet_filepath,model,le,deep_model):\n",
    "    #filepath=tweet_filepath\n",
    "    document=get_top3_social_content(tweet_filepath,model,le,deep_model)\n",
    "    top12_recco=[]\n",
    "    for i in document:\n",
    "        #word=document[i]\n",
    "        #sim_doc=get_top20_similar_items_with_tweet(document[i],tweet_filepath)\n",
    "        #top4_doc=get_top_4recco(sim_doc)\n",
    "        top4_doc=get_top_4items_social_content(i,tweet_filepath)\n",
    "        top12_recco.append((top4_doc,('Frequency Match')))\n",
    "    return top12_recco\n",
    "\n",
    "\n",
    "##########\n",
    "def get_gensim_item_vec(document):\n",
    "    gensim_w2v=np.zeros(((len(document),50)))\n",
    "    for i in range(0,len(document)):\n",
    "        gensim_w2v[i]=document[i]\n",
    "        \n",
    "    return gensim_w2v\n",
    "\n",
    "\n",
    "##########\n",
    "def get_gensim_w2v_sim_matrix(social_content,tweet_filepath):\n",
    "    tweet=get_processed_tweet(tweet_filepath)\n",
    "    \n",
    "    df=get_data_with_cluster_labels('/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels.csv')\n",
    "    filtered_data=df[df['Social Intent']==social_content]\n",
    "    doc_basket=filtered_data['Combined Description']\n",
    "    doc_basket=np.array(doc_basket)\n",
    "\n",
    "    #document=get_top20_similar_items_with_tweet(social_content,tweet_filepath)\n",
    "    from gensim.models import doc2vec\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    data=doc_basket\n",
    "    docs = []\n",
    "    analyzedDocument = namedtuple('AnalyzedDocument', 'words tags') \n",
    "    #converting the words to lowercase and appending it in docs in the form of tuples\n",
    "    #docs[0] will contain the tuple of first doc and it's label\n",
    "    for i in range (0,len(data)):\n",
    "        words = data[i].lower().split()\n",
    "        tags=[str(i)]\n",
    "        docs.append(analyzedDocument(words, tags))\n",
    "\n",
    "    # Train model (set min_count = 1, if you want the model to work with the provided example data set)\n",
    "    model = doc2vec.Doc2Vec(docs, size = 50, window = 300, min_count = 1, workers = 5)\n",
    "    gensim_item_vec=get_gensim_item_vec(model.docvecs)\n",
    "    return gensim_item_vec\n",
    "\n",
    "##########\n",
    "def get_gensim_vec_tweet(tweet_filepath):\n",
    "    tweet_filepath=tweet_filepath\n",
    "    from gensim.models import doc2vec\n",
    "    from collections import namedtuple\n",
    "    tweet_doc=get_tweet_data(tweet_filepath)\n",
    "    data=str(tweet_doc)\n",
    "    docs = []\n",
    "    analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "\n",
    "\n",
    "    words = data.lower().split()\n",
    "    tags=[str(0)]\n",
    "    docs.append(analyzedDocument(words, tags))\n",
    "\n",
    "    # Train model (set min_count = 1, if you want the model to work with the provided example data set)\n",
    "    tweet_model = doc2vec.Doc2Vec(docs, size = 50, window = 300, min_count = 1, workers = 5)\n",
    "    tweet_gensim_vec=tweet_model.docvecs[0]\n",
    "    return tweet_gensim_vec\n",
    "\n",
    "\n",
    "##########\n",
    "def get_gensim_cosine_similarity(document_vec,tweet_vec):\n",
    "    from scipy.spatial.distance import cosine as cs\n",
    "    gensim_cosine_similarity=np.zeros(len(document_vec))\n",
    "    for i in range(0,len(document_vec)):\n",
    "        gensim_cosine_similarity[i]=cs(document_vec[i],tweet_vec)\n",
    "    return gensim_cosine_similarity\n",
    "\n",
    "\n",
    "##########\n",
    "def get_gensim_item_tweet_matrix(social_content,tweet_filepath):\n",
    "    #doc_social_content=doc_social_content\n",
    "    #tweet_filepath=tweet_filepath\n",
    "    gensim_item_matrix=get_gensim_w2v_sim_matrix(social_content,tweet_filepath)\n",
    "    tweet_gensim_vec=get_gensim_vec_tweet(tweet_filepath)\n",
    "    gensim_item_tweet_matrix=get_gensim_cosine_similarity(gensim_item_matrix,tweet_gensim_vec)\n",
    "    return gensim_item_tweet_matrix\n",
    "\n",
    "\n",
    "##########\n",
    "def get_preprocessed_recco_doc(document):\n",
    "    doc_basket1=get_tokenized_doc(document)\n",
    "    doc_basket2=get_lower_doc(doc_basket1)\n",
    "    doc_basket3=get_stopwrd_free_doc(doc_basket2)\n",
    "    doc_basket4=get_alpha_doc(doc_basket3)\n",
    "    doc_basket5=get_lem_doc(doc_basket4)\n",
    "\n",
    "    return doc_basket5\n",
    "\n",
    "\n",
    "##########\n",
    "def get_gensim_similarity_doc_with_tweet(social_content,tweet_filepath):\n",
    "    \n",
    "    df=get_data_with_cluster_labels('/Users/s0c01xa/Documents/Walmart_Bplan/Data with Cluster labels.csv')\n",
    "    filtered_data=df[df['Social Intent']==social_content]\n",
    "    gensim_doc_basket=filtered_data['Combined Description']\n",
    "    gensim_doc_basket=np.array(gensim_doc_basket)\n",
    "    \n",
    "    gensim_item_tweet_matrix=get_gensim_item_tweet_matrix(social_content,tweet_filepath)\n",
    "    \n",
    "    sim=np.argsort(gensim_item_tweet_matrix)[-4:]\n",
    "    top_4_recco=gensim_doc_basket[sim]\n",
    "    return top_4_recco\n",
    "\n",
    "\n",
    "##########\n",
    "def get_top12_gensim_sim_items(tweet_filepath,model,le,deep_model):\n",
    "    document=get_top3_social_content(tweet_filepath,model,le,deep_model)\n",
    "    top12_gensim_recco=[]\n",
    "    for i in document:\n",
    "        tweet_filepath=tweet_filepath\n",
    "        gensim_sim_doc=get_gensim_similarity_doc_with_tweet(i,tweet_filepath)\n",
    "        top12_gensim_recco.append((gensim_sim_doc,('Context Based')))\n",
    "    return top12_gensim_recco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
